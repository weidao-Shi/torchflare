{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TorchFlare is a simple, beginner-friendly and an easy-to-use PyTorch Framework train your models without much effort. It provides an almost Keras-like experience for training your models with all the callbacks, metrics, etc Features A high-level module for Keras-like training. Off-the-shelf Dataloaders for standard tasks(Classification, Regression, etc) Callbacks for model checkpoints, early stopping, and much more! Metrics and much more. Currently, TorchFlare supports CPU and GPU training. DDP and TPU support will be coming soon! Getting Started The core idea around TorchFlare is the Experiment class. It handles all the internal stuff like boiler plate code for training, calling callbacks,metrics,etc. The only thing you need to focus on is creating you PyTorch Model. Also, there are off-the-shelf dataloaders available for standard tasks, so that you don't have to worry about creating Pytorch Datasets. Here is an easy-to-understand example to show how Experiment class works. import torch import torch.nn as nn from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics #Some dummy dataloaders train_dl = SomeTrainingDataloader () valid_dl = SomeValidationDataloader () test_dl = SomeTestingDataloader () Create a pytorch Model model = nn . Sequential ( nn . Linear ( num_features , hidden_state_size ), nn . ReLU (), nn . Linear ( hidden_state_size , num_classes ) ) Define callbacks and metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 )] Define your experiment # Set some constants for training exp = Experiment ( num_epochs = 5 , fp16 = False , device = \"cuda\" , seed = 42 , ) # Compile your experiment with model, optimizer, schedulers, etc exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) # Run your experiment with training dataloader and validation dataloader. exp . fit_on_loader ( train_dl = train_dl , valid_dl = valid_dl ) For inference, you can use infer method, which yields output per batch. You can use it as follows outputs = [] for op in exp . predict_on_loader ( test_loader = test_dl , path_to_model = './models/model.bin' , device = 'cuda' ): op = some_post_process_function ( op ) outputs . extend ( op ) If you want to access your experiments history or plot it. You can do it as follows. history = exp . history # This will return a dict # If you want to plot progress of particular metric as epoch progress use this. exp . plot_history ( keys = [ \"loss\" , \"accuracy\" ] , save_fig = False , plot_fig = True ) Examples Image Classification on CIFAR-10 using TorchFlare. Text Classification on IMDB data. Binary Classification of Tabular Data on previous kaggle competition Tutorial on using Hydra and TorchFlare for efficient workflow and parameter management.","title":"TorchFlare"},{"location":"#features","text":"A high-level module for Keras-like training. Off-the-shelf Dataloaders for standard tasks(Classification, Regression, etc) Callbacks for model checkpoints, early stopping, and much more! Metrics and much more. Currently, TorchFlare supports CPU and GPU training. DDP and TPU support will be coming soon!","title":"Features"},{"location":"#getting-started","text":"The core idea around TorchFlare is the Experiment class. It handles all the internal stuff like boiler plate code for training, calling callbacks,metrics,etc. The only thing you need to focus on is creating you PyTorch Model. Also, there are off-the-shelf dataloaders available for standard tasks, so that you don't have to worry about creating Pytorch Datasets. Here is an easy-to-understand example to show how Experiment class works. import torch import torch.nn as nn from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics #Some dummy dataloaders train_dl = SomeTrainingDataloader () valid_dl = SomeValidationDataloader () test_dl = SomeTestingDataloader () Create a pytorch Model model = nn . Sequential ( nn . Linear ( num_features , hidden_state_size ), nn . ReLU (), nn . Linear ( hidden_state_size , num_classes ) ) Define callbacks and metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 )] Define your experiment # Set some constants for training exp = Experiment ( num_epochs = 5 , fp16 = False , device = \"cuda\" , seed = 42 , ) # Compile your experiment with model, optimizer, schedulers, etc exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) # Run your experiment with training dataloader and validation dataloader. exp . fit_on_loader ( train_dl = train_dl , valid_dl = valid_dl ) For inference, you can use infer method, which yields output per batch. You can use it as follows outputs = [] for op in exp . predict_on_loader ( test_loader = test_dl , path_to_model = './models/model.bin' , device = 'cuda' ): op = some_post_process_function ( op ) outputs . extend ( op ) If you want to access your experiments history or plot it. You can do it as follows. history = exp . history # This will return a dict # If you want to plot progress of particular metric as epoch progress use this. exp . plot_history ( keys = [ \"loss\" , \"accuracy\" ] , save_fig = False , plot_fig = True )","title":"Getting Started"},{"location":"#examples","text":"Image Classification on CIFAR-10 using TorchFlare. Text Classification on IMDB data. Binary Classification of Tabular Data on previous kaggle competition Tutorial on using Hydra and TorchFlare for efficient workflow and parameter management.","title":"Examples"},{"location":"experiment/","text":"Simple class for handling boilerplate code for training, validation and Inference. Methods __init__ ( self , num_epochs , fp16 = False , device = 'cuda' , seed = 42 ) special Init method to set up important variables for training and validation. Parameters: Name Type Description Default num_epochs int The number of epochs to save model. required fp16 bool Set this to True if you want to use mixed precision training. False device str The device where you want train your model. 'cuda' seed int The seed to ensure reproducibility. 42 compile_experiment ( self , model , optimizer , optimizer_params , criterion , callbacks = None , metrics = None , main_metric = None ) Configures the model for training and validation. Parameters: Name Type Description Default model Module The model to be trained. required optimizer Union[torch.optim.optimizer.Optimizer, str, Any] The optimizer to be used or name of optimizer. If you pass in the name of the optimizer, only optimizers available in pytorch are supported. required optimizer_params Dict[str, Union[int, float]] The parameters to be used for the optimizer. required criterion Union[Callable[[torch.Tensor], torch.Tensor], str] The loss function to optimize or name of the loss function. If you pass in the name of the loss function, only loss functions available in pytorch can be supported. required callbacks List The list of callbacks to be used. None metrics List The list of metrics to be used. None main_metric Optional[str] The name of main metric to be monitored. Use lower case version. For examples , use 'accuracy' instead of 'Accuracy'. None Note Supports all the schedulers implemented in pytorch/transformers except SWA. Support for custom scheduling will be added soon. fit ( self , x , y , val_data , batch_size = 64 , dataloader_kwargs = None ) Train and validate the model on training and validation dataset. Parameters: Name Type Description Default x Union[torch.Tensor, numpy.ndarray] A numpy array(or array-like) or torch.tensor for inputs to the model. required y Union[torch.Tensor, numpy.ndarray] Target data. Same type as input data coule numpy array(or array-like) or torch.tensors. required val_data Union[Tuple, List] A tuple or list (x_val , y_val) of numpy arrays or torch.tensors. required batch_size int The batch size to be used for training and validation. 64 dataloader_kwargs Dict Keyword arguments to pass to the PyTorch dataloaders created internally. By default, shuffle=True is passed for the training dataloader but this can be overriden by using this argument. None Note Model will only be saved when ModelCheckpoint callback is used. fit_loader ( self , train_dl , valid_dl ) Train and validate the model using dataloaders. Parameters: Name Type Description Default train_dl DataLoader The training dataloader. required valid_dl DataLoader The validation dataloader. required Note Model will only be saved when ModelCheckpoint callback is used. predict ( self , x , path_to_model , batch_size = 64 , dataloader_kwargs = None , device = 'cuda' ) Method to perform inference on test data. Parameters: Name Type Description Default x Union[torch.Tensor, numpy.ndarray] A numpy array(or array-like) or torch.tensor for inputs to the model. required batch_size int The batch size to be used for inference. 64 device str The device on which you want to perform inference. 'cuda' dataloader_kwargs Dict Keyword arguments to pass to the PyTorch dataloader which is created internally. None path_to_model str str, required predict_on_loader ( self , path_to_model , test_dl , device = 'cuda' ) Method to perform inference on test dataloader. Parameters: Name Type Description Default test_dl DataLoader The dataloader to be use for testing. required device str The device on which you want to perform inference. 'cuda' path_to_model str The full path to model required Yields Output per batch. Examples import torch import torchflare.callbacks as cbs import torchflare.metrics as metrics from torchflare.experiments import Experiment # Defining Training/Validation Dataloaders train_dl = SomeTrainDataloader () valid_dl = SomeValidDataloader () # Defining some basic model model = SomeModel () # Defining params optimizer = \"Adam\" optimizer_params = dict ( lr = 1e-4 ) criterion = \"cross_entropy\" num_epochs = 10 num_classes = 4 # Defining the list of metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False ), ] # Defining the list of callbacks callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 3 ) #Defining Scheduler callback. ] # Creating Experiment and setting the params. exp = Experiment ( num_epochs = num_epochs , save_dir = \"./test_save\" , model_name = \"test_classification.bin\" , fp16 = True , device = device , seed = 42 , compute_train_metrics = False , ) # Compiling the experiment exp . compile_experiment ( model = model , metrics = metric_list , callbacks = callbacks , main_metric = \"accuracy\" , optimizer = optimizer , optimizer_params = optimizer_params , criterion = criterion , ) # Running the experiment exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl )","title":"Experiment"},{"location":"experiment/#torchflare.experiments.experiment.Experiment-methods","text":"","title":"Methods"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.__init__","text":"Init method to set up important variables for training and validation. Parameters: Name Type Description Default num_epochs int The number of epochs to save model. required fp16 bool Set this to True if you want to use mixed precision training. False device str The device where you want train your model. 'cuda' seed int The seed to ensure reproducibility. 42","title":"__init__()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.compile_experiment","text":"Configures the model for training and validation. Parameters: Name Type Description Default model Module The model to be trained. required optimizer Union[torch.optim.optimizer.Optimizer, str, Any] The optimizer to be used or name of optimizer. If you pass in the name of the optimizer, only optimizers available in pytorch are supported. required optimizer_params Dict[str, Union[int, float]] The parameters to be used for the optimizer. required criterion Union[Callable[[torch.Tensor], torch.Tensor], str] The loss function to optimize or name of the loss function. If you pass in the name of the loss function, only loss functions available in pytorch can be supported. required callbacks List The list of callbacks to be used. None metrics List The list of metrics to be used. None main_metric Optional[str] The name of main metric to be monitored. Use lower case version. For examples , use 'accuracy' instead of 'Accuracy'. None Note Supports all the schedulers implemented in pytorch/transformers except SWA. Support for custom scheduling will be added soon.","title":"compile_experiment()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.fit","text":"Train and validate the model on training and validation dataset. Parameters: Name Type Description Default x Union[torch.Tensor, numpy.ndarray] A numpy array(or array-like) or torch.tensor for inputs to the model. required y Union[torch.Tensor, numpy.ndarray] Target data. Same type as input data coule numpy array(or array-like) or torch.tensors. required val_data Union[Tuple, List] A tuple or list (x_val , y_val) of numpy arrays or torch.tensors. required batch_size int The batch size to be used for training and validation. 64 dataloader_kwargs Dict Keyword arguments to pass to the PyTorch dataloaders created internally. By default, shuffle=True is passed for the training dataloader but this can be overriden by using this argument. None Note Model will only be saved when ModelCheckpoint callback is used.","title":"fit()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.fit_loader","text":"Train and validate the model using dataloaders. Parameters: Name Type Description Default train_dl DataLoader The training dataloader. required valid_dl DataLoader The validation dataloader. required Note Model will only be saved when ModelCheckpoint callback is used.","title":"fit_loader()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.predict","text":"Method to perform inference on test data. Parameters: Name Type Description Default x Union[torch.Tensor, numpy.ndarray] A numpy array(or array-like) or torch.tensor for inputs to the model. required batch_size int The batch size to be used for inference. 64 device str The device on which you want to perform inference. 'cuda' dataloader_kwargs Dict Keyword arguments to pass to the PyTorch dataloader which is created internally. None path_to_model str str, required","title":"predict()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.predict_on_loader","text":"Method to perform inference on test dataloader. Parameters: Name Type Description Default test_dl DataLoader The dataloader to be use for testing. required device str The device on which you want to perform inference. 'cuda' path_to_model str The full path to model required Yields Output per batch.","title":"predict_on_loader()"},{"location":"experiment/#examples","text":"import torch import torchflare.callbacks as cbs import torchflare.metrics as metrics from torchflare.experiments import Experiment # Defining Training/Validation Dataloaders train_dl = SomeTrainDataloader () valid_dl = SomeValidDataloader () # Defining some basic model model = SomeModel () # Defining params optimizer = \"Adam\" optimizer_params = dict ( lr = 1e-4 ) criterion = \"cross_entropy\" num_epochs = 10 num_classes = 4 # Defining the list of metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False ), ] # Defining the list of callbacks callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 3 ) #Defining Scheduler callback. ] # Creating Experiment and setting the params. exp = Experiment ( num_epochs = num_epochs , save_dir = \"./test_save\" , model_name = \"test_classification.bin\" , fp16 = True , device = device , seed = 42 , compute_train_metrics = False , ) # Compiling the experiment exp . compile_experiment ( model = model , metrics = metric_list , callbacks = callbacks , main_metric = \"accuracy\" , optimizer = optimizer , optimizer_params = optimizer_params , criterion = criterion , ) # Running the experiment exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl )","title":"Examples"},{"location":"Tutorials/Cam-Tutorials/","text":"Basic Tutorial on Interpreters in TorchFlare We will use GradCam and GradCam++ algorithms to visualize what model sees. We will use an image of tigercat for our example. Importing Libraries import torch from torchflare.interpreters import GradCam , GradCamPP , visualize_cam from PIL import Image import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt % matplotlib inline % load_ext nb_black Transforms and Reading the image # Some transforms. normalize = transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) preprocess = transforms . Compose ([ transforms . ToTensor (), normalize ]) # We use PIL to open image image = Image . open ( \"tigercat.jpg\" ) # convert image to tensor tensor = preprocess ( image ) # reshape 4D tensor (N, C, H, W) tensor = tensor . unsqueeze ( 0 ) image = np . array ( image ) plt . imshow ( image ) Defining the model and the target layer. We are using Resnet18 for our experiment here. model = torchvision . models . resnet18 ( pretrained = True ) target_layer = model . layer4 [ 1 ] . conv2 Interpretation using GradCam algorithm Note: Tigercat has label 282 in imagenet dataset. cam_model = GradCam ( model = model , target_layer = target_layer ) cam = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = cam ) Interpretation using GradCam++ algorithm. cam_model = GradCamPP ( model = model , target_layer = target_layer ) campp = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = campp )","title":"GradCam & GradCamPP"},{"location":"Tutorials/Cam-Tutorials/#basic-tutorial-on-interpreters-in-torchflare","text":"We will use GradCam and GradCam++ algorithms to visualize what model sees. We will use an image of tigercat for our example.","title":"Basic Tutorial on Interpreters in TorchFlare"},{"location":"Tutorials/Cam-Tutorials/#importing-libraries","text":"import torch from torchflare.interpreters import GradCam , GradCamPP , visualize_cam from PIL import Image import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt % matplotlib inline % load_ext nb_black","title":"Importing Libraries"},{"location":"Tutorials/Cam-Tutorials/#transforms-and-reading-the-image","text":"# Some transforms. normalize = transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) preprocess = transforms . Compose ([ transforms . ToTensor (), normalize ]) # We use PIL to open image image = Image . open ( \"tigercat.jpg\" ) # convert image to tensor tensor = preprocess ( image ) # reshape 4D tensor (N, C, H, W) tensor = tensor . unsqueeze ( 0 ) image = np . array ( image ) plt . imshow ( image )","title":"Transforms and Reading the image"},{"location":"Tutorials/Cam-Tutorials/#defining-the-model-and-the-target-layer","text":"We are using Resnet18 for our experiment here. model = torchvision . models . resnet18 ( pretrained = True ) target_layer = model . layer4 [ 1 ] . conv2","title":"Defining the model and the target layer."},{"location":"Tutorials/Cam-Tutorials/#interpretation-using-gradcam-algorithm","text":"Note: Tigercat has label 282 in imagenet dataset. cam_model = GradCam ( model = model , target_layer = target_layer ) cam = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = cam )","title":"Interpretation using GradCam algorithm"},{"location":"Tutorials/Cam-Tutorials/#interpretation-using-gradcam-algorithm_1","text":"cam_model = GradCamPP ( model = model , target_layer = target_layer ) campp = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = campp )","title":"Interpretation using GradCam++ algorithm."},{"location":"Tutorials/Imdb_classification/","text":"Text Classfication using TinyBert Dataset: https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format Importing Libraries import pandas as pd from sklearn.model_selection import train_test_split import torch import torch.nn as nn import transformers import torchflare.callbacks as cbs import torchflare.metrics as metrics import torchflare.criterion as crit from torchflare.experiments import Experiment from torchflare.datasets import TextDataloader Reading the data. df = pd . read_csv ( \"Train.csv\" ) Splitting the dataset into train and validation data. train_df , valid_df = train_test_split ( df , stratify = df . label , test_size = 0.1 , random_state = 42 ) Defining training and validation dataloaders. tokenizer = transformers . AutoTokenizer . from_pretrained ( \"prajjwal1/bert-tiny\" ) train_dl = TextDataloader . from_df ( df = train_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 , shuffle = True ) valid_dl = TextDataloader . from_df ( df = valid_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 ) Defining Network architecture. class Model ( torch . nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . bert = transformers . BertModel . from_pretrained ( \"prajjwal1/bert-tiny\" , return_dict = False ) self . bert_drop = nn . Dropout ( 0.3 ) self . out = nn . Linear ( 128 , 1 ) def forward ( self , input_ids , attention_mask , token_type_ids ): _ , o_2 = self . bert ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) b_o = self . bert_drop ( o_2 ) output = self . out ( b_o ) return output model = Model () Defining metrics, callbacks and parameters for optimizer. metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , patience = 2 , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ) ] # I want to define some custom weight decay to model paramters. # We will use model_params as an argument in optimizer_params to tell torchflare that, hey we are using custom optimizer params for model. # If model_params arguments is not used, torchflare by default will use model.parameters() as default params to optimizer. param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] optimizer_parameters = [ { \"params\" : [ p for n , p in param_optimizer if not any ( nd in n for nd in no_decay ) ], \"weight_decay\" : 0.001 , }, { \"params\" : [ p for n , p in param_optimizer if any ( nd in n for nd in no_decay )], \"weight_decay\" : 0.0 , }, ] Defining, compiling and running the experiment. exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"bert_cls.bin\" , fp16 = True , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) # Compiling the experiment exp . compile_experiment ( model = model , optimizer = \"AdamW\" , optimizer_params = dict ( model_params = optimizer_parameters , lr = 3e-5 ), # used model_params argument for custom optimizer params. callbacks = callbacks , criterion = crit . BCEWithLogitsFlat , # Using BCEWithLogitsFlat since I dont want to handle shapes my outputs and targets. metrics = metric_list , main_metric = \"accuracy\" , ) # Training the models. exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl ) Epoch : 1 / 5 Train : 2250 /2250 [=========================]- 88s 39ms/s tep - train_loss : 0.5132 - train_accuracy : 0.7401 Valid : 250 /250 [=========================]- 7s 28ms/s tep - val_loss : 0.4119 - val_accuracy : 0.7472 Epoch : 2 / 5 Train : 2250 /2250 [=========================]- 106s 47ms/s tep - train_loss : 0.3944 - train_accuracy : 0.7834 Valid : 250 /250 [=========================]- 9s 35ms/s tep - val_loss : 0.3906 - val_accuracy : 0.7855 Epoch : 3 / 5 Train : 2250 /2250 [=========================]- 120s 53ms/s tep - train_loss : 0.3471 - train_accuracy : 0.8050 Valid : 250 /250 [=========================]- 9s 37ms/s tep - val_loss : 0.3641 - val_accuracy : 0.8062 Epoch : 4 / 5 Train : 2250 /2250 [=========================]- 130s 58ms/s tep - train_loss : 0.3032 - train_accuracy : 0.8218 Valid : 250 /250 [=========================]- 9s 38ms/s tep - val_loss : 0.3798 - val_accuracy : 0.8221 Epoch : 5 / 5 Train : 2250 /2250 [=========================]- 136s 60ms/s tep - train_loss : 0.2626 - train_accuracy : 0.8349 Valid : 250 /250 [=========================]- 10s 39ms/s tep - val_loss : 0.3886 - val_accuracy : 0.8350 Plotting experiment history. metrics = [ \"loss\" , \"accuracy\" ] for key in metrics : exp . plot_history ( key = key , save_fig = False , plot_fig = True )","title":"Text Classification"},{"location":"Tutorials/Imdb_classification/#text-classfication-using-tinybert","text":"Dataset: https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format","title":"Text Classfication using TinyBert"},{"location":"Tutorials/Imdb_classification/#importing-libraries","text":"import pandas as pd from sklearn.model_selection import train_test_split import torch import torch.nn as nn import transformers import torchflare.callbacks as cbs import torchflare.metrics as metrics import torchflare.criterion as crit from torchflare.experiments import Experiment from torchflare.datasets import TextDataloader","title":"Importing Libraries"},{"location":"Tutorials/Imdb_classification/#reading-the-data","text":"df = pd . read_csv ( \"Train.csv\" )","title":"Reading the data."},{"location":"Tutorials/Imdb_classification/#splitting-the-dataset-into-train-and-validation-data","text":"train_df , valid_df = train_test_split ( df , stratify = df . label , test_size = 0.1 , random_state = 42 )","title":"Splitting the dataset into train and validation data."},{"location":"Tutorials/Imdb_classification/#defining-training-and-validation-dataloaders","text":"tokenizer = transformers . AutoTokenizer . from_pretrained ( \"prajjwal1/bert-tiny\" ) train_dl = TextDataloader . from_df ( df = train_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 , shuffle = True ) valid_dl = TextDataloader . from_df ( df = valid_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 )","title":"Defining training and validation dataloaders."},{"location":"Tutorials/Imdb_classification/#defining-network-architecture","text":"class Model ( torch . nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . bert = transformers . BertModel . from_pretrained ( \"prajjwal1/bert-tiny\" , return_dict = False ) self . bert_drop = nn . Dropout ( 0.3 ) self . out = nn . Linear ( 128 , 1 ) def forward ( self , input_ids , attention_mask , token_type_ids ): _ , o_2 = self . bert ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) b_o = self . bert_drop ( o_2 ) output = self . out ( b_o ) return output model = Model ()","title":"Defining Network architecture."},{"location":"Tutorials/Imdb_classification/#defining-metrics-callbacks-and-parameters-for-optimizer","text":"metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , patience = 2 , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ) ] # I want to define some custom weight decay to model paramters. # We will use model_params as an argument in optimizer_params to tell torchflare that, hey we are using custom optimizer params for model. # If model_params arguments is not used, torchflare by default will use model.parameters() as default params to optimizer. param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] optimizer_parameters = [ { \"params\" : [ p for n , p in param_optimizer if not any ( nd in n for nd in no_decay ) ], \"weight_decay\" : 0.001 , }, { \"params\" : [ p for n , p in param_optimizer if any ( nd in n for nd in no_decay )], \"weight_decay\" : 0.0 , }, ]","title":"Defining metrics, callbacks and parameters for optimizer."},{"location":"Tutorials/Imdb_classification/#defining-compiling-and-running-the-experiment","text":"exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"bert_cls.bin\" , fp16 = True , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) # Compiling the experiment exp . compile_experiment ( model = model , optimizer = \"AdamW\" , optimizer_params = dict ( model_params = optimizer_parameters , lr = 3e-5 ), # used model_params argument for custom optimizer params. callbacks = callbacks , criterion = crit . BCEWithLogitsFlat , # Using BCEWithLogitsFlat since I dont want to handle shapes my outputs and targets. metrics = metric_list , main_metric = \"accuracy\" , ) # Training the models. exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl ) Epoch : 1 / 5 Train : 2250 /2250 [=========================]- 88s 39ms/s tep - train_loss : 0.5132 - train_accuracy : 0.7401 Valid : 250 /250 [=========================]- 7s 28ms/s tep - val_loss : 0.4119 - val_accuracy : 0.7472 Epoch : 2 / 5 Train : 2250 /2250 [=========================]- 106s 47ms/s tep - train_loss : 0.3944 - train_accuracy : 0.7834 Valid : 250 /250 [=========================]- 9s 35ms/s tep - val_loss : 0.3906 - val_accuracy : 0.7855 Epoch : 3 / 5 Train : 2250 /2250 [=========================]- 120s 53ms/s tep - train_loss : 0.3471 - train_accuracy : 0.8050 Valid : 250 /250 [=========================]- 9s 37ms/s tep - val_loss : 0.3641 - val_accuracy : 0.8062 Epoch : 4 / 5 Train : 2250 /2250 [=========================]- 130s 58ms/s tep - train_loss : 0.3032 - train_accuracy : 0.8218 Valid : 250 /250 [=========================]- 9s 38ms/s tep - val_loss : 0.3798 - val_accuracy : 0.8221 Epoch : 5 / 5 Train : 2250 /2250 [=========================]- 136s 60ms/s tep - train_loss : 0.2626 - train_accuracy : 0.8349 Valid : 250 /250 [=========================]- 10s 39ms/s tep - val_loss : 0.3886 - val_accuracy : 0.8350","title":"Defining, compiling and running the experiment."},{"location":"Tutorials/Imdb_classification/#plotting-experiment-history","text":"metrics = [ \"loss\" , \"accuracy\" ] for key in metrics : exp . plot_history ( key = key , save_fig = False , plot_fig = True )","title":"Plotting experiment history."},{"location":"Tutorials/MNIST-Classification/","text":"Image Classification on MNIST We will explore fit method and an interesting callback called DiscordNotifierCallback which will send epoch metrics and history plots to our personal discord server. Importing Libraries import pandas as pd import numpy as np from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics from sklearn.model_selection import train_test_split import os import torch import torch.nn as nn import torch.nn.functional as F Reading data and preprocessing. train_df = pd . read_csv ( \"dataset/train.csv\" ) classes = train_df . label . nunique () train_labels = train_df [ 'label' ] . values train_images = ( train_df . iloc [:, 1 :] . values ) . astype ( 'float32' ) train_images , val_images , train_labels , val_labels = train_test_split ( train_images , train_labels , stratify = train_labels , random_state = 123 , test_size = 0.20 ) train_images = train_images . reshape ( train_images . shape [ 0 ], 1 , 28 , 28 ) val_images = val_images . reshape ( val_images . shape [ 0 ], 1 , 28 , 28 ) train_images = train_images / 255.0 val_images = val_images / 255.0 Defining Network architecture. class Net ( torch . nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Sequential ( nn . Conv2d ( in_channels = 1 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . MaxPool2d ( kernel_size = 2 , stride = 2 ), nn . Dropout2d ( 0.25 ), ) self . conv2 = nn . Sequential ( nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . MaxPool2d ( kernel_size = 2 , stride = 2 ), nn . Dropout2d ( 0.25 ), ) self . fc1 = nn . Sequential ( nn . Dropout ( 0.25 ), nn . Linear ( 7 * 7 * 64 , 256 ), nn . BatchNorm1d ( num_features = 256 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . Dropout ( 0.25 ), ) self . fc2 = nn . Sequential ( nn . Dropout ( 0.25 ), nn . Linear ( 256 , 10 ), ) def forward ( self , x ): x = self . conv1 ( x ) x = self . conv2 ( x ) # Flatten the 3 last dimensions (channels, width, height) to one x = x . view ( x . size ( 0 ), x . size ( 1 ) * x . size ( 2 ) * x . size ( 3 )) x = self . fc1 ( x ) x = self . fc2 ( x ) return x Defining metrics and callbacks. metric_list = [ metrics . Accuracy ( num_classes = classes , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" , patience = 5 ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ), cbs . DiscordNotifierCallback ( exp_name = \"MNIST-EXP\" , webhook_url = os . environ . get ( \"DISCORD_WEBHOOK\" ) , send_figures = True ) ] Defining, compiling the experiment. exp = Experiment ( num_epochs = 3 , fp16 = True , device = \"cuda\" , seed = 42 , ) exp . compile_experiment ( model = Net (), optimizer = \"Adam\" , optimizer_params = dict ( lr = 1e-3 ), callbacks = callbacks , criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) Using the fit method to train our model. exp . fit ( x = train_images , y = train_labels , val_data = ( val_images , val_labels ), batch_size = 32 ) Epoch : 1 / 3 Train : 1050 /1050 [=========================]- 25s 24ms/s tep - train_loss : 0.1929 - train_accuracy : 0.9447 Valid : 263 /263 [=========================]- 1s 5ms/s tep - val_loss : 0.0582 - val_accuracy : 0.9522 Epoch : 2 / 3 Train : 1050 /1050 [=========================]- 25s 23ms/s tep - train_loss : 0.0819 - train_accuracy : 0.9624 Valid : 263 /263 [=========================]- 1s 6ms/s tep - val_loss : 0.0328 - val_accuracy : 0.9652 Epoch : 3 / 3 Train : 1050 /1050 [=========================]- 25s 24ms/s tep - train_loss : 0.0668 - train_accuracy : 0.9691 Valid : 263 /263 [=========================]- 2s 7ms/s tep - val_loss : 0.0344 - val_accuracy : 0.9705 Here is a screenshot of how the progress on discord server looks like.","title":"How to use fit methods"},{"location":"Tutorials/MNIST-Classification/#image-classification-on-mnist","text":"We will explore fit method and an interesting callback called DiscordNotifierCallback which will send epoch metrics and history plots to our personal discord server.","title":"Image Classification on MNIST"},{"location":"Tutorials/MNIST-Classification/#importing-libraries","text":"import pandas as pd import numpy as np from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics from sklearn.model_selection import train_test_split import os import torch import torch.nn as nn import torch.nn.functional as F","title":"Importing Libraries"},{"location":"Tutorials/MNIST-Classification/#reading-data-and-preprocessing","text":"train_df = pd . read_csv ( \"dataset/train.csv\" ) classes = train_df . label . nunique () train_labels = train_df [ 'label' ] . values train_images = ( train_df . iloc [:, 1 :] . values ) . astype ( 'float32' ) train_images , val_images , train_labels , val_labels = train_test_split ( train_images , train_labels , stratify = train_labels , random_state = 123 , test_size = 0.20 ) train_images = train_images . reshape ( train_images . shape [ 0 ], 1 , 28 , 28 ) val_images = val_images . reshape ( val_images . shape [ 0 ], 1 , 28 , 28 ) train_images = train_images / 255.0 val_images = val_images / 255.0","title":"Reading data and preprocessing."},{"location":"Tutorials/MNIST-Classification/#defining-network-architecture","text":"class Net ( torch . nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Sequential ( nn . Conv2d ( in_channels = 1 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . MaxPool2d ( kernel_size = 2 , stride = 2 ), nn . Dropout2d ( 0.25 ), ) self . conv2 = nn . Sequential ( nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( num_features = 64 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . MaxPool2d ( kernel_size = 2 , stride = 2 ), nn . Dropout2d ( 0.25 ), ) self . fc1 = nn . Sequential ( nn . Dropout ( 0.25 ), nn . Linear ( 7 * 7 * 64 , 256 ), nn . BatchNorm1d ( num_features = 256 , eps = 1e-05 , momentum = 0.1 ), nn . ReLU (), nn . Dropout ( 0.25 ), ) self . fc2 = nn . Sequential ( nn . Dropout ( 0.25 ), nn . Linear ( 256 , 10 ), ) def forward ( self , x ): x = self . conv1 ( x ) x = self . conv2 ( x ) # Flatten the 3 last dimensions (channels, width, height) to one x = x . view ( x . size ( 0 ), x . size ( 1 ) * x . size ( 2 ) * x . size ( 3 )) x = self . fc1 ( x ) x = self . fc2 ( x ) return x","title":"Defining Network architecture."},{"location":"Tutorials/MNIST-Classification/#defining-metrics-and-callbacks","text":"metric_list = [ metrics . Accuracy ( num_classes = classes , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" , patience = 5 ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ), cbs . DiscordNotifierCallback ( exp_name = \"MNIST-EXP\" , webhook_url = os . environ . get ( \"DISCORD_WEBHOOK\" ) , send_figures = True ) ]","title":"Defining metrics and callbacks."},{"location":"Tutorials/MNIST-Classification/#defining-compiling-the-experiment","text":"exp = Experiment ( num_epochs = 3 , fp16 = True , device = \"cuda\" , seed = 42 , ) exp . compile_experiment ( model = Net (), optimizer = \"Adam\" , optimizer_params = dict ( lr = 1e-3 ), callbacks = callbacks , criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , )","title":"Defining, compiling the experiment."},{"location":"Tutorials/MNIST-Classification/#using-the-fit-method-to-train-our-model","text":"exp . fit ( x = train_images , y = train_labels , val_data = ( val_images , val_labels ), batch_size = 32 ) Epoch : 1 / 3 Train : 1050 /1050 [=========================]- 25s 24ms/s tep - train_loss : 0.1929 - train_accuracy : 0.9447 Valid : 263 /263 [=========================]- 1s 5ms/s tep - val_loss : 0.0582 - val_accuracy : 0.9522 Epoch : 2 / 3 Train : 1050 /1050 [=========================]- 25s 23ms/s tep - train_loss : 0.0819 - train_accuracy : 0.9624 Valid : 263 /263 [=========================]- 1s 6ms/s tep - val_loss : 0.0328 - val_accuracy : 0.9652 Epoch : 3 / 3 Train : 1050 /1050 [=========================]- 25s 24ms/s tep - train_loss : 0.0668 - train_accuracy : 0.9691 Valid : 263 /263 [=========================]- 2s 7ms/s tep - val_loss : 0.0344 - val_accuracy : 0.9705","title":"Using the fit method to train our model."},{"location":"Tutorials/MNIST-Classification/#here-is-a-screenshot-of-how-the-progress-on-discord-server-looks-like","text":"","title":"Here is a screenshot of how the progress on discord server looks like."},{"location":"Tutorials/image_classification/","text":"Image Classification on CIFAR-10 Dataset : https://www.kaggle.com/c/cifar-10 Importing Libraries import torch from torchflare.datasets import ImageDataloader , show_batch import torchflare.callbacks as cbs import pandas as pd from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.models as models import torchvision.transforms as transforms from torchflare.experiments import Experiment import torchflare.metrics as metrics % load_ext nb_black Reading data and preprocessing. df = pd . read_csv ( \"dataset/trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True ) Splitting data into train and validation test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 ) Defining augmentations train_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 )), ] ) valid_transform = transforms . Compose ([ transforms . ToTensor ()]) Creating Training, validation and Test dataloaders. # Creating Training Dataloader. train_dl = ImageDataloader . from_df ( path = \"dataset/train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = train_transform , df = train_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) # Creating Validation Dataloader. valid_dl = ImageDataloader . from_df ( path = \"dataset/train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = valid_transform , df = valid_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 32 , shuffle = False , num_workers = 0 ) # Creating Test Dataloader. test_dl = ImageDataloader . from_df ( path = \"dataset/train\" , image_col = \"id\" , label_cols = None , # Setting label_cols as None since we wont have labels for test data. augmentations = valid_transform , df = test_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 16 , num_workers = 0 ) Visualizing batch of training data. show_batch ( train_dl ) Clipping input data to the valid range for imshow with RGB data ( [ 0 .. 1 ] for floats or [ 0 .. 255 ] for integers ) . Defining Network architecture. class Net ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () Defining metrics and callbacks. metric_list = [ metrics . Accuracy ( num_classes = len ( classes ), multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" , patience = 5 ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ) ] Defining, compiling and running the experiment. exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"cifar10.bin\" , fp16 = True , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl ) Epoch : 1 / 5 Train : 875 /875 [=========================]- 139s 159ms/s tep - train_loss : 1.8800 - train_accuracy : 0.3141 Valid : 375 /375 [=========================]- 47s 124ms/s tep - val_loss : 2.4238 - val_accuracy : 0.2670 Epoch : 2 / 5 Train : 875 /875 [=========================]- 11s 13ms/s tep - train_loss : 1.5838 - train_accuracy : 0.3309 Valid : 375 /375 [=========================]- 3s 8ms/s tep - val_loss : 2.2545 - val_accuracy : 0.3120 Epoch : 3 / 5 Train : 875 /875 [=========================]- 11s 13ms/s tep - train_loss : 1.4719 - train_accuracy : 0.3528 Valid : 375 /375 [=========================]- 3s 8ms/s tep - val_loss : 2.3558 - val_accuracy : 0.3385 Epoch : 4 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.3983 - train_accuracy : 0.3684 Valid : 375 /375 [=========================]- 3s 8ms/s tep - val_loss : 2.1588 - val_accuracy : 0.3598 Epoch : 5 / 5 Train : 875 /875 [=========================]- 12s 14ms/s tep - train_loss : 1.3391 - train_accuracy : 0.3833 Valid : 375 /375 [=========================]- 3s 9ms/s tep - val_loss : 2.2122 - val_accuracy : 0.3756 Plotting experiment history. keys = [ \"loss\" , \"accuracy\" ] exp . plot_history ( keys = keys , plot_fig = True , save_fig = False )","title":"Image classification"},{"location":"Tutorials/image_classification/#image-classification-on-cifar-10","text":"Dataset : https://www.kaggle.com/c/cifar-10","title":"Image Classification on CIFAR-10"},{"location":"Tutorials/image_classification/#importing-libraries","text":"import torch from torchflare.datasets import ImageDataloader , show_batch import torchflare.callbacks as cbs import pandas as pd from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.models as models import torchvision.transforms as transforms from torchflare.experiments import Experiment import torchflare.metrics as metrics % load_ext nb_black","title":"Importing Libraries"},{"location":"Tutorials/image_classification/#reading-data-and-preprocessing","text":"df = pd . read_csv ( \"dataset/trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True )","title":"Reading data and preprocessing."},{"location":"Tutorials/image_classification/#splitting-data-into-train-and-validation","text":"test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 )","title":"Splitting data into train and validation"},{"location":"Tutorials/image_classification/#defining-augmentations","text":"train_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 )), ] ) valid_transform = transforms . Compose ([ transforms . ToTensor ()])","title":"Defining augmentations"},{"location":"Tutorials/image_classification/#creating-training-validation-and-test-dataloaders","text":"# Creating Training Dataloader. train_dl = ImageDataloader . from_df ( path = \"dataset/train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = train_transform , df = train_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) # Creating Validation Dataloader. valid_dl = ImageDataloader . from_df ( path = \"dataset/train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = valid_transform , df = valid_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 32 , shuffle = False , num_workers = 0 ) # Creating Test Dataloader. test_dl = ImageDataloader . from_df ( path = \"dataset/train\" , image_col = \"id\" , label_cols = None , # Setting label_cols as None since we wont have labels for test data. augmentations = valid_transform , df = test_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 16 , num_workers = 0 )","title":"Creating Training, validation and Test dataloaders."},{"location":"Tutorials/image_classification/#visualizing-batch-of-training-data","text":"show_batch ( train_dl ) Clipping input data to the valid range for imshow with RGB data ( [ 0 .. 1 ] for floats or [ 0 .. 255 ] for integers ) .","title":"Visualizing batch of training data."},{"location":"Tutorials/image_classification/#defining-network-architecture","text":"class Net ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net ()","title":"Defining Network architecture."},{"location":"Tutorials/image_classification/#defining-metrics-and-callbacks","text":"metric_list = [ metrics . Accuracy ( num_classes = len ( classes ), multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" , patience = 5 ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ) ]","title":"Defining metrics and callbacks."},{"location":"Tutorials/image_classification/#defining-compiling-and-running-the-experiment","text":"exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"cifar10.bin\" , fp16 = True , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl ) Epoch : 1 / 5 Train : 875 /875 [=========================]- 139s 159ms/s tep - train_loss : 1.8800 - train_accuracy : 0.3141 Valid : 375 /375 [=========================]- 47s 124ms/s tep - val_loss : 2.4238 - val_accuracy : 0.2670 Epoch : 2 / 5 Train : 875 /875 [=========================]- 11s 13ms/s tep - train_loss : 1.5838 - train_accuracy : 0.3309 Valid : 375 /375 [=========================]- 3s 8ms/s tep - val_loss : 2.2545 - val_accuracy : 0.3120 Epoch : 3 / 5 Train : 875 /875 [=========================]- 11s 13ms/s tep - train_loss : 1.4719 - train_accuracy : 0.3528 Valid : 375 /375 [=========================]- 3s 8ms/s tep - val_loss : 2.3558 - val_accuracy : 0.3385 Epoch : 4 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.3983 - train_accuracy : 0.3684 Valid : 375 /375 [=========================]- 3s 8ms/s tep - val_loss : 2.1588 - val_accuracy : 0.3598 Epoch : 5 / 5 Train : 875 /875 [=========================]- 12s 14ms/s tep - train_loss : 1.3391 - train_accuracy : 0.3833 Valid : 375 /375 [=========================]- 3s 9ms/s tep - val_loss : 2.2122 - val_accuracy : 0.3756","title":"Defining, compiling and running the experiment."},{"location":"Tutorials/image_classification/#plotting-experiment-history","text":"keys = [ \"loss\" , \"accuracy\" ] exp . plot_history ( keys = keys , plot_fig = True , save_fig = False )","title":"Plotting experiment history."},{"location":"Tutorials/image_classification_hydra/","text":"Image Classification on CIFAR-10 with Hydra and TorchFlare Dataset : https://www.kaggle.com/c/cifar-10 Hydra:cc : https://github.com/facebookresearch/hydra We will use Hydra:cc and yaml file to manage our parameters for easy usage. from hydra.experimental import compose , initialize from omegaconf import OmegaConf from hydra.utils import * import torch from torchflare.datasets import ImageDataloader , show_batch import torchflare.callbacks as cbs import pandas as pd from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.models as models import torchvision.transforms as transforms from torchflare.experiments import Experiment % load_ext nb_black Initializing HydraConfig Note compose API works only in jupyter notebook. initialize ( config_path = \"config\" ) cfg = compose ( config_name = \"config\" ) Reading data and preprocessing. df = pd . read_csv ( \"dataset/trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True ) Splitting data into train and validation test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 ) Setting up augmentations and dataloaders. transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) test_transform = transforms . Compose ([ transforms . ToTensor ()]) train_dl = ImageDataloader . from_df ( df = train_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) valid_dl = ImageDataloader . from_df ( df = valid_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = False , num_workers = 0 ) Visualizing batch of training data. show_batch ( train_dl ) Clipping input data to the valid range for imshow with RGB data ( [ 0 .. 1 ] for floats or [ 0 .. 255 ] for integers ) . Defining Network architecture. class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x Defining metrics and callbacks. callbacks = [ instantiate ( cfg . callbacks . early_stopping ), instantiate ( cfg . callbacks . model_checkpoint ), instantiate ( cfg . callbacks . scheduler ), ] metrics = [ instantiate ( cfg . metric )] Setting up experiment. exp = Experiment ( ** cfg . experiment . constant_params ) Compiling the experiment. exp . compile_experiment ( model = Net (), callbacks = callbacks , metrics = metrics , ** cfg . experiment . compile_params ) Running the experiment. exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl ) Epoch : 1 / 5 Train : 875 /875 [=========================]- 183s 209ms/s tep - train_loss : 1.8461 - train_accuracy : 0.3277 Valid : 375 /375 [=========================]- 74s 197ms/s tep - val_loss : 1.6591 - val_accuracy : 0.3475 Epoch : 2 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.5756 - train_accuracy : 0.3811 Valid : 375 /375 [=========================]- 4s 10ms/s tep - val_loss : 1.5218 - val_accuracy : 0.3907 Epoch : 3 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.4696 - train_accuracy : 0.4112 Valid : 375 /375 [=========================]- 4s 10ms/s tep - val_loss : 1.4553 - val_accuracy : 0.4176 Epoch : 4 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.3928 - train_accuracy : 0.4328 Valid : 375 /375 [=========================]- 4s 11ms/s tep - val_loss : 1.3959 - val_accuracy : 0.4379 Epoch : 5 / 5 Train : 875 /875 [=========================]- 13s 15ms/s tep - train_loss : 1.3329 - train_accuracy : 0.4505 Valid : 375 /375 [=========================]- 4s 10ms/s tep - val_loss : 1.3526 - val_accuracy : 0.4540 Plotting experiment history. plot_metrics = [ \"loss\" , \"accuracy\" ] exp . plot_history ( keys = plot_metric , save_fig = False , plot_fig = True )","title":"Parameter management with Hydra"},{"location":"Tutorials/image_classification_hydra/#image-classification-on-cifar-10-with-hydra-and-torchflare","text":"Dataset : https://www.kaggle.com/c/cifar-10 Hydra:cc : https://github.com/facebookresearch/hydra We will use Hydra:cc and yaml file to manage our parameters for easy usage. from hydra.experimental import compose , initialize from omegaconf import OmegaConf from hydra.utils import * import torch from torchflare.datasets import ImageDataloader , show_batch import torchflare.callbacks as cbs import pandas as pd from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.models as models import torchvision.transforms as transforms from torchflare.experiments import Experiment % load_ext nb_black","title":"Image Classification on CIFAR-10 with Hydra and TorchFlare"},{"location":"Tutorials/image_classification_hydra/#initializing-hydraconfig","text":"Note compose API works only in jupyter notebook. initialize ( config_path = \"config\" ) cfg = compose ( config_name = \"config\" )","title":"Initializing HydraConfig"},{"location":"Tutorials/image_classification_hydra/#reading-data-and-preprocessing","text":"df = pd . read_csv ( \"dataset/trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True )","title":"Reading data and preprocessing."},{"location":"Tutorials/image_classification_hydra/#splitting-data-into-train-and-validation","text":"test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 )","title":"Splitting data into train and validation"},{"location":"Tutorials/image_classification_hydra/#setting-up-augmentations-and-dataloaders","text":"transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) test_transform = transforms . Compose ([ transforms . ToTensor ()]) train_dl = ImageDataloader . from_df ( df = train_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) valid_dl = ImageDataloader . from_df ( df = valid_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = False , num_workers = 0 )","title":"Setting up augmentations and dataloaders."},{"location":"Tutorials/image_classification_hydra/#visualizing-batch-of-training-data","text":"show_batch ( train_dl ) Clipping input data to the valid range for imshow with RGB data ( [ 0 .. 1 ] for floats or [ 0 .. 255 ] for integers ) .","title":"Visualizing batch of training data."},{"location":"Tutorials/image_classification_hydra/#defining-network-architecture","text":"class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x","title":"Defining Network architecture."},{"location":"Tutorials/image_classification_hydra/#defining-metrics-and-callbacks","text":"callbacks = [ instantiate ( cfg . callbacks . early_stopping ), instantiate ( cfg . callbacks . model_checkpoint ), instantiate ( cfg . callbacks . scheduler ), ] metrics = [ instantiate ( cfg . metric )]","title":"Defining metrics and callbacks."},{"location":"Tutorials/image_classification_hydra/#setting-up-experiment","text":"exp = Experiment ( ** cfg . experiment . constant_params )","title":"Setting up experiment."},{"location":"Tutorials/image_classification_hydra/#compiling-the-experiment","text":"exp . compile_experiment ( model = Net (), callbacks = callbacks , metrics = metrics , ** cfg . experiment . compile_params )","title":"Compiling the experiment."},{"location":"Tutorials/image_classification_hydra/#running-the-experiment","text":"exp . fit_loader ( train_dl = train_dl , valid_dl = valid_dl ) Epoch : 1 / 5 Train : 875 /875 [=========================]- 183s 209ms/s tep - train_loss : 1.8461 - train_accuracy : 0.3277 Valid : 375 /375 [=========================]- 74s 197ms/s tep - val_loss : 1.6591 - val_accuracy : 0.3475 Epoch : 2 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.5756 - train_accuracy : 0.3811 Valid : 375 /375 [=========================]- 4s 10ms/s tep - val_loss : 1.5218 - val_accuracy : 0.3907 Epoch : 3 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.4696 - train_accuracy : 0.4112 Valid : 375 /375 [=========================]- 4s 10ms/s tep - val_loss : 1.4553 - val_accuracy : 0.4176 Epoch : 4 / 5 Train : 875 /875 [=========================]- 12s 13ms/s tep - train_loss : 1.3928 - train_accuracy : 0.4328 Valid : 375 /375 [=========================]- 4s 11ms/s tep - val_loss : 1.3959 - val_accuracy : 0.4379 Epoch : 5 / 5 Train : 875 /875 [=========================]- 13s 15ms/s tep - train_loss : 1.3329 - train_accuracy : 0.4505 Valid : 375 /375 [=========================]- 4s 10ms/s tep - val_loss : 1.3526 - val_accuracy : 0.4540","title":"Running the experiment."},{"location":"Tutorials/image_classification_hydra/#plotting-experiment-history","text":"plot_metrics = [ \"loss\" , \"accuracy\" ] exp . plot_history ( keys = plot_metric , save_fig = False , plot_fig = True )","title":"Plotting experiment history."},{"location":"Tutorials/tabular_classification/","text":"Classification on Tabular Data Dataset: https://www.kaggle.com/c/cat-in-the-dat-ii We will learn how to perform CrossValidation with torchflare. Importing Libraries import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader import os import copy import traceback import datetime import random from torchflare.experiments import Experiment import torchflare.metrics as metrics import torchflare.callbacks as cbs import torchflare.criterion as crit from torchflare.datasets import TabularDataset , CVSplit % load_ext nb_black Reducing memory usage(some typical kaggle stuff). def reduce_mem_usage ( df , verbose = True ): ''' Reduce file memory usage Source: https://www.kaggle.com/artgor Parameters: ----------- df: DataFrame Dataset on which to perform transformation verbose: bool Print additional information Returns: -------- DataFrame Dataset as pandas DataFrame ''' numerics = [ 'int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64' ] start_mem = df . memory_usage () . sum () / 1024 ** 2 for col in df . columns : col_type = df [ col ] . dtypes if col_type in numerics : c_min = df [ col ] . min () c_max = df [ col ] . max () if str ( col_type )[: 3 ] == 'int' : if c_min > np . iinfo ( np . int8 ) . min and c_max < np . iinfo ( np . int8 ) . max : df [ col ] = df [ col ] . astype ( np . int8 ) elif c_min > np . iinfo ( np . int16 ) . min and c_max < np . iinfo ( np . int16 ) . max : df [ col ] = df [ col ] . astype ( np . int16 ) elif c_min > np . iinfo ( np . int32 ) . min and c_max < np . iinfo ( np . int32 ) . max : df [ col ] = df [ col ] . astype ( np . int32 ) elif c_min > np . iinfo ( np . int64 ) . min and c_max < np . iinfo ( np . int64 ) . max : df [ col ] = df [ col ] . astype ( np . int64 ) else : c_prec = df [ col ] . apply ( lambda x : np . finfo ( x ) . precision ) . max () if c_min > np . finfo ( np . float16 ) . min and c_max < np . finfo ( np . float16 ) \\ . max and c_prec == np . finfo ( np . float16 ) . precision : df [ col ] = df [ col ] . astype ( np . float16 ) elif c_min > np . finfo ( np . float32 ) . min and c_max < np . finfo ( np . float32 ) \\ . max and c_prec == np . finfo ( np . float32 ) . precision : df [ col ] = df [ col ] . astype ( np . float32 ) else : df [ col ] = df [ col ] . astype ( np . float64 ) end_mem = df . memory_usage () . sum () / 1024 ** 2 if verbose : print ( 'Mem. usage decreased to {:5.2f} Mb ( {:.1f}% r eduction)' \\ . format ( end_mem , 100 * ( start_mem - end_mem ) / start_mem )) return ( df ) Reading the data files. train_df = pd . read_csv ( \"train.csv\" ) features = train_df . columns . difference ([ \"id\" , \"target\" ]) . tolist () target = \"target\" Label Encoding Columns label_encoders = {} for cat_col in features : label_encoders [ cat_col ] = LabelEncoder () train_df [ cat_col ] = label_encoders [ cat_col ] . fit_transform ( train_df [ cat_col ] . astype ( \"category\" ) . cat . codes . fillna ( - 1 ) . values ) train_df = reduce_mem_usage ( train_df ) Mem. usage decreased to 19.46 Mb (83.0% reduction) cat_dims = [ int ( train_df [ col ] . nunique ()) for col in features ] emb_dims = [( x , min ( 50 , ( x + 1 ) // 2 )) for x in cat_dims ] Defining the Network architecture. The network uses concept of entity embeddings . class Model ( nn . Module ): def __init__ ( self , emb_dims , lin_layer_sizes , output_size , emb_dropout , lin_layer_dropouts ): \"\"\" emb_dims: List of two element tuples For each categorical feature the first element of a tuple will denote the number of unique values of the categorical feature. The second element will denote the embedding dimension to be used for that feature. \"\"\" super ( Model , self ) . __init__ () # Embedding layers self . emb_layers = nn . ModuleList ([ nn . Embedding ( x , y ) for x , y in emb_dims ]) self . no_of_embs = sum ([ y for x , y in emb_dims ]) # Linear Layers first_lin_layer = nn . Linear ( in_features = self . no_of_embs , out_features = lin_layer_sizes [ 0 ] ) self . lin_layers = nn . ModuleList ( [ first_lin_layer ] + [ nn . Linear ( lin_layer_sizes [ i ], lin_layer_sizes [ i + 1 ]) for i in range ( len ( lin_layer_sizes ) - 1 ) ] ) for lin_layer in self . lin_layers : nn . init . kaiming_normal_ ( lin_layer . weight . data ) # Output Layer self . output_layer = nn . Linear ( lin_layer_sizes [ - 1 ], output_size ) nn . init . kaiming_normal_ ( self . output_layer . weight . data ) # Batch Norm Layers self . first_bn_layer = nn . BatchNorm1d ( self . no_of_embs ) self . bn_layers = nn . ModuleList ( [ nn . BatchNorm1d ( size ) for size in lin_layer_sizes ] ) # Dropout Layers self . emb_dropout_layer = nn . Dropout ( emb_dropout ) self . droput_layers = nn . ModuleList ( [ nn . Dropout ( size ) for size in lin_layer_dropouts ] ) def forward ( self , cat_data ): if self . no_of_embs != 0 : x = [ emb_layer ( cat_data [:, i ]) for i , emb_layer in enumerate ( self . emb_layers ) ] x = torch . cat ( x , 1 ) x = self . first_bn_layer ( x ) x = self . emb_dropout_layer ( x ) for lin_layer , dropout_layer , bn_layer in zip ( self . lin_layers , self . droput_layers , self . bn_layers ): x = F . relu ( lin_layer ( x )) x = dropout_layer ( x ) x = bn_layer ( x ) x = self . output_layer ( x ) return x Defining the cross validation strategy and some dataloader params. The class CVSplit is used to perform cross validation. n_splits = 3 ds = TabularDataset . from_df ( df = train_df , feature_cols = features , label_cols = target ) cv = CVSplit ( dataset = ds , cv = \"StratifiedKFold\" , n_splits = n_splits , shuffle = True , random_state = 42 ) train_dl_params = { \"batch_size\" : 32 , \"shuffle\" : True , \"num_workers\" : 0 } valid_dl_params = { \"batch_size\" : 32 , \"shuffle\" : True , \"num_workers\" : 0 } Running experiment over all folds and visualizing experiment history per fold. for fold in range ( n_splits ): print ( f \"Fold: { fold } \" ) # This where you get a train and val dataloaders for a particular fold. train_dl , valid_dl = cv . get_loaders ( fold = fold , train_params = train_dl_params , val_params = valid_dl_params ) metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False , threshold = 0.6 )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , patience = 3 , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ), ] model = Model ( emb_dims , lin_layer_sizes = [ 200 , 200 ], output_size = 1 , emb_dropout = 0.3 , lin_layer_dropouts = [ 0.3 , 0.3 ], ) exp = Experiment ( num_epochs = 3 , save_dir = \"./models\" , model_name = f \"tabular_cls_ { fold } .bin\" , fp16 = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) exp . compile_experiment ( model = model , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , criterion = crit . BCEWithLogitsFlat , metrics = metric_list , main_metric = \"accuracy\" , ) exp . fit_loader ( train_dl , valid_dl ) keys = [ \"loss\" , \"accuracy\" ] exp . plot_history ( keys = keys , save_fig = False , plot_fig = True ) Fold : 0 Epoch : 1 / 3 Train : 12500 /12500 [=========================]- 82s 7ms/s tep - train_loss : 0.4651 - train_accuracy : 0.8088 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4219 - val_accuracy : 0.8102 Epoch : 2 / 3 Train : 12500 /12500 [=========================]- 89s 7ms/s tep - train_loss : 0.4301 - train_accuracy : 0.8117 Valid : 6250 /6250 [=========================]- 14s 2ms/s tep - val_loss : 0.4109 - val_accuracy : 0.8121 Epoch : 3 / 3 Train : 12500 /12500 [=========================]- 85s 7ms/s tep - train_loss : 0.4212 - train_accuracy : 0.8128 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4048 - val_accuracy : 0.8133 Fold : 1 Epoch : 1 / 3 Train : 12500 /12500 [=========================]- 82s 7ms/s tep - train_loss : 0.4710 - train_accuracy : 0.8088 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4230 - val_accuracy : 0.8102 Epoch : 2 / 3 Train : 12500 /12500 [=========================]- 84s 7ms/s tep - train_loss : 0.4307 - train_accuracy : 0.8117 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4098 - val_accuracy : 0.8124 Epoch : 3 / 3 Train : 12500 /12500 [=========================]- 84s 7ms/s tep - train_loss : 0.4220 - train_accuracy : 0.8130 Valid : 6250 /6250 [=========================]- 12s 2ms/s tep - val_loss : 0.4064 - val_accuracy : 0.8133 Fold : 2 Epoch : 1 / 3 Train : 12500 /12500 [=========================]- 89s 7ms/s tep - train_loss : 0.4702 - train_accuracy : 0.8088 Valid : 6250 /6250 [=========================]- 12s 2ms/s tep - val_loss : 0.4225 - val_accuracy : 0.8103 Epoch : 2 / 3 Train : 12500 /12500 [=========================]- 89s 7ms/s tep - train_loss : 0.4301 - train_accuracy : 0.8118 Valid : 6250 /6250 [=========================]- 12s 2ms/s tep - val_loss : 0.4111 - val_accuracy : 0.8121 Epoch : 3 / 3 Train : 12500 /12500 [=========================]- 91s 7ms/s tep - train_loss : 0.4215 - train_accuracy : 0.8128 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4074 - val_accuracy : 0.8131","title":"Tabular Classification"},{"location":"Tutorials/tabular_classification/#classification-on-tabular-data","text":"Dataset: https://www.kaggle.com/c/cat-in-the-dat-ii We will learn how to perform CrossValidation with torchflare.","title":"Classification on Tabular Data"},{"location":"Tutorials/tabular_classification/#importing-libraries","text":"import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader import os import copy import traceback import datetime import random from torchflare.experiments import Experiment import torchflare.metrics as metrics import torchflare.callbacks as cbs import torchflare.criterion as crit from torchflare.datasets import TabularDataset , CVSplit % load_ext nb_black","title":"Importing Libraries"},{"location":"Tutorials/tabular_classification/#reducing-memory-usagesome-typical-kaggle-stuff","text":"def reduce_mem_usage ( df , verbose = True ): ''' Reduce file memory usage Source: https://www.kaggle.com/artgor Parameters: ----------- df: DataFrame Dataset on which to perform transformation verbose: bool Print additional information Returns: -------- DataFrame Dataset as pandas DataFrame ''' numerics = [ 'int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64' ] start_mem = df . memory_usage () . sum () / 1024 ** 2 for col in df . columns : col_type = df [ col ] . dtypes if col_type in numerics : c_min = df [ col ] . min () c_max = df [ col ] . max () if str ( col_type )[: 3 ] == 'int' : if c_min > np . iinfo ( np . int8 ) . min and c_max < np . iinfo ( np . int8 ) . max : df [ col ] = df [ col ] . astype ( np . int8 ) elif c_min > np . iinfo ( np . int16 ) . min and c_max < np . iinfo ( np . int16 ) . max : df [ col ] = df [ col ] . astype ( np . int16 ) elif c_min > np . iinfo ( np . int32 ) . min and c_max < np . iinfo ( np . int32 ) . max : df [ col ] = df [ col ] . astype ( np . int32 ) elif c_min > np . iinfo ( np . int64 ) . min and c_max < np . iinfo ( np . int64 ) . max : df [ col ] = df [ col ] . astype ( np . int64 ) else : c_prec = df [ col ] . apply ( lambda x : np . finfo ( x ) . precision ) . max () if c_min > np . finfo ( np . float16 ) . min and c_max < np . finfo ( np . float16 ) \\ . max and c_prec == np . finfo ( np . float16 ) . precision : df [ col ] = df [ col ] . astype ( np . float16 ) elif c_min > np . finfo ( np . float32 ) . min and c_max < np . finfo ( np . float32 ) \\ . max and c_prec == np . finfo ( np . float32 ) . precision : df [ col ] = df [ col ] . astype ( np . float32 ) else : df [ col ] = df [ col ] . astype ( np . float64 ) end_mem = df . memory_usage () . sum () / 1024 ** 2 if verbose : print ( 'Mem. usage decreased to {:5.2f} Mb ( {:.1f}% r eduction)' \\ . format ( end_mem , 100 * ( start_mem - end_mem ) / start_mem )) return ( df )","title":"Reducing memory usage(some typical kaggle stuff)."},{"location":"Tutorials/tabular_classification/#reading-the-data-files","text":"train_df = pd . read_csv ( \"train.csv\" ) features = train_df . columns . difference ([ \"id\" , \"target\" ]) . tolist () target = \"target\"","title":"Reading the data files."},{"location":"Tutorials/tabular_classification/#label-encoding-columns","text":"label_encoders = {} for cat_col in features : label_encoders [ cat_col ] = LabelEncoder () train_df [ cat_col ] = label_encoders [ cat_col ] . fit_transform ( train_df [ cat_col ] . astype ( \"category\" ) . cat . codes . fillna ( - 1 ) . values ) train_df = reduce_mem_usage ( train_df ) Mem. usage decreased to 19.46 Mb (83.0% reduction) cat_dims = [ int ( train_df [ col ] . nunique ()) for col in features ] emb_dims = [( x , min ( 50 , ( x + 1 ) // 2 )) for x in cat_dims ]","title":"Label Encoding Columns"},{"location":"Tutorials/tabular_classification/#defining-the-network-architecture","text":"The network uses concept of entity embeddings . class Model ( nn . Module ): def __init__ ( self , emb_dims , lin_layer_sizes , output_size , emb_dropout , lin_layer_dropouts ): \"\"\" emb_dims: List of two element tuples For each categorical feature the first element of a tuple will denote the number of unique values of the categorical feature. The second element will denote the embedding dimension to be used for that feature. \"\"\" super ( Model , self ) . __init__ () # Embedding layers self . emb_layers = nn . ModuleList ([ nn . Embedding ( x , y ) for x , y in emb_dims ]) self . no_of_embs = sum ([ y for x , y in emb_dims ]) # Linear Layers first_lin_layer = nn . Linear ( in_features = self . no_of_embs , out_features = lin_layer_sizes [ 0 ] ) self . lin_layers = nn . ModuleList ( [ first_lin_layer ] + [ nn . Linear ( lin_layer_sizes [ i ], lin_layer_sizes [ i + 1 ]) for i in range ( len ( lin_layer_sizes ) - 1 ) ] ) for lin_layer in self . lin_layers : nn . init . kaiming_normal_ ( lin_layer . weight . data ) # Output Layer self . output_layer = nn . Linear ( lin_layer_sizes [ - 1 ], output_size ) nn . init . kaiming_normal_ ( self . output_layer . weight . data ) # Batch Norm Layers self . first_bn_layer = nn . BatchNorm1d ( self . no_of_embs ) self . bn_layers = nn . ModuleList ( [ nn . BatchNorm1d ( size ) for size in lin_layer_sizes ] ) # Dropout Layers self . emb_dropout_layer = nn . Dropout ( emb_dropout ) self . droput_layers = nn . ModuleList ( [ nn . Dropout ( size ) for size in lin_layer_dropouts ] ) def forward ( self , cat_data ): if self . no_of_embs != 0 : x = [ emb_layer ( cat_data [:, i ]) for i , emb_layer in enumerate ( self . emb_layers ) ] x = torch . cat ( x , 1 ) x = self . first_bn_layer ( x ) x = self . emb_dropout_layer ( x ) for lin_layer , dropout_layer , bn_layer in zip ( self . lin_layers , self . droput_layers , self . bn_layers ): x = F . relu ( lin_layer ( x )) x = dropout_layer ( x ) x = bn_layer ( x ) x = self . output_layer ( x ) return x","title":"Defining the Network architecture."},{"location":"Tutorials/tabular_classification/#defining-the-cross-validation-strategy-and-some-dataloader-params","text":"The class CVSplit is used to perform cross validation. n_splits = 3 ds = TabularDataset . from_df ( df = train_df , feature_cols = features , label_cols = target ) cv = CVSplit ( dataset = ds , cv = \"StratifiedKFold\" , n_splits = n_splits , shuffle = True , random_state = 42 ) train_dl_params = { \"batch_size\" : 32 , \"shuffle\" : True , \"num_workers\" : 0 } valid_dl_params = { \"batch_size\" : 32 , \"shuffle\" : True , \"num_workers\" : 0 }","title":"Defining the cross validation strategy and some dataloader params."},{"location":"Tutorials/tabular_classification/#running-experiment-over-all-folds-and-visualizing-experiment-history-per-fold","text":"for fold in range ( n_splits ): print ( f \"Fold: { fold } \" ) # This where you get a train and val dataloaders for a particular fold. train_dl , valid_dl = cv . get_loaders ( fold = fold , train_params = train_dl_params , val_params = valid_dl_params ) metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False , threshold = 0.6 )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , patience = 3 , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ReduceLROnPlateau ( mode = \"max\" , patience = 2 ), ] model = Model ( emb_dims , lin_layer_sizes = [ 200 , 200 ], output_size = 1 , emb_dropout = 0.3 , lin_layer_dropouts = [ 0.3 , 0.3 ], ) exp = Experiment ( num_epochs = 3 , save_dir = \"./models\" , model_name = f \"tabular_cls_ { fold } .bin\" , fp16 = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) exp . compile_experiment ( model = model , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , criterion = crit . BCEWithLogitsFlat , metrics = metric_list , main_metric = \"accuracy\" , ) exp . fit_loader ( train_dl , valid_dl ) keys = [ \"loss\" , \"accuracy\" ] exp . plot_history ( keys = keys , save_fig = False , plot_fig = True ) Fold : 0 Epoch : 1 / 3 Train : 12500 /12500 [=========================]- 82s 7ms/s tep - train_loss : 0.4651 - train_accuracy : 0.8088 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4219 - val_accuracy : 0.8102 Epoch : 2 / 3 Train : 12500 /12500 [=========================]- 89s 7ms/s tep - train_loss : 0.4301 - train_accuracy : 0.8117 Valid : 6250 /6250 [=========================]- 14s 2ms/s tep - val_loss : 0.4109 - val_accuracy : 0.8121 Epoch : 3 / 3 Train : 12500 /12500 [=========================]- 85s 7ms/s tep - train_loss : 0.4212 - train_accuracy : 0.8128 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4048 - val_accuracy : 0.8133 Fold : 1 Epoch : 1 / 3 Train : 12500 /12500 [=========================]- 82s 7ms/s tep - train_loss : 0.4710 - train_accuracy : 0.8088 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4230 - val_accuracy : 0.8102 Epoch : 2 / 3 Train : 12500 /12500 [=========================]- 84s 7ms/s tep - train_loss : 0.4307 - train_accuracy : 0.8117 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4098 - val_accuracy : 0.8124 Epoch : 3 / 3 Train : 12500 /12500 [=========================]- 84s 7ms/s tep - train_loss : 0.4220 - train_accuracy : 0.8130 Valid : 6250 /6250 [=========================]- 12s 2ms/s tep - val_loss : 0.4064 - val_accuracy : 0.8133 Fold : 2 Epoch : 1 / 3 Train : 12500 /12500 [=========================]- 89s 7ms/s tep - train_loss : 0.4702 - train_accuracy : 0.8088 Valid : 6250 /6250 [=========================]- 12s 2ms/s tep - val_loss : 0.4225 - val_accuracy : 0.8103 Epoch : 2 / 3 Train : 12500 /12500 [=========================]- 89s 7ms/s tep - train_loss : 0.4301 - train_accuracy : 0.8118 Valid : 6250 /6250 [=========================]- 12s 2ms/s tep - val_loss : 0.4111 - val_accuracy : 0.8121 Epoch : 3 / 3 Train : 12500 /12500 [=========================]- 91s 7ms/s tep - train_loss : 0.4215 - train_accuracy : 0.8128 Valid : 6250 /6250 [=========================]- 11s 2ms/s tep - val_loss : 0.4074 - val_accuracy : 0.8131","title":"Running experiment over all folds and visualizing experiment history per fold."},{"location":"callbacks/early_stopping/","text":"Implementation of Early Stopping Callback. Methods __init__ ( self , mode , monitor = 'val_loss' , patience = 5 , min_delta = 1e-07 ) special Constructor for EarlyStopping class. Parameters: Name Type Description Default monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' patience int Number of epochs with no improvement after which training will be stopped. 5 mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. required min_delta float Minimum change in the monitored quantity to qualify as an improvement. 1e-07 EarlyStopping will only use the values of metrics/loss obtained on validation set. Source code in torchflare/callbacks/early_stopping.py def __init__ ( self , mode : str , monitor : str = \"val_loss\" , patience : int = 5 , min_delta : float = 1e-7 , ): \"\"\"Constructor for EarlyStopping class. Args: monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. patience: Number of epochs with no improvement after which training will be stopped. mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. min_delta: Minimum change in the monitored quantity to qualify as an improvement. Note: EarlyStopping will only use the values of metrics/loss obtained on validation set. \"\"\" super ( EarlyStopping , self ) . __init__ ( order = CallbackOrder . STOPPING ) if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor self . patience = patience self . mode = mode self . min_delta = min_delta self . stopping_counter = 0 self . improvement , self . best_score = init_improvement ( mode = self . mode , min_delta = self . min_delta ) self . stopping_counter = 0 on_epoch_end ( self ) Function which will determine when to stop the training depending on the score. Source code in torchflare/callbacks/early_stopping.py def on_epoch_end ( self ): \"\"\"Function which will determine when to stop the training depending on the score.\"\"\" epoch_score = self . exp . exp_logs . get ( self . monitor ) if self . improvement ( epoch_score , self . best_score ): self . best_score = epoch_score self . stopping_counter = 0 else : self . stopping_counter += 1 if self . stopping_counter >= self . patience : print ( \"Early Stopping !\" ) self . exp . stop_training = True on_experiment_end ( self ) Reset to defaults. Source code in torchflare/callbacks/early_stopping.py def on_experiment_end ( self ): \"\"\"Reset to defaults.\"\"\" self . stopping_counter = 0 self . best_score = None self . improvement = None on_experiment_start ( self ) Start of experiment. Source code in torchflare/callbacks/early_stopping.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . stopping_counter = 0 self . best_score = math . inf if self . mode == \"min\" else - math . inf Examples import torchflare.callbacks as cbs early_stop = cbs . EarlyStopping ( monitor = \"val_accuracy\" , patience = 5 , mode = \"max\" )","title":"EarlyStopping"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping-methods","text":"","title":"Methods"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping.__init__","text":"Constructor for EarlyStopping class. Parameters: Name Type Description Default monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' patience int Number of epochs with no improvement after which training will be stopped. 5 mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. required min_delta float Minimum change in the monitored quantity to qualify as an improvement. 1e-07 EarlyStopping will only use the values of metrics/loss obtained on validation set. Source code in torchflare/callbacks/early_stopping.py def __init__ ( self , mode : str , monitor : str = \"val_loss\" , patience : int = 5 , min_delta : float = 1e-7 , ): \"\"\"Constructor for EarlyStopping class. Args: monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. patience: Number of epochs with no improvement after which training will be stopped. mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. min_delta: Minimum change in the monitored quantity to qualify as an improvement. Note: EarlyStopping will only use the values of metrics/loss obtained on validation set. \"\"\" super ( EarlyStopping , self ) . __init__ ( order = CallbackOrder . STOPPING ) if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor self . patience = patience self . mode = mode self . min_delta = min_delta self . stopping_counter = 0 self . improvement , self . best_score = init_improvement ( mode = self . mode , min_delta = self . min_delta ) self . stopping_counter = 0","title":"__init__()"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping.on_epoch_end","text":"Function which will determine when to stop the training depending on the score. Source code in torchflare/callbacks/early_stopping.py def on_epoch_end ( self ): \"\"\"Function which will determine when to stop the training depending on the score.\"\"\" epoch_score = self . exp . exp_logs . get ( self . monitor ) if self . improvement ( epoch_score , self . best_score ): self . best_score = epoch_score self . stopping_counter = 0 else : self . stopping_counter += 1 if self . stopping_counter >= self . patience : print ( \"Early Stopping !\" ) self . exp . stop_training = True","title":"on_epoch_end()"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping.on_experiment_end","text":"Reset to defaults. Source code in torchflare/callbacks/early_stopping.py def on_experiment_end ( self ): \"\"\"Reset to defaults.\"\"\" self . stopping_counter = 0 self . best_score = None self . improvement = None","title":"on_experiment_end()"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping.on_experiment_start","text":"Start of experiment. Source code in torchflare/callbacks/early_stopping.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . stopping_counter = 0 self . best_score = math . inf if self . mode == \"min\" else - math . inf","title":"on_experiment_start()"},{"location":"callbacks/early_stopping/#examples","text":"import torchflare.callbacks as cbs early_stop = cbs . EarlyStopping ( monitor = \"val_accuracy\" , patience = 5 , mode = \"max\" )","title":"Examples"},{"location":"callbacks/model_checkpoint/","text":"Callback for Checkpointing your model. Methods __init__ ( self , mode , monitor = 'val_loss' , save_dir = './models' , file_name = 'model.bin' ) special Constructor for ModelCheckpoint class. Parameters: Name Type Description Default mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. required monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' save_dir str The directory where you want to save the model files. './models' file_name str The name of file. Default : model.bin 'model.bin' ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values : 1 ) ' model_state_dict ' : The state dictionary of model 2 ) ' optimizer_state_dict ' : The state dictionary of optimizer 4 ) ' Epoch ' : The epoch at which model was saved . Model checkpoint will be saved based on the values of metrics / loss obtained from validation set . Source code in torchflare/callbacks/model_checkpoint.py def __init__ ( self , mode : str , monitor : str = \"val_loss\" , save_dir : str = \"./models\" , file_name : str = \"model.bin\" ): \"\"\"Constructor for ModelCheckpoint class. Args: mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. save_dir: The directory where you want to save the model files. file_name: The name of file. Default : model.bin Note: ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values: 1) 'model_state_dict' : The state dictionary of model 2) 'optimizer_state_dict' : The state dictionary of optimizer 4) 'Epoch' : The epoch at which model was saved. Model checkpoint will be saved based on the values of metrics/loss obtained from validation set. \"\"\" super ( ModelCheckpoint , self ) . __init__ ( order = CallbackOrder . INTERNAL ) self . mode = mode self . eps = 1e-7 if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor self . improvement , self . best_val = init_improvement ( mode = self . mode , min_delta = self . eps ) self . path = os . path . join ( save_dir , file_name ) checkpoint ( self , epoch ) Method to save the state dictionaries of model, optimizer,etc. Parameters: Name Type Description Default epoch int The epoch at which model is saved. required Source code in torchflare/callbacks/model_checkpoint.py def checkpoint ( self , epoch : int ): \"\"\"Method to save the state dictionaries of model, optimizer,etc. Args: epoch : The epoch at which model is saved. \"\"\" torch . save ( { \"model_state_dict\" : self . exp . model . state_dict (), \"optimizer_state_dict\" : self . exp . optimizer . state_dict (), \"Epoch\" : epoch , }, self . path , ) on_epoch_end ( self ) Method to save best model depending on the monitored quantity. Source code in torchflare/callbacks/model_checkpoint.py def on_epoch_end ( self ): \"\"\"Method to save best model depending on the monitored quantity.\"\"\" val = self . exp . exp_logs . get ( self . monitor ) if self . improvement ( score = val , best = self . best_val ): self . checkpoint ( epoch = self . exp . exp_logs . get ( self . exp . epoch_key )) on_experiment_end ( self ) Reset to default. Source code in torchflare/callbacks/model_checkpoint.py def on_experiment_end ( self ): \"\"\"Reset to default.\"\"\" if self . mode == \"max\" : self . best_val = - np . inf else : self . best_val = np . inf Examples import torchflare.callbacks as cbs model_ckpt = cbs . ModelCheckpoint ( monitor = \"val_accuracy\" , mode = \"max\" )","title":"ModelCheckpoint"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint-methods","text":"","title":"Methods"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint.__init__","text":"Constructor for ModelCheckpoint class. Parameters: Name Type Description Default mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. required monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' save_dir str The directory where you want to save the model files. './models' file_name str The name of file. Default : model.bin 'model.bin' ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values : 1 ) ' model_state_dict ' : The state dictionary of model 2 ) ' optimizer_state_dict ' : The state dictionary of optimizer 4 ) ' Epoch ' : The epoch at which model was saved . Model checkpoint will be saved based on the values of metrics / loss obtained from validation set . Source code in torchflare/callbacks/model_checkpoint.py def __init__ ( self , mode : str , monitor : str = \"val_loss\" , save_dir : str = \"./models\" , file_name : str = \"model.bin\" ): \"\"\"Constructor for ModelCheckpoint class. Args: mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. save_dir: The directory where you want to save the model files. file_name: The name of file. Default : model.bin Note: ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values: 1) 'model_state_dict' : The state dictionary of model 2) 'optimizer_state_dict' : The state dictionary of optimizer 4) 'Epoch' : The epoch at which model was saved. Model checkpoint will be saved based on the values of metrics/loss obtained from validation set. \"\"\" super ( ModelCheckpoint , self ) . __init__ ( order = CallbackOrder . INTERNAL ) self . mode = mode self . eps = 1e-7 if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor self . improvement , self . best_val = init_improvement ( mode = self . mode , min_delta = self . eps ) self . path = os . path . join ( save_dir , file_name )","title":"__init__()"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint.checkpoint","text":"Method to save the state dictionaries of model, optimizer,etc. Parameters: Name Type Description Default epoch int The epoch at which model is saved. required Source code in torchflare/callbacks/model_checkpoint.py def checkpoint ( self , epoch : int ): \"\"\"Method to save the state dictionaries of model, optimizer,etc. Args: epoch : The epoch at which model is saved. \"\"\" torch . save ( { \"model_state_dict\" : self . exp . model . state_dict (), \"optimizer_state_dict\" : self . exp . optimizer . state_dict (), \"Epoch\" : epoch , }, self . path , )","title":"checkpoint()"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint.on_epoch_end","text":"Method to save best model depending on the monitored quantity. Source code in torchflare/callbacks/model_checkpoint.py def on_epoch_end ( self ): \"\"\"Method to save best model depending on the monitored quantity.\"\"\" val = self . exp . exp_logs . get ( self . monitor ) if self . improvement ( score = val , best = self . best_val ): self . checkpoint ( epoch = self . exp . exp_logs . get ( self . exp . epoch_key ))","title":"on_epoch_end()"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint.on_experiment_end","text":"Reset to default. Source code in torchflare/callbacks/model_checkpoint.py def on_experiment_end ( self ): \"\"\"Reset to default.\"\"\" if self . mode == \"max\" : self . best_val = - np . inf else : self . best_val = np . inf","title":"on_experiment_end()"},{"location":"callbacks/model_checkpoint/#examples","text":"import torchflare.callbacks as cbs model_ckpt = cbs . ModelCheckpoint ( monitor = \"val_accuracy\" , mode = \"max\" )","title":"Examples"},{"location":"callbacks/LRScheduler/cosineannealing_lr/","text":"Set the learning rate of each parameter group using a cosine annealing schedule. Methods __init__ ( self , T_max , eta_min = 0 , last_epoch =- 1 , step_on_batch = True ) special Constructor for CosineAnnealingLR. Parameters: Name Type Description Default T_max int Max number of epochs or iterations. required eta_min float Min learning rate. Defaults to 0. 0 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to True. True Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , T_max : int , eta_min : float = 0 , last_epoch : int = - 1 , step_on_batch : bool = True ): # noqa \"\"\"Constructor for CosineAnnealingLR. Args: T_max (int): Max number of epochs or iterations. eta_min (float, optional): Min learning rate. Defaults to 0. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to True. \"\"\" super () . __init__ ( lambda opt : _schedulers . CosineAnnealingLR ( opt , T_max , eta_min = eta_min , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"CosineAnnealingLR"},{"location":"callbacks/LRScheduler/cosineannealing_lr/#torchflare.callbacks.lr_schedulers.CosineAnnealingLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/cosineannealing_lr/#torchflare.callbacks.lr_schedulers.CosineAnnealingLR.__init__","text":"Constructor for CosineAnnealingLR. Parameters: Name Type Description Default T_max int Max number of epochs or iterations. required eta_min float Min learning rate. Defaults to 0. 0 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to True. True Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , T_max : int , eta_min : float = 0 , last_epoch : int = - 1 , step_on_batch : bool = True ): # noqa \"\"\"Constructor for CosineAnnealingLR. Args: T_max (int): Max number of epochs or iterations. eta_min (float, optional): Min learning rate. Defaults to 0. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to True. \"\"\" super () . __init__ ( lambda opt : _schedulers . CosineAnnealingLR ( opt , T_max , eta_min = eta_min , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"__init__()"},{"location":"callbacks/LRScheduler/cosinewarm_lr/","text":"Set the learning rate of each parameter group using a cosine annealing schedule with a warm restart. Methods __init__ ( self , T_0 , T_mult = 1 , eta_min = 0 , last_epoch =- 1 , step_on_batch = True ) special Constructor for CosineAnnealingWarmRestarts. Parameters: Name Type Description Default T_0 int Number of epochs or iterations for the first restart. required T_mult int T increase factor after a restart. 1 eta_min int Min learning rate. Defaults to 0. 0 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to True. True Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , T_0 : int , T_mult : int = 1 , eta_min : int = 0 , last_epoch : int = - 1 , step_on_batch : bool = True ): # noqa \"\"\"Constructor for CosineAnnealingWarmRestarts. Args: T_0 (int): Number of epochs or iterations for the first restart. T_mult (int): T increase factor after a restart. eta_min (float, optional): Min learning rate. Defaults to 0. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to True. \"\"\" super () . __init__ ( lambda opt : _schedulers . CosineAnnealingWarmRestarts ( opt , T_0 , T_mult = T_mult , eta_min = eta_min , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"CosineAnnealingWarmRestarts"},{"location":"callbacks/LRScheduler/cosinewarm_lr/#torchflare.callbacks.lr_schedulers.CosineAnnealingWarmRestarts-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/cosinewarm_lr/#torchflare.callbacks.lr_schedulers.CosineAnnealingWarmRestarts.__init__","text":"Constructor for CosineAnnealingWarmRestarts. Parameters: Name Type Description Default T_0 int Number of epochs or iterations for the first restart. required T_mult int T increase factor after a restart. 1 eta_min int Min learning rate. Defaults to 0. 0 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to True. True Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , T_0 : int , T_mult : int = 1 , eta_min : int = 0 , last_epoch : int = - 1 , step_on_batch : bool = True ): # noqa \"\"\"Constructor for CosineAnnealingWarmRestarts. Args: T_0 (int): Number of epochs or iterations for the first restart. T_mult (int): T increase factor after a restart. eta_min (float, optional): Min learning rate. Defaults to 0. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to True. \"\"\" super () . __init__ ( lambda opt : _schedulers . CosineAnnealingWarmRestarts ( opt , T_0 , T_mult = T_mult , eta_min = eta_min , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"__init__()"},{"location":"callbacks/LRScheduler/cyclic_lr/","text":"Sets the learning rate of each parameter group according to cyclical learning rate policy. Methods __init__ ( self , base_lr , max_lr , step_size_up = 2000 , step_size_down = None , mode = 'triangular' , gamma = 1.0 , scale_fn = None , scale_mode = 'cycle' , cycle_momentum = True , base_momentum = 0.8 , max_momentum = 0.9 , last_epoch =- 1 , step_on_batch = True ) special Constructor for CyclicLR. Parameters: Name Type Description Default base_lr float Initial learning rate. required max_lr float Max learning rate. required step_size_up int Increase phase duration in epochs or iterations. Defaults to 2000. 2000 step_size_down Optional[int] Decrease phase duration in epochs or iterations. Defaults to None. None mode str Should be 'triangular', 'triangular2' or 'exp_range'. Defaults to 'triangular'. 'triangular' gamma float Constant for the 'exp_range' policy. Defaults to 1. 1.0 scale_fn Optional[Callable[[float], float]] Custom scaling policy function. Defaults to None. None scale_mode str Should be 'cycle' or 'iterations'. Defaults to 'cycle'. 'cycle' cycle_momentum bool Momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. True base_momentum float Lower momentum boundaries in the cycle for each parameter group. Defaults to 0.8. 0.8 max_momentum float Upper momentum boundaries in the cycle for each parameter group. Defaults to 0.9. 0.9 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to True. True Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , base_lr : float , max_lr : float , step_size_up : int = 2000 , step_size_down : Optional [ int ] = None , mode : str = \"triangular\" , gamma : float = 1.0 , scale_fn : Optional [ Callable [[ float ], float ]] = None , scale_mode : str = \"cycle\" , cycle_momentum : bool = True , base_momentum : float = 0.8 , max_momentum : float = 0.9 , last_epoch : int = - 1 , step_on_batch : bool = True , ): \"\"\"Constructor for CyclicLR. Args: base_lr (float or list of float): Initial learning rate. max_lr (float or list of float): Max learning rate. step_size_up (int, optional): Increase phase duration in epochs or iterations. Defaults to 2000. step_size_down (int, optional): Decrease phase duration in epochs or iterations. Defaults to None. mode (str, optional): Should be 'triangular', 'triangular2' or 'exp_range'. Defaults to 'triangular'. gamma (float, optional): Constant for the 'exp_range' policy. Defaults to 1. scale_fn (function, optional): Custom scaling policy function. Defaults to None. scale_mode (str, optional): Should be 'cycle' or 'iterations'. Defaults to 'cycle'. cycle_momentum (bool, optional): Momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. base_momentum (float or list of float, optional): Lower momentum boundaries in the cycle for each parameter group. Defaults to 0.8. max_momentum (float or list of float, optional): Upper momentum boundaries in the cycle for each parameter group. Defaults to 0.9. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to True. \"\"\" super () . __init__ ( lambda opt : _schedulers . CyclicLR ( opt , base_lr , max_lr , step_size_up = step_size_up , step_size_down = step_size_down , mode = mode , gamma = gamma , scale_fn = scale_fn , scale_mode = scale_mode , cycle_momentum = cycle_momentum , base_momentum = base_momentum , max_momentum = max_momentum , last_epoch = last_epoch , ), step_on_batch = step_on_batch , )","title":"CyclicLR"},{"location":"callbacks/LRScheduler/cyclic_lr/#torchflare.callbacks.lr_schedulers.CyclicLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/cyclic_lr/#torchflare.callbacks.lr_schedulers.CyclicLR.__init__","text":"Constructor for CyclicLR. Parameters: Name Type Description Default base_lr float Initial learning rate. required max_lr float Max learning rate. required step_size_up int Increase phase duration in epochs or iterations. Defaults to 2000. 2000 step_size_down Optional[int] Decrease phase duration in epochs or iterations. Defaults to None. None mode str Should be 'triangular', 'triangular2' or 'exp_range'. Defaults to 'triangular'. 'triangular' gamma float Constant for the 'exp_range' policy. Defaults to 1. 1.0 scale_fn Optional[Callable[[float], float]] Custom scaling policy function. Defaults to None. None scale_mode str Should be 'cycle' or 'iterations'. Defaults to 'cycle'. 'cycle' cycle_momentum bool Momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. True base_momentum float Lower momentum boundaries in the cycle for each parameter group. Defaults to 0.8. 0.8 max_momentum float Upper momentum boundaries in the cycle for each parameter group. Defaults to 0.9. 0.9 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to True. True Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , base_lr : float , max_lr : float , step_size_up : int = 2000 , step_size_down : Optional [ int ] = None , mode : str = \"triangular\" , gamma : float = 1.0 , scale_fn : Optional [ Callable [[ float ], float ]] = None , scale_mode : str = \"cycle\" , cycle_momentum : bool = True , base_momentum : float = 0.8 , max_momentum : float = 0.9 , last_epoch : int = - 1 , step_on_batch : bool = True , ): \"\"\"Constructor for CyclicLR. Args: base_lr (float or list of float): Initial learning rate. max_lr (float or list of float): Max learning rate. step_size_up (int, optional): Increase phase duration in epochs or iterations. Defaults to 2000. step_size_down (int, optional): Decrease phase duration in epochs or iterations. Defaults to None. mode (str, optional): Should be 'triangular', 'triangular2' or 'exp_range'. Defaults to 'triangular'. gamma (float, optional): Constant for the 'exp_range' policy. Defaults to 1. scale_fn (function, optional): Custom scaling policy function. Defaults to None. scale_mode (str, optional): Should be 'cycle' or 'iterations'. Defaults to 'cycle'. cycle_momentum (bool, optional): Momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. base_momentum (float or list of float, optional): Lower momentum boundaries in the cycle for each parameter group. Defaults to 0.8. max_momentum (float or list of float, optional): Upper momentum boundaries in the cycle for each parameter group. Defaults to 0.9. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to True. \"\"\" super () . __init__ ( lambda opt : _schedulers . CyclicLR ( opt , base_lr , max_lr , step_size_up = step_size_up , step_size_down = step_size_down , mode = mode , gamma = gamma , scale_fn = scale_fn , scale_mode = scale_mode , cycle_momentum = cycle_momentum , base_momentum = base_momentum , max_momentum = max_momentum , last_epoch = last_epoch , ), step_on_batch = step_on_batch , )","title":"__init__()"},{"location":"callbacks/LRScheduler/exponential_lr/","text":"Multiply learning rate by a given factor on each epoch. Methods __init__ ( self , gamma , last_epoch =- 1 , step_on_batch = False ) special Constructor for ExponentialLR. Parameters: Name Type Description Default gamma float The multiplicative factor. Defaults to 0.1. required last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , gamma : float , last_epoch : int = - 1 , step_on_batch : bool = False ): \"\"\"Constructor for ExponentialLR. Args: gamma (float, optional): The multiplicative factor. Defaults to 0.1. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . ExponentialLR ( opt , gamma , last_epoch = last_epoch ), step_on_batch = step_on_batch )","title":"ExponentialLR"},{"location":"callbacks/LRScheduler/exponential_lr/#torchflare.callbacks.lr_schedulers.ExponentialLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/exponential_lr/#torchflare.callbacks.lr_schedulers.ExponentialLR.__init__","text":"Constructor for ExponentialLR. Parameters: Name Type Description Default gamma float The multiplicative factor. Defaults to 0.1. required last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , gamma : float , last_epoch : int = - 1 , step_on_batch : bool = False ): \"\"\"Constructor for ExponentialLR. Args: gamma (float, optional): The multiplicative factor. Defaults to 0.1. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . ExponentialLR ( opt , gamma , last_epoch = last_epoch ), step_on_batch = step_on_batch )","title":"__init__()"},{"location":"callbacks/LRScheduler/lambda_lr/","text":"Multiply learning rate by a factor computed with a given function. The function should take int value number of epochs as the only argument. Methods __init__ ( self , lr_lambda , last_epoch =- 1 , step_on_batch = False ) special Constructor for lambda scheduler. Parameters: Name Type Description Default lr_lambda Union[Callable[[int], float], List[Callable[[int], float]]] Lambda function for the learning rate factor computation. required last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , lr_lambda : Union [ Callable [[ int ], float ], List [ Callable [[ int ], float ]]], last_epoch : int = - 1 , step_on_batch : bool = False , ): \"\"\"Constructor for lambda scheduler. Args: lr_lambda (function or list of functions): Lambda function for the learning rate factor computation. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . LambdaLR ( opt , lr_lambda , last_epoch = last_epoch ), step_on_batch = step_on_batch )","title":"LambdaLR"},{"location":"callbacks/LRScheduler/lambda_lr/#torchflare.callbacks.lr_schedulers.LambdaLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/lambda_lr/#torchflare.callbacks.lr_schedulers.LambdaLR.__init__","text":"Constructor for lambda scheduler. Parameters: Name Type Description Default lr_lambda Union[Callable[[int], float], List[Callable[[int], float]]] Lambda function for the learning rate factor computation. required last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , lr_lambda : Union [ Callable [[ int ], float ], List [ Callable [[ int ], float ]]], last_epoch : int = - 1 , step_on_batch : bool = False , ): \"\"\"Constructor for lambda scheduler. Args: lr_lambda (function or list of functions): Lambda function for the learning rate factor computation. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . LambdaLR ( opt , lr_lambda , last_epoch = last_epoch ), step_on_batch = step_on_batch )","title":"__init__()"},{"location":"callbacks/LRScheduler/multiplicative_lr/","text":"Multiply the learning rate of each parameter group by the factor given in the specified function. Methods __init__ ( self , lr_lambda , last_epoch =- 1 , step_on_batch = False ) special Constructor for MultiplicativeLR. Parameters: Name Type Description Default lr_lambda Union[Callable[[int], float], List[Callable[[int], float]]] A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in an optimizer.param_groups. required last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , lr_lambda : Union [ Callable [[ int ], float ], List [ Callable [[ int ], float ]]], last_epoch : int = - 1 , step_on_batch : bool = False , ): \"\"\"Constructor for MultiplicativeLR. Args: lr_lambda (function or list of functions): A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in an optimizer.param_groups. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . MultiplicativeLR ( opt , lr_lambda , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"MultiplicativeLR"},{"location":"callbacks/LRScheduler/multiplicative_lr/#torchflare.callbacks.lr_schedulers.MultiplicativeLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/multiplicative_lr/#torchflare.callbacks.lr_schedulers.MultiplicativeLR.__init__","text":"Constructor for MultiplicativeLR. Parameters: Name Type Description Default lr_lambda Union[Callable[[int], float], List[Callable[[int], float]]] A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in an optimizer.param_groups. required last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , lr_lambda : Union [ Callable [[ int ], float ], List [ Callable [[ int ], float ]]], last_epoch : int = - 1 , step_on_batch : bool = False , ): \"\"\"Constructor for MultiplicativeLR. Args: lr_lambda (function or list of functions): A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in an optimizer.param_groups. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . MultiplicativeLR ( opt , lr_lambda , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"__init__()"},{"location":"callbacks/LRScheduler/multistep_lr/","text":"Multiply learning rate by a given factor on each epoch from a given list. Methods __init__ ( self , milestones , gamma = 0.1 , last_epoch =- 1 , step_on_batch = False ) special Constructor class for MultiStepLR. Parameters: Name Type Description Default milestones Iterable[int] List of epochs number to perform lr step. required gamma float The multiplicative factor. Defaults to 0.1. 0.1 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , milestones : Iterable [ int ], gamma : float = 0.1 , last_epoch : int = - 1 , step_on_batch : bool = False ): \"\"\"Constructor class for MultiStepLR. Args: milestones (list of int): List of epochs number to perform lr step. gamma (float, optional): The multiplicative factor. Defaults to 0.1. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . MultiStepLR ( opt , milestones , gamma = gamma , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"MultiStepLR"},{"location":"callbacks/LRScheduler/multistep_lr/#torchflare.callbacks.lr_schedulers.MultiStepLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/multistep_lr/#torchflare.callbacks.lr_schedulers.MultiStepLR.__init__","text":"Constructor class for MultiStepLR. Parameters: Name Type Description Default milestones Iterable[int] List of epochs number to perform lr step. required gamma float The multiplicative factor. Defaults to 0.1. 0.1 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , milestones : Iterable [ int ], gamma : float = 0.1 , last_epoch : int = - 1 , step_on_batch : bool = False ): \"\"\"Constructor class for MultiStepLR. Args: milestones (list of int): List of epochs number to perform lr step. gamma (float, optional): The multiplicative factor. Defaults to 0.1. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . MultiStepLR ( opt , milestones , gamma = gamma , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"__init__()"},{"location":"callbacks/LRScheduler/onecycle_lr/","text":"Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. Methods __init__ ( self , max_lr , total_steps = None , epochs = None , steps_per_epoch = None , pct_start = 0.3 , anneal_strategy = 'cos' , cycle_momentum = True , base_momentum = 0.85 , max_momentum = 0.95 , div_factor = 25.0 , final_div_factor = 10000.0 , last_epoch =- 1 ) special Constructor for OneCycleLR. Parameters: Name Type Description Default max_lr Union[float, List[float]] Upper learning rate boundaries in the cycle for each parameter group. required total_steps Optional[int] The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Defaults to None. None epochs Optional[int] The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. None steps_per_epoch Optional[int] The number of steps per an epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. None pct_start float The percentage of the cycle (in number of steps) spent increasing the learning rate. Defaults to 0.3. 0.3 anneal_strategy str {'cos', 'linear'} Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for linear annealing. Defaults to 'cos'. 'cos' cycle_momentum bool If True , momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. True base_momentum Union[float, List[float]] Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is 'base_momentum' and learning rate is 'max_lr'. Defaults to 0.85. 0.85 max_momentum Union[float, List[float]] Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is 'max_momentum' and learning rate is 'base_lr' Defaults to 0.95. 0.95 div_factor float Determines the initial learning rate via initial_lr = max_lr/div_factor Defaults to 25. 25.0 final_div_factor float Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Defaults to 1e4. 10000.0 last_epoch int The index of last epoch. Default: -1. -1 Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , max_lr : Union [ float , List [ float ]], total_steps : Optional [ int ] = None , epochs : Optional [ int ] = None , steps_per_epoch : Optional [ int ] = None , pct_start : float = 0.3 , anneal_strategy : str = \"cos\" , cycle_momentum : bool = True , base_momentum : Union [ float , List [ float ]] = 0.85 , max_momentum : Union [ float , List [ float ]] = 0.95 , div_factor : float = 25.0 , final_div_factor : float = 1e4 , last_epoch : int = - 1 , ): \"\"\"Constructor for OneCycleLR. Args: max_lr (float or list of float): Upper learning rate boundaries in the cycle for each parameter group. total_steps (int): The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Defaults to None. epochs (int): The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. steps_per_epoch (int): The number of steps per an epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. pct_start (float): The percentage of the cycle (in number of steps) spent increasing the learning rate. Defaults to 0.3. anneal_strategy (str): {'cos', 'linear'} Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for linear annealing. Defaults to 'cos'. cycle_momentum (bool): If ``True``, momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. base_momentum (float or list of float): Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is 'base_momentum' and learning rate is 'max_lr'. Defaults to 0.85. max_momentum (float or list of float): Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is 'max_momentum' and learning rate is 'base_lr' Defaults to 0.95. div_factor (float): Determines the initial learning rate via initial_lr = max_lr/div_factor Defaults to 25. final_div_factor (float): Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Defaults to 1e4. last_epoch (int): The index of last epoch. Default: -1. \"\"\" super () . __init__ ( lambda opt : _schedulers . OneCycleLR ( opt , max_lr , total_steps = total_steps , epochs = epochs , steps_per_epoch = steps_per_epoch , pct_start = pct_start , anneal_strategy = anneal_strategy , cycle_momentum = cycle_momentum , base_momentum = base_momentum , max_momentum = max_momentum , div_factor = div_factor , final_div_factor = final_div_factor , last_epoch = last_epoch , ), step_on_batch = True , )","title":"OneCycleLR"},{"location":"callbacks/LRScheduler/onecycle_lr/#torchflare.callbacks.lr_schedulers.OneCycleLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/onecycle_lr/#torchflare.callbacks.lr_schedulers.OneCycleLR.__init__","text":"Constructor for OneCycleLR. Parameters: Name Type Description Default max_lr Union[float, List[float]] Upper learning rate boundaries in the cycle for each parameter group. required total_steps Optional[int] The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Defaults to None. None epochs Optional[int] The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. None steps_per_epoch Optional[int] The number of steps per an epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. None pct_start float The percentage of the cycle (in number of steps) spent increasing the learning rate. Defaults to 0.3. 0.3 anneal_strategy str {'cos', 'linear'} Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for linear annealing. Defaults to 'cos'. 'cos' cycle_momentum bool If True , momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. True base_momentum Union[float, List[float]] Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is 'base_momentum' and learning rate is 'max_lr'. Defaults to 0.85. 0.85 max_momentum Union[float, List[float]] Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is 'max_momentum' and learning rate is 'base_lr' Defaults to 0.95. 0.95 div_factor float Determines the initial learning rate via initial_lr = max_lr/div_factor Defaults to 25. 25.0 final_div_factor float Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Defaults to 1e4. 10000.0 last_epoch int The index of last epoch. Default: -1. -1 Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , max_lr : Union [ float , List [ float ]], total_steps : Optional [ int ] = None , epochs : Optional [ int ] = None , steps_per_epoch : Optional [ int ] = None , pct_start : float = 0.3 , anneal_strategy : str = \"cos\" , cycle_momentum : bool = True , base_momentum : Union [ float , List [ float ]] = 0.85 , max_momentum : Union [ float , List [ float ]] = 0.95 , div_factor : float = 25.0 , final_div_factor : float = 1e4 , last_epoch : int = - 1 , ): \"\"\"Constructor for OneCycleLR. Args: max_lr (float or list of float): Upper learning rate boundaries in the cycle for each parameter group. total_steps (int): The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Defaults to None. epochs (int): The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. steps_per_epoch (int): The number of steps per an epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Defaults to None. pct_start (float): The percentage of the cycle (in number of steps) spent increasing the learning rate. Defaults to 0.3. anneal_strategy (str): {'cos', 'linear'} Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for linear annealing. Defaults to 'cos'. cycle_momentum (bool): If ``True``, momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Defaults to True. base_momentum (float or list of float): Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is 'base_momentum' and learning rate is 'max_lr'. Defaults to 0.85. max_momentum (float or list of float): Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is 'max_momentum' and learning rate is 'base_lr' Defaults to 0.95. div_factor (float): Determines the initial learning rate via initial_lr = max_lr/div_factor Defaults to 25. final_div_factor (float): Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Defaults to 1e4. last_epoch (int): The index of last epoch. Default: -1. \"\"\" super () . __init__ ( lambda opt : _schedulers . OneCycleLR ( opt , max_lr , total_steps = total_steps , epochs = epochs , steps_per_epoch = steps_per_epoch , pct_start = pct_start , anneal_strategy = anneal_strategy , cycle_momentum = cycle_momentum , base_momentum = base_momentum , max_momentum = max_momentum , div_factor = div_factor , final_div_factor = final_div_factor , last_epoch = last_epoch , ), step_on_batch = True , )","title":"__init__()"},{"location":"callbacks/LRScheduler/reduce_lr/","text":"Reduce learning rate when a metric has stopped improving. Methods __init__ ( self , mode = 'min' , factor = 0.1 , patience = 10 , verbose = False , threshold = 0.0001 , threshold_mode = 'rel' , cooldown = 0 , min_lr = 0 , eps = 1e-08 ) special Constructor for ReduceLRonPlateau. Parameters: Name Type Description Default mode One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. 'min' factor float The multiplicative factor. Defaults to 0.1. 0.1 patience int Number of training epochs without the metric improvement to update the learning rate. Defaults to 10. 10 verbose bool Print info on each update to stdout. Defaults to False. False threshold float Threshold for considering the changes significant. Defaults to 1e-4. 0.0001 threshold_mode str Should be 'rel', 'abs'. Defaults to 'rel'. 'rel' cooldown int Number of epochs to wait before resuming normal operation after lr has been updated. Defaults to 0. 0 min_lr float or list of float Min learning rate. Defaults to 0. 0 eps float Min significant learning rate update. Defaults to 1e-8. 1e-08 Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , mode = \"min\" , factor = 0.1 , patience = 10 , verbose = False , threshold = 1e-4 , threshold_mode = \"rel\" , cooldown = 0 , min_lr = 0 , eps = 1e-8 , ): \"\"\"Constructor for ReduceLRonPlateau. Args: mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. factor (float, optional): The multiplicative factor. Defaults to 0.1. patience (int, optional): Number of training epochs without the metric improvement to update the learning rate. Defaults to 10. verbose (bool, optional): Print info on each update to stdout. Defaults to False. threshold (float, optional): Threshold for considering the changes significant. Defaults to 1e-4. threshold_mode (str, optional): Should be 'rel', 'abs'. Defaults to 'rel'. cooldown (int, optional): Number of epochs to wait before resuming normal operation after lr has been updated. Defaults to 0. min_lr (float or list of float, optional): Min learning rate. Defaults to 0. eps (float, optional): Min significant learning rate update. Defaults to 1e-8. \"\"\" super () . __init__ ( lambda opt : _schedulers . ReduceLROnPlateau ( opt , mode = mode , factor = factor , patience = patience , verbose = verbose , threshold = threshold , threshold_mode = threshold_mode , cooldown = cooldown , min_lr = min_lr , eps = eps , ), step_on_batch = False , )","title":"ReduceLROnPlateau"},{"location":"callbacks/LRScheduler/reduce_lr/#torchflare.callbacks.lr_schedulers.ReduceLROnPlateau-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/reduce_lr/#torchflare.callbacks.lr_schedulers.ReduceLROnPlateau.__init__","text":"Constructor for ReduceLRonPlateau. Parameters: Name Type Description Default mode One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. 'min' factor float The multiplicative factor. Defaults to 0.1. 0.1 patience int Number of training epochs without the metric improvement to update the learning rate. Defaults to 10. 10 verbose bool Print info on each update to stdout. Defaults to False. False threshold float Threshold for considering the changes significant. Defaults to 1e-4. 0.0001 threshold_mode str Should be 'rel', 'abs'. Defaults to 'rel'. 'rel' cooldown int Number of epochs to wait before resuming normal operation after lr has been updated. Defaults to 0. 0 min_lr float or list of float Min learning rate. Defaults to 0. 0 eps float Min significant learning rate update. Defaults to 1e-8. 1e-08 Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , mode = \"min\" , factor = 0.1 , patience = 10 , verbose = False , threshold = 1e-4 , threshold_mode = \"rel\" , cooldown = 0 , min_lr = 0 , eps = 1e-8 , ): \"\"\"Constructor for ReduceLRonPlateau. Args: mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. factor (float, optional): The multiplicative factor. Defaults to 0.1. patience (int, optional): Number of training epochs without the metric improvement to update the learning rate. Defaults to 10. verbose (bool, optional): Print info on each update to stdout. Defaults to False. threshold (float, optional): Threshold for considering the changes significant. Defaults to 1e-4. threshold_mode (str, optional): Should be 'rel', 'abs'. Defaults to 'rel'. cooldown (int, optional): Number of epochs to wait before resuming normal operation after lr has been updated. Defaults to 0. min_lr (float or list of float, optional): Min learning rate. Defaults to 0. eps (float, optional): Min significant learning rate update. Defaults to 1e-8. \"\"\" super () . __init__ ( lambda opt : _schedulers . ReduceLROnPlateau ( opt , mode = mode , factor = factor , patience = patience , verbose = verbose , threshold = threshold , threshold_mode = threshold_mode , cooldown = cooldown , min_lr = min_lr , eps = eps , ), step_on_batch = False , )","title":"__init__()"},{"location":"callbacks/LRScheduler/schedulers/","text":"Wrapper class for scheduler callbacks. Methods __init__ ( self , scheduler , step_on_batch ) special Constructor class for Scheduler callback. Parameters: Name Type Description Default scheduler A pytorch scheduler required step_on_batch bool Whether the scheduler steps after batch or not. required Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , scheduler , step_on_batch : bool ): \"\"\"Constructor class for Scheduler callback. Args: scheduler: A pytorch scheduler step_on_batch: Whether the scheduler steps after batch or not. \"\"\" super ( LRSchedulerCallback , self ) . __init__ ( order = CallbackOrder . INTERNAL ) self . _scheduler = scheduler self . step_on_batch = step_on_batch self . scheduler = None on_batch_end ( self ) Step at end of batch. Source code in torchflare/callbacks/lr_schedulers.py def on_batch_end ( self ): \"\"\"Step at end of batch.\"\"\" if self . scheduler is not None and self . step_on_batch : self . scheduler . step () on_epoch_end ( self ) Step at the end of epoch. Source code in torchflare/callbacks/lr_schedulers.py def on_epoch_end ( self ): \"\"\"Step at the end of epoch.\"\"\" if self . scheduler is not None and not self . step_on_batch : if isinstance ( self . scheduler , _schedulers . ReduceLROnPlateau ): val = self . exp . exp_logs . get ( self . exp . val_key + self . exp . main_metric ) self . scheduler . step ( val ) else : self . scheduler . step () on_experiment_start ( self ) Set scheduler. Source code in torchflare/callbacks/lr_schedulers.py def on_experiment_start ( self ): \"\"\"Set scheduler.\"\"\" if self . scheduler is None : self . scheduler = self . _scheduler ( self . exp . optimizer )","title":"LRSchedulerCallback"},{"location":"callbacks/LRScheduler/schedulers/#torchflare.callbacks.lr_schedulers.LRSchedulerCallback-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/schedulers/#torchflare.callbacks.lr_schedulers.LRSchedulerCallback.__init__","text":"Constructor class for Scheduler callback. Parameters: Name Type Description Default scheduler A pytorch scheduler required step_on_batch bool Whether the scheduler steps after batch or not. required Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , scheduler , step_on_batch : bool ): \"\"\"Constructor class for Scheduler callback. Args: scheduler: A pytorch scheduler step_on_batch: Whether the scheduler steps after batch or not. \"\"\" super ( LRSchedulerCallback , self ) . __init__ ( order = CallbackOrder . INTERNAL ) self . _scheduler = scheduler self . step_on_batch = step_on_batch self . scheduler = None","title":"__init__()"},{"location":"callbacks/LRScheduler/schedulers/#torchflare.callbacks.lr_schedulers.LRSchedulerCallback.on_batch_end","text":"Step at end of batch. Source code in torchflare/callbacks/lr_schedulers.py def on_batch_end ( self ): \"\"\"Step at end of batch.\"\"\" if self . scheduler is not None and self . step_on_batch : self . scheduler . step ()","title":"on_batch_end()"},{"location":"callbacks/LRScheduler/schedulers/#torchflare.callbacks.lr_schedulers.LRSchedulerCallback.on_epoch_end","text":"Step at the end of epoch. Source code in torchflare/callbacks/lr_schedulers.py def on_epoch_end ( self ): \"\"\"Step at the end of epoch.\"\"\" if self . scheduler is not None and not self . step_on_batch : if isinstance ( self . scheduler , _schedulers . ReduceLROnPlateau ): val = self . exp . exp_logs . get ( self . exp . val_key + self . exp . main_metric ) self . scheduler . step ( val ) else : self . scheduler . step ()","title":"on_epoch_end()"},{"location":"callbacks/LRScheduler/schedulers/#torchflare.callbacks.lr_schedulers.LRSchedulerCallback.on_experiment_start","text":"Set scheduler. Source code in torchflare/callbacks/lr_schedulers.py def on_experiment_start ( self ): \"\"\"Set scheduler.\"\"\" if self . scheduler is None : self . scheduler = self . _scheduler ( self . exp . optimizer )","title":"on_experiment_start()"},{"location":"callbacks/LRScheduler/step_lr/","text":"Multiply learning rate by a given factor with a given period. Methods __init__ ( self , step_size , gamma = 0.1 , last_epoch =- 1 , step_on_batch = False ) special Constructor for StepLR. Parameters: Name Type Description Default step_size int Period of learning rate update in epochs. required gamma float The multiplicative factor. Defaults to 0.1. 0.1 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , step_size : int , gamma : float = 0.1 , last_epoch : int = - 1 , step_on_batch : bool = False ): \"\"\"Constructor for StepLR. Args: step_size (int): Period of learning rate update in epochs. gamma (float, optional): The multiplicative factor. Defaults to 0.1. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . StepLR ( opt , step_size , gamma = gamma , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"StepLR"},{"location":"callbacks/LRScheduler/step_lr/#torchflare.callbacks.lr_schedulers.StepLR-methods","text":"","title":"Methods"},{"location":"callbacks/LRScheduler/step_lr/#torchflare.callbacks.lr_schedulers.StepLR.__init__","text":"Constructor for StepLR. Parameters: Name Type Description Default step_size int Period of learning rate update in epochs. required gamma float The multiplicative factor. Defaults to 0.1. 0.1 last_epoch int The index of last epoch. Default: -1. -1 step_on_batch bool Step on each training iteration rather than each epoch. Defaults to False. False Source code in torchflare/callbacks/lr_schedulers.py def __init__ ( self , step_size : int , gamma : float = 0.1 , last_epoch : int = - 1 , step_on_batch : bool = False ): \"\"\"Constructor for StepLR. Args: step_size (int): Period of learning rate update in epochs. gamma (float, optional): The multiplicative factor. Defaults to 0.1. last_epoch (int): The index of last epoch. Default: -1. step_on_batch (bool): Step on each training iteration rather than each epoch. Defaults to False. \"\"\" super () . __init__ ( lambda opt : _schedulers . StepLR ( opt , step_size , gamma = gamma , last_epoch = last_epoch ), step_on_batch = step_on_batch , )","title":"__init__()"},{"location":"callbacks/logging/comet_logger/","text":"Callback to log your metrics and loss values to Comet to track your experiments. For more information about Comet look at Comet.ml Methods __init__ ( self , api_token , params , project_name , workspace , tags ) special Constructor for CometLogger class. Parameters: Name Type Description Default api_token str Your API key obtained from comet.ml required params dict The hyperparameters for your model and experiment as a dictionary required project_name str Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. required workspace str Attach an experiment to a project that belongs to this workspace required tags List[str] List of strings. required Source code in torchflare/callbacks/logging/comet_logger.py def __init__ ( self , api_token : str , params : dict , project_name : str , workspace : str , tags : List [ str ], ): \"\"\"Constructor for CometLogger class. Args: api_token: Your API key obtained from comet.ml params: The hyperparameters for your model and experiment as a dictionary project_name: Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. workspace: Attach an experiment to a project that belongs to this workspace tags: List of strings. \"\"\" super ( CometLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . api_token = api_token self . project_name = project_name self . workspace = workspace self . params = params self . tags = tags self . experiment = None on_epoch_end ( self ) Function to log your metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/comet_logger.py def on_epoch_end ( self ): \"\"\"Function to log your metrics and values at the end of very epoch.\"\"\" logs = { k : v for k , v in self . exp . exp_logs . items () if k != self . exp . epoch_key } self . experiment . log_metrics ( logs , step = self . exp . exp_logs [ self . exp . epoch_key ]) on_experiment_end ( self ) Function to close the experiment when training ends. Source code in torchflare/callbacks/logging/comet_logger.py def on_experiment_end ( self ): \"\"\"Function to close the experiment when training ends.\"\"\" self . experiment . end () self . experiment = None on_experiment_start ( self ) Start of experiment. Source code in torchflare/callbacks/logging/comet_logger.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . experiment = comet_ml . Experiment ( project_name = self . project_name , api_key = self . api_token , workspace = self . workspace , log_code = False , display_summary_level = 0 , ) if self . tags is not None : self . experiment . add_tags ( self . tags ) if self . params is not None : self . experiment . log_parameters ( self . params ) Examples from torchflare.callbacks import CometLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = CometLogger ( project_name = \"experiment_10\" , workspace = \"username\" , params = params , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"CometLogger"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger.__init__","text":"Constructor for CometLogger class. Parameters: Name Type Description Default api_token str Your API key obtained from comet.ml required params dict The hyperparameters for your model and experiment as a dictionary required project_name str Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. required workspace str Attach an experiment to a project that belongs to this workspace required tags List[str] List of strings. required Source code in torchflare/callbacks/logging/comet_logger.py def __init__ ( self , api_token : str , params : dict , project_name : str , workspace : str , tags : List [ str ], ): \"\"\"Constructor for CometLogger class. Args: api_token: Your API key obtained from comet.ml params: The hyperparameters for your model and experiment as a dictionary project_name: Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. workspace: Attach an experiment to a project that belongs to this workspace tags: List of strings. \"\"\" super ( CometLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . api_token = api_token self . project_name = project_name self . workspace = workspace self . params = params self . tags = tags self . experiment = None","title":"__init__()"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger.on_epoch_end","text":"Function to log your metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/comet_logger.py def on_epoch_end ( self ): \"\"\"Function to log your metrics and values at the end of very epoch.\"\"\" logs = { k : v for k , v in self . exp . exp_logs . items () if k != self . exp . epoch_key } self . experiment . log_metrics ( logs , step = self . exp . exp_logs [ self . exp . epoch_key ])","title":"on_epoch_end()"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger.on_experiment_end","text":"Function to close the experiment when training ends. Source code in torchflare/callbacks/logging/comet_logger.py def on_experiment_end ( self ): \"\"\"Function to close the experiment when training ends.\"\"\" self . experiment . end () self . experiment = None","title":"on_experiment_end()"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger.on_experiment_start","text":"Start of experiment. Source code in torchflare/callbacks/logging/comet_logger.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . experiment = comet_ml . Experiment ( project_name = self . project_name , api_key = self . api_token , workspace = self . workspace , log_code = False , display_summary_level = 0 , ) if self . tags is not None : self . experiment . add_tags ( self . tags ) if self . params is not None : self . experiment . log_parameters ( self . params )","title":"on_experiment_start()"},{"location":"callbacks/logging/comet_logger/#examples","text":"from torchflare.callbacks import CometLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = CometLogger ( project_name = \"experiment_10\" , workspace = \"username\" , params = params , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"Examples"},{"location":"callbacks/logging/neptune_logger/","text":"Callback to log your metrics and loss values to Neptune to track your experiments. For more information about Neptune take a look at Neptune Methods __init__ ( self , project_dir , api_token , params = None , experiment_name = None , tags = None ) special Constructor for NeptuneLogger Class. Parameters: Name Type Description Default project_dir str The qualified name of a project in a form of namespace/project_name required params dict he hyperparameters for your model and experiment as a dictionary None experiment_name str The name of the experiment None api_token str User\u2019s API token required tags List[str] List of strings. None Source code in torchflare/callbacks/logging/neptune_logger.py def __init__ ( self , project_dir : str , api_token : str , params : dict = None , experiment_name : str = None , tags : List [ str ] = None , ): \"\"\"Constructor for NeptuneLogger Class. Args: project_dir: The qualified name of a project in a form of namespace/project_name params: he hyperparameters for your model and experiment as a dictionary experiment_name: The name of the experiment api_token: User\u2019s API token tags: List of strings. \"\"\" super ( NeptuneLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . project_dir = project_dir self . api_token = api_token self . params = params self . tags = tags self . experiment_name = experiment_name self . experiment = None on_epoch_end ( self ) Method to log metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/neptune_logger.py def on_epoch_end ( self ): \"\"\"Method to log metrics and values at the end of very epoch.\"\"\" for key , value in self . exp . exp_logs . items (): if key != self . exp . epoch_key : epoch = self . exp . exp_logs [ self . exp . epoch_key ] self . _log_metrics ( name = key , value = value , epoch = epoch ) on_experiment_end ( self ) Method to end experiment after training is done. Source code in torchflare/callbacks/logging/neptune_logger.py def on_experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . stop () self . experiment = None on_experiment_start ( self ) Start of experiment. Source code in torchflare/callbacks/logging/neptune_logger.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . experiment = neptune . init ( project = self . project_dir , api_token = self . api_token , tags = self . tags , name = self . experiment_name ) self . experiment [ \"params\" ] = self . params Examples from torchflare.callbacks import NeptuneLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = NeptuneLogger ( project_dir = \"username/Experiments\" , params = params , experiment_name = \"Experiment_10\" , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"NeptuneLogger"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger.__init__","text":"Constructor for NeptuneLogger Class. Parameters: Name Type Description Default project_dir str The qualified name of a project in a form of namespace/project_name required params dict he hyperparameters for your model and experiment as a dictionary None experiment_name str The name of the experiment None api_token str User\u2019s API token required tags List[str] List of strings. None Source code in torchflare/callbacks/logging/neptune_logger.py def __init__ ( self , project_dir : str , api_token : str , params : dict = None , experiment_name : str = None , tags : List [ str ] = None , ): \"\"\"Constructor for NeptuneLogger Class. Args: project_dir: The qualified name of a project in a form of namespace/project_name params: he hyperparameters for your model and experiment as a dictionary experiment_name: The name of the experiment api_token: User\u2019s API token tags: List of strings. \"\"\" super ( NeptuneLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . project_dir = project_dir self . api_token = api_token self . params = params self . tags = tags self . experiment_name = experiment_name self . experiment = None","title":"__init__()"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger.on_epoch_end","text":"Method to log metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/neptune_logger.py def on_epoch_end ( self ): \"\"\"Method to log metrics and values at the end of very epoch.\"\"\" for key , value in self . exp . exp_logs . items (): if key != self . exp . epoch_key : epoch = self . exp . exp_logs [ self . exp . epoch_key ] self . _log_metrics ( name = key , value = value , epoch = epoch )","title":"on_epoch_end()"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger.on_experiment_end","text":"Method to end experiment after training is done. Source code in torchflare/callbacks/logging/neptune_logger.py def on_experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . stop () self . experiment = None","title":"on_experiment_end()"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger.on_experiment_start","text":"Start of experiment. Source code in torchflare/callbacks/logging/neptune_logger.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . experiment = neptune . init ( project = self . project_dir , api_token = self . api_token , tags = self . tags , name = self . experiment_name ) self . experiment [ \"params\" ] = self . params","title":"on_experiment_start()"},{"location":"callbacks/logging/neptune_logger/#examples","text":"from torchflare.callbacks import NeptuneLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = NeptuneLogger ( project_dir = \"username/Experiments\" , params = params , experiment_name = \"Experiment_10\" , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"Examples"},{"location":"callbacks/logging/tensorboard_logger/","text":"Callback to use Tensorboard to log your metrics and losses. Methods __init__ ( self , log_dir ) special Constructor for TensorboardLogger class. Parameters: Name Type Description Default log_dir str The directory where you want to save your experiments. required Source code in torchflare/callbacks/logging/tensorboard_logger.py def __init__ ( self , log_dir : str ): \"\"\"Constructor for TensorboardLogger class. Args: log_dir: The directory where you want to save your experiments. \"\"\" super ( TensorboardLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . log_dir = log_dir self . _experiment = None on_epoch_end ( self ) Method to log metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/tensorboard_logger.py def on_epoch_end ( self ): \"\"\"Method to log metrics and values at the end of very epoch.\"\"\" for key , value in self . exp . exp_logs . items (): if key != self . exp . epoch_key : epoch = self . exp . exp_logs [ self . exp . epoch_key ] self . _experiment . add_scalar ( tag = key , scalar_value = value , global_step = epoch ) on_experiment_end ( self ) Method to end experiment after training is done. Source code in torchflare/callbacks/logging/tensorboard_logger.py def on_experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . _experiment . close () self . _experiment = None on_experiment_start ( self ) Start of experiment. Source code in torchflare/callbacks/logging/tensorboard_logger.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . _experiment = SummaryWriter ( log_dir = self . log_dir )","title":"TensorboardLogger"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger.__init__","text":"Constructor for TensorboardLogger class. Parameters: Name Type Description Default log_dir str The directory where you want to save your experiments. required Source code in torchflare/callbacks/logging/tensorboard_logger.py def __init__ ( self , log_dir : str ): \"\"\"Constructor for TensorboardLogger class. Args: log_dir: The directory where you want to save your experiments. \"\"\" super ( TensorboardLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . log_dir = log_dir self . _experiment = None","title":"__init__()"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger.on_epoch_end","text":"Method to log metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/tensorboard_logger.py def on_epoch_end ( self ): \"\"\"Method to log metrics and values at the end of very epoch.\"\"\" for key , value in self . exp . exp_logs . items (): if key != self . exp . epoch_key : epoch = self . exp . exp_logs [ self . exp . epoch_key ] self . _experiment . add_scalar ( tag = key , scalar_value = value , global_step = epoch )","title":"on_epoch_end()"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger.on_experiment_end","text":"Method to end experiment after training is done. Source code in torchflare/callbacks/logging/tensorboard_logger.py def on_experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . _experiment . close () self . _experiment = None","title":"on_experiment_end()"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger.on_experiment_start","text":"Start of experiment. Source code in torchflare/callbacks/logging/tensorboard_logger.py def on_experiment_start ( self ): \"\"\"Start of experiment.\"\"\" self . _experiment = SummaryWriter ( log_dir = self . log_dir )","title":"on_experiment_start()"},{"location":"callbacks/logging/wandb_logger/","text":"Callback to log your metrics and loss values to wandb platform. For more information about wandb take a look at Weights and Biases Methods __init__ ( self , project , entity , name = None , config = None , tags = None , notes = None , directory = None ) special Constructor of WandbLogger. Parameters: Name Type Description Default project str The name of the project where you're sending the new run required entity str An entity is a username or team name where you're sending runs. required name str A short display name for this run None config Dict The hyperparameters for your model and experiment as a dictionary None tags List[str] List of strings. None directory str where to save wandb local run directory. If set to None it will use experiments save_dir argument. None notes Optional[str] A longer description of the run, like a -m commit message in git None Note set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log Source code in torchflare/callbacks/logging/wandb_logger.py def __init__ ( self , project : str , entity : str , name : str = None , config : Dict = None , tags : List [ str ] = None , notes : Optional [ str ] = None , directory : str = None , ): \"\"\"Constructor of WandbLogger. Args: project: The name of the project where you're sending the new run entity: An entity is a username or team name where you're sending runs. name: A short display name for this run config: The hyperparameters for your model and experiment as a dictionary tags: List of strings. directory: where to save wandb local run directory. If set to None it will use experiments save_dir argument. notes: A longer description of the run, like a -m commit message in git Note: set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log \"\"\" super ( WandbLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . entity = entity self . project = project self . name = name self . config = config self . tags = tags self . notes = notes self . dir = directory self . experiment = None on_epoch_end ( self ) Method to log metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/wandb_logger.py def on_epoch_end ( self ): \"\"\"Method to log metrics and values at the end of very epoch.\"\"\" logs = { k : v for k , v in self . exp . exp_logs . items () if k != self . exp . epoch_key } self . experiment . log ( logs ) on_experiment_end ( self ) Method to end experiment after training is done. Source code in torchflare/callbacks/logging/wandb_logger.py def on_experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . finish () on_experiment_start ( self ) Experiment start. Source code in torchflare/callbacks/logging/wandb_logger.py def on_experiment_start ( self ): \"\"\"Experiment start.\"\"\" self . experiment = wandb . init ( entity = self . entity , project = self . project , name = self . name , config = self . config , tags = self . tags , notes = self . notes , dir = self . dir , ) Examples from torchflare.callbacks import WandbLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = WandbLogger ( project = \"Experiment\" , entity = \"username\" , name = \"Experiment_10\" , config = params , tags = [ \"Experiment\" , \"fold_0\" ], )","title":"WandbLogger"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger.__init__","text":"Constructor of WandbLogger. Parameters: Name Type Description Default project str The name of the project where you're sending the new run required entity str An entity is a username or team name where you're sending runs. required name str A short display name for this run None config Dict The hyperparameters for your model and experiment as a dictionary None tags List[str] List of strings. None directory str where to save wandb local run directory. If set to None it will use experiments save_dir argument. None notes Optional[str] A longer description of the run, like a -m commit message in git None Note set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log Source code in torchflare/callbacks/logging/wandb_logger.py def __init__ ( self , project : str , entity : str , name : str = None , config : Dict = None , tags : List [ str ] = None , notes : Optional [ str ] = None , directory : str = None , ): \"\"\"Constructor of WandbLogger. Args: project: The name of the project where you're sending the new run entity: An entity is a username or team name where you're sending runs. name: A short display name for this run config: The hyperparameters for your model and experiment as a dictionary tags: List of strings. directory: where to save wandb local run directory. If set to None it will use experiments save_dir argument. notes: A longer description of the run, like a -m commit message in git Note: set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log \"\"\" super ( WandbLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . entity = entity self . project = project self . name = name self . config = config self . tags = tags self . notes = notes self . dir = directory self . experiment = None","title":"__init__()"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger.on_epoch_end","text":"Method to log metrics and values at the end of very epoch. Source code in torchflare/callbacks/logging/wandb_logger.py def on_epoch_end ( self ): \"\"\"Method to log metrics and values at the end of very epoch.\"\"\" logs = { k : v for k , v in self . exp . exp_logs . items () if k != self . exp . epoch_key } self . experiment . log ( logs )","title":"on_epoch_end()"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger.on_experiment_end","text":"Method to end experiment after training is done. Source code in torchflare/callbacks/logging/wandb_logger.py def on_experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . finish ()","title":"on_experiment_end()"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger.on_experiment_start","text":"Experiment start. Source code in torchflare/callbacks/logging/wandb_logger.py def on_experiment_start ( self ): \"\"\"Experiment start.\"\"\" self . experiment = wandb . init ( entity = self . entity , project = self . project , name = self . name , config = self . config , tags = self . tags , notes = self . notes , dir = self . dir , )","title":"on_experiment_start()"},{"location":"callbacks/logging/wandb_logger/#examples","text":"from torchflare.callbacks import WandbLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = WandbLogger ( project = \"Experiment\" , entity = \"username\" , name = \"Experiment_10\" , config = params , tags = [ \"Experiment\" , \"fold_0\" ], )","title":"Examples"},{"location":"callbacks/notifiers/discord_notifier/","text":"Class to Dispatch Training progress and plots to your Discord Sever. Methods __init__ ( self , exp_name , webhook_url , send_figures = False ) special Constructor method for DiscordNotifierCallback. Parameters: Name Type Description Default exp_name str The name of your experiment bot. (Can be anything) required webhook_url str The webhook url of your discord server/channel. required send_figures bool Whether to send the plots of model history to the server. False Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , exp_name : str , webhook_url : str , send_figures : bool = False ): \"\"\"Constructor method for DiscordNotifierCallback. Args: exp_name : The name of your experiment bot. (Can be anything) webhook_url : The webhook url of your discord server/channel. send_figures: Whether to send the plots of model history to the server. \"\"\" super ( DiscordNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . exp_name = exp_name self . webhook_url = webhook_url self . send_figures = send_figures on_epoch_end ( self ) On epoch end dispatch per epoch metrics. Source code in torchflare/callbacks/notifiers/message_notifiers.py def on_epoch_end ( self ): \"\"\"On epoch end dispatch per epoch metrics.\"\"\" data = { \"username\" : self . exp_name , \"embeds\" : [{ \"description\" : prepare_data ( self . exp . exp_logs )}], } response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) try : response . raise_for_status () except requests . exceptions . HTTPError as err : print ( err ) on_experiment_end ( self ) On experiment end dispatch experiment history plots. Source code in torchflare/callbacks/notifiers/message_notifiers.py def on_experiment_end ( self ): \"\"\"On experiment end dispatch experiment history plots.\"\"\" if self . send_figures : self . _send_figs () Examples import torchflare.callbacks as cbs discord_notif = cbs . DiscordNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" , exp_name = \"MODEL_RUN\" )","title":"DiscordNotifierCallback"},{"location":"callbacks/notifiers/discord_notifier/#torchflare.callbacks.notifiers.message_notifiers.DiscordNotifierCallback-methods","text":"","title":"Methods"},{"location":"callbacks/notifiers/discord_notifier/#torchflare.callbacks.notifiers.message_notifiers.DiscordNotifierCallback.__init__","text":"Constructor method for DiscordNotifierCallback. Parameters: Name Type Description Default exp_name str The name of your experiment bot. (Can be anything) required webhook_url str The webhook url of your discord server/channel. required send_figures bool Whether to send the plots of model history to the server. False Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , exp_name : str , webhook_url : str , send_figures : bool = False ): \"\"\"Constructor method for DiscordNotifierCallback. Args: exp_name : The name of your experiment bot. (Can be anything) webhook_url : The webhook url of your discord server/channel. send_figures: Whether to send the plots of model history to the server. \"\"\" super ( DiscordNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . exp_name = exp_name self . webhook_url = webhook_url self . send_figures = send_figures","title":"__init__()"},{"location":"callbacks/notifiers/discord_notifier/#torchflare.callbacks.notifiers.message_notifiers.DiscordNotifierCallback.on_epoch_end","text":"On epoch end dispatch per epoch metrics. Source code in torchflare/callbacks/notifiers/message_notifiers.py def on_epoch_end ( self ): \"\"\"On epoch end dispatch per epoch metrics.\"\"\" data = { \"username\" : self . exp_name , \"embeds\" : [{ \"description\" : prepare_data ( self . exp . exp_logs )}], } response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) try : response . raise_for_status () except requests . exceptions . HTTPError as err : print ( err )","title":"on_epoch_end()"},{"location":"callbacks/notifiers/discord_notifier/#torchflare.callbacks.notifiers.message_notifiers.DiscordNotifierCallback.on_experiment_end","text":"On experiment end dispatch experiment history plots. Source code in torchflare/callbacks/notifiers/message_notifiers.py def on_experiment_end ( self ): \"\"\"On experiment end dispatch experiment history plots.\"\"\" if self . send_figures : self . _send_figs ()","title":"on_experiment_end()"},{"location":"callbacks/notifiers/discord_notifier/#examples","text":"import torchflare.callbacks as cbs discord_notif = cbs . DiscordNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" , exp_name = \"MODEL_RUN\" )","title":"Examples"},{"location":"callbacks/notifiers/slack_notifier/","text":"Class to Dispatch Training progress to your Slack channel. Methods __init__ ( self , webhook_url ) special Constructor method for SlackNotifierCallback. Parameters: Name Type Description Default webhook_url str Slack webhook url required Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , webhook_url : str ): \"\"\"Constructor method for SlackNotifierCallback. Args: webhook_url : Slack webhook url \"\"\" super ( SlackNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . webhook_url = webhook_url on_epoch_end ( self ) This function will dispatch messages to your Slack channel. Source code in torchflare/callbacks/notifiers/message_notifiers.py def on_epoch_end ( self ): \"\"\"This function will dispatch messages to your Slack channel.\"\"\" data = { \"text\" : prepare_data ( self . exp . exp_logs )} response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) if response . status_code != 200 : raise ValueError ( \"Request to server returned an error {} , the response is: \\n {} \" . format ( response . status_code , response . text ) ) Examples import torchflare.callbacks as cbs slack_notif = cbs . SlackNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" )","title":"SlackNotifierCallback"},{"location":"callbacks/notifiers/slack_notifier/#torchflare.callbacks.notifiers.message_notifiers.SlackNotifierCallback-methods","text":"","title":"Methods"},{"location":"callbacks/notifiers/slack_notifier/#torchflare.callbacks.notifiers.message_notifiers.SlackNotifierCallback.__init__","text":"Constructor method for SlackNotifierCallback. Parameters: Name Type Description Default webhook_url str Slack webhook url required Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , webhook_url : str ): \"\"\"Constructor method for SlackNotifierCallback. Args: webhook_url : Slack webhook url \"\"\" super ( SlackNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . webhook_url = webhook_url","title":"__init__()"},{"location":"callbacks/notifiers/slack_notifier/#torchflare.callbacks.notifiers.message_notifiers.SlackNotifierCallback.on_epoch_end","text":"This function will dispatch messages to your Slack channel. Source code in torchflare/callbacks/notifiers/message_notifiers.py def on_epoch_end ( self ): \"\"\"This function will dispatch messages to your Slack channel.\"\"\" data = { \"text\" : prepare_data ( self . exp . exp_logs )} response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) if response . status_code != 200 : raise ValueError ( \"Request to server returned an error {} , the response is: \\n {} \" . format ( response . status_code , response . text ) )","title":"on_epoch_end()"},{"location":"callbacks/notifiers/slack_notifier/#examples","text":"import torchflare.callbacks as cbs slack_notif = cbs . SlackNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" )","title":"Examples"},{"location":"criterion/cross_entropy/","text":"Implements variants for Cross Entropy loss. Classes LabelSmoothingCrossEntropy NLL loss with targets smoothing. Methods __init__ ( self , smoothing = 0.1 ) special Constructor method for LabelSmoothingCrossEntropy. Parameters: Name Type Description Default smoothing float targets smoothing factor 0.1 Exceptions: Type Description ValueError value error is raised if smoothing > 1.0. Source code in torchflare/criterion/cross_entropy.py def __init__ ( self , smoothing : float = 0.1 ): \"\"\"Constructor method for LabelSmoothingCrossEntropy. Args: smoothing : targets smoothing factor Raises: ValueError: value error is raised if smoothing > 1.0. \"\"\" super ( LabelSmoothingCrossEntropy , self ) . __init__ () if smoothing > 1.0 : raise ValueError ( \"Smoothing value must be less than 1.\" ) self . smoothing = smoothing self . confidence = 1.0 - smoothing forward ( self , logits , target ) Forward method. Parameters: Name Type Description Default logits Tensor Raw logits from the net. required target Tensor The targets. required Returns: Type Description Tensor The computed loss value. Source code in torchflare/criterion/cross_entropy.py def forward ( self , logits : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: logits: Raw logits from the net. target: The targets. Returns: The computed loss value. \"\"\" logprobs = F . log_softmax ( logits , dim =- 1 ) nll_loss = - logprobs . gather ( dim =- 1 , index = target . unsqueeze ( 1 )) nll_loss = nll_loss . squeeze ( 1 ) smooth_loss = - logprobs . mean ( dim =- 1 ) loss = self . confidence * nll_loss + self . smoothing * smooth_loss return loss . mean () SymmetricCE Pytorch Implementation of Symmetric Cross Entropy. Paper: https://arxiv.org/abs/1908.06112 Methods __init__ ( self , num_classes , alpha = 1.0 , beta = 1.0 ) special Constructor method for symmetric CE. Parameters: Name Type Description Default alpha float The alpha value for symmetricCE. 1.0 beta float The beta value for symmetricCE. 1.0 num_classes The number of classes. required Source code in torchflare/criterion/cross_entropy.py def __init__ ( self , num_classes , alpha : float = 1.0 , beta : float = 1.0 ): \"\"\"Constructor method for symmetric CE. Args: alpha: The alpha value for symmetricCE. beta: The beta value for symmetricCE. num_classes: The number of classes. \"\"\" super ( SymmetricCE , self ) . __init__ () self . alpha = alpha self . beta = beta self . num_classes = num_classes self . ce = nn . CrossEntropyLoss () forward ( self , logits , targets ) Forward method. Source code in torchflare/criterion/cross_entropy.py def forward ( self , logits : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method.\"\"\" ce = self . ce ( logits , targets ) logits = F . softmax ( logits , dim = 1 ) logits = torch . clamp ( logits , min = 1e-7 , max = 1.0 ) if logits . is_cuda : label_one_hot = torch . nn . functional . one_hot ( targets , self . num_classes ) . float () . cuda () else : label_one_hot = torch . nn . functional . one_hot ( targets , self . num_classes ) label_one_hot = torch . clamp ( label_one_hot , min = 1e-4 , max = 1.0 ) rce = - 1 * torch . sum ( logits * torch . log ( label_one_hot ), dim = 1 ) loss = self . alpha * ce + self . beta * rce . mean () return loss Functions BCEFlat ( x , y ) Same as F.binary_cross_entropy but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" x = torch . sigmoid ( x ) y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy ( x , y ) BCEWithLogitsFlat ( x , y ) Same as F.binary_cross_entropy_with_logits but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEWithLogitsFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy_with_logits but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy_with_logits ( x , y )","title":"Cross Entropy Losses"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy-classes","text":"","title":"Classes"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.LabelSmoothingCrossEntropy","text":"NLL loss with targets smoothing.","title":"LabelSmoothingCrossEntropy"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.LabelSmoothingCrossEntropy-methods","text":"","title":"Methods"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.SymmetricCE","text":"Pytorch Implementation of Symmetric Cross Entropy. Paper: https://arxiv.org/abs/1908.06112","title":"SymmetricCE"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.SymmetricCE-methods","text":"","title":"Methods"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy-functions","text":"","title":"Functions"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.BCEFlat","text":"Same as F.binary_cross_entropy but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" x = torch . sigmoid ( x ) y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy ( x , y )","title":"BCEFlat()"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.BCEWithLogitsFlat","text":"Same as F.binary_cross_entropy_with_logits but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEWithLogitsFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy_with_logits but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy_with_logits ( x , y )","title":"BCEWithLogitsFlat()"},{"location":"criterion/focal_loss/","text":"Implements variants for Focal loss. Classes BCEFocalLoss Implementation of Focal Loss for Binary Classification Problems. Focal loss was proposed in Focal Loss for Dense Object Detection . Methods __init__ ( self , gamma = 0 , eps = 1e-07 , reduction = 'mean' ) special Constructor Method for FocalLoss class. Parameters: Name Type Description Default gamma The focal parameter. Defaults to 0. 0 eps Constant for computational stability. 1e-07 reduction The reduction parameter for Cross Entropy Loss. 'mean' Source code in torchflare/criterion/focal_loss.py def __init__ ( self , gamma = 0 , eps = 1e-7 , reduction = \"mean\" ): \"\"\"Constructor Method for FocalLoss class. Args: gamma : The focal parameter. Defaults to 0. eps : Constant for computational stability. reduction: The reduction parameter for Cross Entropy Loss. \"\"\" super ( BCEFocalLoss , self ) . __init__ () self . gamma = gamma self . reduction = reduction self . eps = eps self . bce = torch . nn . BCEWithLogitsLoss ( reduction = \"none\" ) forward ( self , logits , targets ) Forward method. Parameters: Name Type Description Default logits Tensor The raw logits from the network of shape (N,*) where C = number of classes , * = extra dims required targets Tensor The targets required Returns: Type Description Tensor The computed loss value Source code in torchflare/criterion/focal_loss.py def forward ( self , logits : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: logits: The raw logits from the network of shape (N,*) where C = number of classes , * = extra dims targets: The targets Returns: The computed loss value \"\"\" targets = targets . view ( logits . shape ) logp = self . bce ( logits , targets ) p = torch . exp ( - logp ) loss = ( 1 - p ) ** self . gamma * logp return loss . mean () if self . reduction == \"mean\" else loss . sum () if self . reduction == \"sum\" else loss FocalCosineLoss Implementation Focal cosine loss. Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble . Source : https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/203271 Methods __init__ ( self , alpha = 1 , gamma = 2 , xent = 0.1 , reduction = 'mean' ) special Constructor for FocalCosineLoss. Todo: Add documentation. Source code in torchflare/criterion/focal_loss.py def __init__ ( self , alpha : float = 1 , gamma : float = 2 , xent : float = 0.1 , reduction = \"mean\" ): \"\"\"Constructor for FocalCosineLoss. Todo: Add documentation. \"\"\" super ( FocalCosineLoss , self ) . __init__ () self . alpha = alpha self . gamma = gamma self . xent = xent self . reduction = reduction forward ( self , logits , target ) Forward Method. Source code in torchflare/criterion/focal_loss.py def forward ( self , logits : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method.\"\"\" cosine_loss = F . cosine_embedding_loss ( logits , torch . nn . functional . one_hot ( target , num_classes = logits . size ( - 1 )), torch . tensor ([ 1 ], device = target . device ), reduction = self . reduction , ) cent_loss = F . cross_entropy ( F . normalize ( logits ), target , reduction = \"none\" ) pt = torch . exp ( - cent_loss ) focal_loss = self . alpha * ( 1 - pt ) ** self . gamma * cent_loss if self . reduction == \"mean\" : focal_loss = torch . mean ( focal_loss ) return cosine_loss + self . xent * focal_loss FocalLoss Implementation of Focal Loss. Focal loss was proposed in Focal Loss for Dense Object Detection . Methods __init__ ( self , gamma = 0 , eps = 1e-07 , reduction = 'mean' ) special Constructor Method for FocalLoss class. Parameters: Name Type Description Default gamma The focal parameter. Defaults to 0. 0 eps Constant for computational stability. 1e-07 reduction The reduction parameter for Cross Entropy Loss. 'mean' Source code in torchflare/criterion/focal_loss.py def __init__ ( self , gamma = 0 , eps = 1e-7 , reduction = \"mean\" ): \"\"\"Constructor Method for FocalLoss class. Args: gamma : The focal parameter. Defaults to 0. eps : Constant for computational stability. reduction: The reduction parameter for Cross Entropy Loss. \"\"\" super ( FocalLoss , self ) . __init__ () self . gamma = gamma self . reduction = reduction self . eps = eps self . ce = torch . nn . CrossEntropyLoss ( reduction = \"none\" ) forward ( self , logits , targets ) Forward method. Parameters: Name Type Description Default logits Tensor The raw logits from the network of shape (N,C,*) where C = number of classes , * = extra dims required targets Tensor The targets of shape (N , *). required Returns: Type Description Tensor The computed loss value Source code in torchflare/criterion/focal_loss.py def forward ( self , logits : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: logits: The raw logits from the network of shape (N,C,*) where C = number of classes , * = extra dims targets: The targets of shape (N , *). Returns: The computed loss value \"\"\" logp = self . ce ( logits , targets ) p = torch . exp ( - logp ) loss = ( 1 - p ) ** self . gamma * logp return loss . mean () if self . reduction == \"mean\" else loss . sum () if self . reduction == \"sum\" else loss","title":"Focal Loss variants"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss-classes","text":"","title":"Classes"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.BCEFocalLoss","text":"Implementation of Focal Loss for Binary Classification Problems. Focal loss was proposed in Focal Loss for Dense Object Detection .","title":"BCEFocalLoss"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.BCEFocalLoss-methods","text":"","title":"Methods"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalCosineLoss","text":"Implementation Focal cosine loss. Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble . Source : https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/203271","title":"FocalCosineLoss"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalCosineLoss-methods","text":"","title":"Methods"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalLoss","text":"Implementation of Focal Loss. Focal loss was proposed in Focal Loss for Dense Object Detection .","title":"FocalLoss"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalLoss-methods","text":"","title":"Methods"},{"location":"criterion/segmentation/","text":"Implements IouLoss. Classes IOULoss Computes intersection over union Loss. IOULoss = 1 - iou_score Methods __init__ ( self , class_dim = 1 ) special Constructor method for IOULoss. Parameters: Name Type Description Default class_dim indicates class dimension (K) for outputs and targets tensors (default = 1) 1 Source code in torchflare/criterion/iou_loss.py def __init__ ( self , class_dim = 1 ): \"\"\"Constructor method for IOULoss. Args: class_dim: indicates class dimension (K) for outputs and targets tensors (default = 1) \"\"\" super ( IOULoss , self ) . __init__ () self . iou = IOU ( threshold = None , class_dim = class_dim ) forward ( self , outputs , targets ) Forward Method. Parameters: Name Type Description Default outputs Tensor outputs from the net after applying activations. required targets Tensor The targets. required Returns: Type Description Tensor The computed loss value. Source code in torchflare/criterion/iou_loss.py def forward ( self , outputs : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method. Args: outputs: outputs from the net after applying activations. targets: The targets. Returns: The computed loss value. \"\"\" self . iou . reset () self . iou . accumulate ( outputs = outputs , targets = targets ) return 1 - self . iou . value Implements DiceLoss. Classes DiceLoss Implementation of Dice Loss. Methods __init__ ( self , class_dim = 1 ) special Constructor method for Dice Loss. Parameters: Name Type Description Default class_dim The dimension indication class. 1 Source code in torchflare/criterion/dice_loss.py def __init__ ( self , class_dim = 1 ): \"\"\"Constructor method for Dice Loss. Args: class_dim: The dimension indication class. \"\"\" super ( DiceLoss , self ) . __init__ () self . dice = DiceScore ( threshold = None , class_dim = class_dim ) forward ( self , outputs , targets ) Forward method. Parameters: Name Type Description Default outputs Tensor outputs from the net after applying activations. required targets Tensor The targets. required Returns: Type Description Tensor The computed loss value. Source code in torchflare/criterion/dice_loss.py def forward ( self , outputs : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: outputs: outputs from the net after applying activations. targets: The targets. Returns: The computed loss value. \"\"\" self . dice . reset () self . dice . accumulate ( outputs = outputs , targets = targets ) return 1 - self . dice . value","title":"Segmentation Losses"},{"location":"criterion/segmentation/#torchflare.criterion.iou_loss-classes","text":"","title":"Classes"},{"location":"criterion/segmentation/#torchflare.criterion.iou_loss.IOULoss","text":"Computes intersection over union Loss. IOULoss = 1 - iou_score","title":"IOULoss"},{"location":"criterion/segmentation/#torchflare.criterion.iou_loss.IOULoss-methods","text":"","title":"Methods"},{"location":"criterion/segmentation/#torchflare.criterion.dice_loss-classes","text":"","title":"Classes"},{"location":"criterion/segmentation/#torchflare.criterion.dice_loss.DiceLoss","text":"Implementation of Dice Loss.","title":"DiceLoss"},{"location":"criterion/segmentation/#torchflare.criterion.dice_loss.DiceLoss-methods","text":"","title":"Methods"},{"location":"criterion/triplet/","text":"Implements triplet loss. Classes TripletLoss Computes Triplet loss. Methods __init__ ( self , normalize_features = True , margin = None , hard_mining = True ) special Constructor method for TripletLoss. Parameters: Name Type Description Default normalize_features bool Whether to normalize the features. Default = True True margin float The value for margin. Default = None. None hard_mining bool Whether to use hard sample mining. Default = True. True Source code in torchflare/criterion/triplet_loss.py def __init__ ( self , normalize_features : bool = True , margin : float = None , hard_mining : bool = True , ): \"\"\"Constructor method for TripletLoss. Args: normalize_features: Whether to normalize the features. Default = True margin: The value for margin. Default = None. hard_mining: Whether to use hard sample mining. Default = True. \"\"\" super ( TripletLoss , self ) . __init__ () self . normalize_features = normalize_features self . margin = margin self . hard_mining = hard_mining forward ( self , embedding , targets ) Forward Method. Parameters: Name Type Description Default embedding Tensor The output of the network. required targets Tensor The targets. required Returns: Type Description Tensor The computed Triplet Loss. Source code in torchflare/criterion/triplet_loss.py def forward ( self , embedding : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method. Args: embedding: The output of the network. targets: The targets. Returns: The computed Triplet Loss. \"\"\" distance_matrix = ( cosine_dist ( embedding , embedding ) if self . normalize_features else euclidean_dist ( embedding , embedding ) ) n = distance_matrix . size ( 0 ) pos_idxs = targets . view ( n , 1 ) . expand ( n , n ) . eq ( targets . view ( n , 1 ) . expand ( n , n ) . t ()) . float () neg_idxs = targets . view ( n , 1 ) . expand ( n , n ) . ne ( targets . view ( n , 1 ) . expand ( n , n ) . t ()) . float () if self . hard_mining : dist_ap , dist_an = hard_example_mining ( distance_matrix = distance_matrix , pos_idxs = pos_idxs , neg_idxs = neg_idxs ) else : dist_ap , dist_an = weighted_example_mining ( distance_matrix = distance_matrix , pos_idxs = pos_idxs , neg_idxs = neg_idxs ) y = dist_an . new () . resize_as_ ( dist_an ) . fill_ ( 1 ) if self . margin is not None and self . margin > 0 : loss = F . margin_ranking_loss ( dist_an , dist_ap , y , margin = self . margin ) else : loss = F . soft_margin_loss ( dist_an - dist_ap , y ) # fmt: off if loss == float ( \"Inf\" ): loss = F . margin_ranking_loss ( dist_an , dist_ap , y , margin = 0.3 ) # fmt: on return loss Functions hard_example_mining ( distance_matrix , pos_idxs , neg_idxs ) For each anchor, find the hardest positive and negative sample. Parameters: Name Type Description Default distance_matrix pair wise distance between samples, shape [N, M] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. Source code in torchflare/criterion/triplet_loss.py def hard_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the hardest positive and negative sample. Args: distance_matrix: pair wise distance between samples, shape [N, M] pos_idxs: positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note: Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 # `dist_ap` means distance(anchor, positive) # both `dist_ap` and `relative_p_inds` with shape [N] dist_ap , _ = torch . max ( distance_matrix * pos_idxs , dim = 1 ) # `dist_an` means distance(anchor, negative) # both `dist_an` and `relative_n_inds` with shape [N] dist_an , _ = torch . min ( distance_matrix * neg_idxs + pos_idxs * 99999999.0 , dim = 1 ) return dist_ap , dist_an weighted_example_mining ( distance_matrix , pos_idxs , neg_idxs ) For each anchor, find the weighted positive and negative sample. Parameters: Name Type Description Default distance_matrix pytorch Variable, pair wise distance between samples, shape [N, N] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] Source code in torchflare/criterion/triplet_loss.py def weighted_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the weighted positive and negative sample. Args: distance_matrix: pytorch Variable, pair wise distance between samples, shape [N, N] pos_idxs:positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 dist_ap = distance_matrix * pos_idxs dist_an = distance_matrix * neg_idxs weights_ap = softmax_weights ( dist_ap , pos_idxs ) weights_an = softmax_weights ( - dist_an , neg_idxs ) dist_ap = torch . sum ( dist_ap * weights_ap , dim = 1 ) dist_an = torch . sum ( dist_an * weights_an , dim = 1 ) return dist_ap , dist_an","title":"Triplet Loss"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss-classes","text":"","title":"Classes"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.TripletLoss","text":"Computes Triplet loss.","title":"TripletLoss"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.TripletLoss-methods","text":"","title":"Methods"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss-functions","text":"","title":"Functions"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.hard_example_mining","text":"For each anchor, find the hardest positive and negative sample. Parameters: Name Type Description Default distance_matrix pair wise distance between samples, shape [N, M] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. Source code in torchflare/criterion/triplet_loss.py def hard_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the hardest positive and negative sample. Args: distance_matrix: pair wise distance between samples, shape [N, M] pos_idxs: positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note: Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 # `dist_ap` means distance(anchor, positive) # both `dist_ap` and `relative_p_inds` with shape [N] dist_ap , _ = torch . max ( distance_matrix * pos_idxs , dim = 1 ) # `dist_an` means distance(anchor, negative) # both `dist_an` and `relative_n_inds` with shape [N] dist_an , _ = torch . min ( distance_matrix * neg_idxs + pos_idxs * 99999999.0 , dim = 1 ) return dist_ap , dist_an","title":"hard_example_mining()"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.weighted_example_mining","text":"For each anchor, find the weighted positive and negative sample. Parameters: Name Type Description Default distance_matrix pytorch Variable, pair wise distance between samples, shape [N, N] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] Source code in torchflare/criterion/triplet_loss.py def weighted_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the weighted positive and negative sample. Args: distance_matrix: pytorch Variable, pair wise distance between samples, shape [N, N] pos_idxs:positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 dist_ap = distance_matrix * pos_idxs dist_an = distance_matrix * neg_idxs weights_ap = softmax_weights ( dist_ap , pos_idxs ) weights_an = softmax_weights ( - dist_an , neg_idxs ) dist_ap = torch . sum ( dist_ap * weights_ap , dim = 1 ) dist_an = torch . sum ( dist_an * weights_an , dim = 1 ) return dist_ap , dist_an","title":"weighted_example_mining()"},{"location":"dataloaders/image_data/","text":"Class to create easy to use dataloaders. Methods from_csv ( path , csv_path , image_col , label_cols = None , augmentations = None , convert_mode = 'RGB' , extension = None ) classmethod Classmethod to create a dataset for image data when you have image names/ids , labels in a csv. Parameters: Name Type Description Default path str The path where images are saved. required csv_path str The full path to csv. Example: ./train/train_data.csv required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None label_cols Union[str, List[str]] The list of columns containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataloader Pytorch dataset created from dataframe. For inference do not pass in the label_cols , keep it None . Augmentations must be Compose objects from albumentations or torchvision . from_df ( path , df , image_col , label_cols = None , augmentations = None , convert_mode = 'RGB' , extension = None ) classmethod Classmethod to create a dataset for image data when you have image names/ids , labels in dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None label_cols Union[str, List[str]] The list of columns containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataloader Pytorch dataset created from dataframe. For inference do not pass in the label_cols , keep it None . Augmentations must be Compose objects from albumentations or torchvision . from_folders ( path , augmentations = None , convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataloader Pytorch image dataset created from folders Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg get_loader ( self , batch_size = 32 , shuffle = True , ** dl_params ) Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments. Examples from_df from torchflare.datasets import ImageDataloader dl = ImageDataloader . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augs , extension = '.jpg' ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) from_folders from torchflare.datasets import ImageDataloader dl = ImageDataloader . from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) from_csv from torchflare.datasets import ImageDataloader dl = ImageDataloader . from_csv ( csv_path = \"./train/train.csv\" , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augs , extension = '.jpg' ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"ImageData"},{"location":"dataloaders/image_data/#torchflare.datasets.image_dataloader.ImageDataloader-methods","text":"","title":"Methods"},{"location":"dataloaders/image_data/#torchflare.datasets.image_dataloader.ImageDataloader.from_csv","text":"Classmethod to create a dataset for image data when you have image names/ids , labels in a csv. Parameters: Name Type Description Default path str The path where images are saved. required csv_path str The full path to csv. Example: ./train/train_data.csv required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None label_cols Union[str, List[str]] The list of columns containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataloader Pytorch dataset created from dataframe. For inference do not pass in the label_cols , keep it None . Augmentations must be Compose objects from albumentations or torchvision .","title":"from_csv()"},{"location":"dataloaders/image_data/#torchflare.datasets.image_dataloader.ImageDataloader.from_df","text":"Classmethod to create a dataset for image data when you have image names/ids , labels in dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None label_cols Union[str, List[str]] The list of columns containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataloader Pytorch dataset created from dataframe. For inference do not pass in the label_cols , keep it None . Augmentations must be Compose objects from albumentations or torchvision .","title":"from_df()"},{"location":"dataloaders/image_data/#torchflare.datasets.image_dataloader.ImageDataloader.from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataloader Pytorch image dataset created from folders Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg","title":"from_folders()"},{"location":"dataloaders/image_data/#torchflare.datasets.image_dataloader.ImageDataloader.get_loader","text":"Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments.","title":"get_loader()"},{"location":"dataloaders/image_data/#examples","text":"","title":"Examples"},{"location":"dataloaders/image_data/#from_df","text":"from torchflare.datasets import ImageDataloader dl = ImageDataloader . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augs , extension = '.jpg' ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"from_df"},{"location":"dataloaders/image_data/#from_folders","text":"from torchflare.datasets import ImageDataloader dl = ImageDataloader . from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"from_folders"},{"location":"dataloaders/image_data/#from_csv","text":"from torchflare.datasets import ImageDataloader dl = ImageDataloader . from_csv ( csv_path = \"./train/train.csv\" , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augs , extension = '.jpg' ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"from_csv"},{"location":"dataloaders/segmentation_data/","text":"Class to create easy to use dataloaders. Methods from_folders ( image_path , mask_path = None , augmentations = None , image_convert_mode = 'L' , mask_convert_mode = 'L' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SegmentationDataloader Pytorch Segmentation dataset created from folders. from_rle ( path , df , image_col , mask_cols = None , augmentations = None , mask_size = None , num_classes = None , extension = None , image_convert_mode = 'RGB' ) classmethod Classmethod to create a dataset for segmentation when you have, rule length encodings stored in a dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images and the masks. None mask_cols List[str] The list of columns containing the rule length encoding. None mask_size Tuple[int, int] The size of mask. None num_classes List The list of num_classes. None extension Optional[str] The image file extension. None image_convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description SegmentationDataloader Pytorch dataset created from Rule-length encodings This method will make only binary masks . If you want to create a dataloader for testing set mask_cols = None , mask_size = None , num_classes = None . get_loader ( self , batch_size = 32 , shuffle = True , ** dl_params ) Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments. from_rle from torchflare.datasets import SegmentationDataloader dl = SegmentationDataloader . from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) from_folders from torchflare.datasets import SegmentationDataloader dl = SegmentationDataloader . from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"SegmentationData"},{"location":"dataloaders/segmentation_data/#torchflare.datasets.segmentation_dataloader.SegmentationDataloader-methods","text":"","title":"Methods"},{"location":"dataloaders/segmentation_data/#torchflare.datasets.segmentation_dataloader.SegmentationDataloader.from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SegmentationDataloader Pytorch Segmentation dataset created from folders.","title":"from_folders()"},{"location":"dataloaders/segmentation_data/#torchflare.datasets.segmentation_dataloader.SegmentationDataloader.from_rle","text":"Classmethod to create a dataset for segmentation when you have, rule length encodings stored in a dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images and the masks. None mask_cols List[str] The list of columns containing the rule length encoding. None mask_size Tuple[int, int] The size of mask. None num_classes List The list of num_classes. None extension Optional[str] The image file extension. None image_convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description SegmentationDataloader Pytorch dataset created from Rule-length encodings This method will make only binary masks . If you want to create a dataloader for testing set mask_cols = None , mask_size = None , num_classes = None .","title":"from_rle()"},{"location":"dataloaders/segmentation_data/#torchflare.datasets.segmentation_dataloader.SegmentationDataloader.get_loader","text":"Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments.","title":"get_loader()"},{"location":"dataloaders/segmentation_data/#from_rle","text":"from torchflare.datasets import SegmentationDataloader dl = SegmentationDataloader . from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"from_rle"},{"location":"dataloaders/segmentation_data/#from_folders","text":"from torchflare.datasets import SegmentationDataloader dl = SegmentationDataloader . from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"from_folders"},{"location":"dataloaders/tabular_data/","text":"Class to create easy to use dataloaders. Methods from_csv ( csv_path , feature_cols , label_cols = None ) classmethod Classmethod to create a dataset for tabular data from csv. Parameters: Name Type Description Default csv_path str The full path to csv. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description TabularDataloader Tabular pytorch dataset. from_df ( df , feature_cols , label_cols = None ) classmethod Classmethod to create dataset for tabular data from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe containing features and labels. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description TabularDataloader Tabular pytorch dataset get_loader ( self , batch_size = 32 , shuffle = True , ** dl_params ) Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments. from_df from torchflare.datasets import TabularDataloader dl = TabularDataloader . from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) from_csv from torchflare.datasets import TabularDataloader dl = TabularDataloader . from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"TabularData"},{"location":"dataloaders/tabular_data/#torchflare.datasets.tabular_dataloader.TabularDataloader-methods","text":"","title":"Methods"},{"location":"dataloaders/tabular_data/#torchflare.datasets.tabular_dataloader.TabularDataloader.from_csv","text":"Classmethod to create a dataset for tabular data from csv. Parameters: Name Type Description Default csv_path str The full path to csv. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description TabularDataloader Tabular pytorch dataset.","title":"from_csv()"},{"location":"dataloaders/tabular_data/#torchflare.datasets.tabular_dataloader.TabularDataloader.from_df","text":"Classmethod to create dataset for tabular data from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe containing features and labels. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description TabularDataloader Tabular pytorch dataset","title":"from_df()"},{"location":"dataloaders/tabular_data/#torchflare.datasets.tabular_dataloader.TabularDataloader.get_loader","text":"Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments.","title":"get_loader()"},{"location":"dataloaders/tabular_data/#from_df","text":"from torchflare.datasets import TabularDataloader dl = TabularDataloader . from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"from_df"},{"location":"dataloaders/tabular_data/#from_csv","text":"from torchflare.datasets import TabularDataloader dl = TabularDataloader . from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"from_csv"},{"location":"dataloaders/text_data/","text":"Class to create easy to use dataloaders. Methods from_df ( df , input_col , label_cols , tokenizer , max_len ) classmethod Classmethod to create a dataset as required by transformers for text classification tasks. Parameters: Name Type Description Default df pd.DataFrame The dataframe containing sentences and labels. required input_col str The name of column containing sentences. required label_cols Optional[Union[str, List[str]]] name of label column, or a list containing names of label columns. required tokenizer The tokenizer to be used to tokenize the sentences. required max_len int The max_length to be used by the tokenizer. required Returns: Type Description TextDataloader pytorch dataset for text classification using huggingface. get_loader ( self , batch_size = 32 , shuffle = True , ** dl_params ) Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments. from_df from torchflare.datasets import TextDataloader dl = TextDataloader . data_from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args. )","title":"TextData"},{"location":"dataloaders/text_data/#torchflare.datasets.text_dataloader.TextDataloader-methods","text":"","title":"Methods"},{"location":"dataloaders/text_data/#torchflare.datasets.text_dataloader.TextDataloader.from_df","text":"Classmethod to create a dataset as required by transformers for text classification tasks. Parameters: Name Type Description Default df pd.DataFrame The dataframe containing sentences and labels. required input_col str The name of column containing sentences. required label_cols Optional[Union[str, List[str]]] name of label column, or a list containing names of label columns. required tokenizer The tokenizer to be used to tokenize the sentences. required max_len int The max_length to be used by the tokenizer. required Returns: Type Description TextDataloader pytorch dataset for text classification using huggingface.","title":"from_df()"},{"location":"dataloaders/text_data/#torchflare.datasets.text_dataloader.TextDataloader.get_loader","text":"Method to get dataloader. Parameters: Name Type Description Default batch_size int The batch size to use 32 shuffle bool Whether to shuffle the inputs. True **dl_params Keyword arguments related to dataloader {} Returns: Type Description DataLoader A PyTorch dataloader with given arguments.","title":"get_loader()"},{"location":"dataloaders/text_data/#from_df","text":"from torchflare.datasets import TextDataloader dl = TextDataloader . data_from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args. )","title":"from_df"},{"location":"datasets/cross_val/","text":"Class to perform cross validation on given dataset. Methods __init__ ( self , dataset , cv , n_splits , ** kwargs ) special Constructor class for CVSplit class. Parameters: Name Type Description Default dataset Dataset A PyTorch style dataset. Dataset must be the one implemented in torchflare. required cv str The cross-validation splitting strategy. required n_splits int The number of splits. required **kwargs keyword arguments related to cross validation strategy. {} Note Only supports KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, RepeatedKFold, RepeatedStratifiedKFold cross validation schemes. Source code in torchflare/datasets/cross_val.py def __init__ ( self , dataset : Dataset , cv : str , n_splits : int , ** kwargs ): \"\"\"Constructor class for CVSplit class. Args: dataset: A PyTorch style dataset. Dataset must be the one implemented in torchflare. cv: The cross-validation splitting strategy. n_splits: The number of splits. **kwargs: keyword arguments related to cross validation strategy. Note: Only supports KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, RepeatedKFold, RepeatedStratifiedKFold cross validation schemes. Raises: ValueError if cv strategy not in the specified ones. \"\"\" if cv not in [ \"KFold\" , \"ShuffleSplit\" , \"StratifiedKFold\" , \"StratifiedShuffleSplit\" , \"RepeatedKFold\" , \"RepeatedStratifiedKFold\" , ]: raise ValueError ( f \"Does not support { cv } \" ) self . X , self . y = None , None self . cv = getattr ( sklearn . model_selection , cv )( n_splits = n_splits , ** kwargs ) self . dataset = dataset self . fold_dict = {} self . _get_fold () get_loaders ( self , fold , train_params , val_params ) Generates training and validation dataloaders as per the given fold. Parameters: Name Type Description Default fold int The fold/split number for which you want dataloader. required train_params Dict A dictionary containing parameters for training dataloader. required val_params Dict A dictionary containing parameters for validation dataloader. required Returns: Type Description Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader] Training dataloader and validation dataloader for a given fold. Source code in torchflare/datasets/cross_val.py def get_loaders ( self , fold : int , train_params : Dict , val_params : Dict ) -> Tuple [ DataLoader , DataLoader ]: \"\"\"Generates training and validation dataloaders as per the given fold. Args: fold: The fold/split number for which you want dataloader. train_params: A dictionary containing parameters for training dataloader. val_params: A dictionary containing parameters for validation dataloader. Returns: Training dataloader and validation dataloader for a given fold. \"\"\" train_data = torch . utils . data . Subset ( self . dataset , self . fold_dict [ fold ] . get ( \"train_idx\" )) valid_data = torch . utils . data . Subset ( self . dataset , self . fold_dict [ fold ] . get ( \"val_idx\" )) train_dl = torch . utils . data . DataLoader ( train_data , ** train_params ) valid_dl = torch . utils . data . DataLoader ( valid_data , ** val_params ) return train_dl , valid_dl Examples from torchflare.datasets import CVSplit , ImageDataset n_splits = 5 # Creating a PyTorch Dataset. ds = ImageDataset . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augmentations , extension = '.jpg' convert_mode = \"RGB\" ) cv_data = CVSplit ( dataset = ds , cv = \"KFold\" , n_splits = n_splits , shuffle = True , random_state = 42 ) for fold in range ( n_splits ): # Generate Train and validation dataloaders per fold. train_dl , valid_dl = cv_data . get_loaders ( fold = fold , train_params = dict ( batch_size = 64 , shuffle = True , num_workers = 0 ), val_params = dict ( batch_size = 32 , shuffle = False ))","title":"CrossValidation"},{"location":"datasets/cross_val/#torchflare.datasets.cross_val.CVSplit-methods","text":"","title":"Methods"},{"location":"datasets/cross_val/#torchflare.datasets.cross_val.CVSplit.__init__","text":"Constructor class for CVSplit class. Parameters: Name Type Description Default dataset Dataset A PyTorch style dataset. Dataset must be the one implemented in torchflare. required cv str The cross-validation splitting strategy. required n_splits int The number of splits. required **kwargs keyword arguments related to cross validation strategy. {} Note Only supports KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, RepeatedKFold, RepeatedStratifiedKFold cross validation schemes. Source code in torchflare/datasets/cross_val.py def __init__ ( self , dataset : Dataset , cv : str , n_splits : int , ** kwargs ): \"\"\"Constructor class for CVSplit class. Args: dataset: A PyTorch style dataset. Dataset must be the one implemented in torchflare. cv: The cross-validation splitting strategy. n_splits: The number of splits. **kwargs: keyword arguments related to cross validation strategy. Note: Only supports KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, RepeatedKFold, RepeatedStratifiedKFold cross validation schemes. Raises: ValueError if cv strategy not in the specified ones. \"\"\" if cv not in [ \"KFold\" , \"ShuffleSplit\" , \"StratifiedKFold\" , \"StratifiedShuffleSplit\" , \"RepeatedKFold\" , \"RepeatedStratifiedKFold\" , ]: raise ValueError ( f \"Does not support { cv } \" ) self . X , self . y = None , None self . cv = getattr ( sklearn . model_selection , cv )( n_splits = n_splits , ** kwargs ) self . dataset = dataset self . fold_dict = {} self . _get_fold ()","title":"__init__()"},{"location":"datasets/cross_val/#torchflare.datasets.cross_val.CVSplit.get_loaders","text":"Generates training and validation dataloaders as per the given fold. Parameters: Name Type Description Default fold int The fold/split number for which you want dataloader. required train_params Dict A dictionary containing parameters for training dataloader. required val_params Dict A dictionary containing parameters for validation dataloader. required Returns: Type Description Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader] Training dataloader and validation dataloader for a given fold. Source code in torchflare/datasets/cross_val.py def get_loaders ( self , fold : int , train_params : Dict , val_params : Dict ) -> Tuple [ DataLoader , DataLoader ]: \"\"\"Generates training and validation dataloaders as per the given fold. Args: fold: The fold/split number for which you want dataloader. train_params: A dictionary containing parameters for training dataloader. val_params: A dictionary containing parameters for validation dataloader. Returns: Training dataloader and validation dataloader for a given fold. \"\"\" train_data = torch . utils . data . Subset ( self . dataset , self . fold_dict [ fold ] . get ( \"train_idx\" )) valid_data = torch . utils . data . Subset ( self . dataset , self . fold_dict [ fold ] . get ( \"val_idx\" )) train_dl = torch . utils . data . DataLoader ( train_data , ** train_params ) valid_dl = torch . utils . data . DataLoader ( valid_data , ** val_params ) return train_dl , valid_dl","title":"get_loaders()"},{"location":"datasets/cross_val/#examples","text":"from torchflare.datasets import CVSplit , ImageDataset n_splits = 5 # Creating a PyTorch Dataset. ds = ImageDataset . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augmentations , extension = '.jpg' convert_mode = \"RGB\" ) cv_data = CVSplit ( dataset = ds , cv = \"KFold\" , n_splits = n_splits , shuffle = True , random_state = 42 ) for fold in range ( n_splits ): # Generate Train and validation dataloaders per fold. train_dl , valid_dl = cv_data . get_loaders ( fold = fold , train_params = dict ( batch_size = 64 , shuffle = True , num_workers = 0 ), val_params = dict ( batch_size = 32 , shuffle = False ))","title":"Examples"},{"location":"datasets/image_data/","text":"Class to create the dataset for Image Classification. Methods from_df ( path , df , image_col , label_cols = None , augmentations = None , extension = None , convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset from the given dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None label_cols Union[str, List[str]] Column name or list of column names containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. Source code in torchflare/datasets/classification.py @classmethod def from_df ( cls , path : str , df : pd . DataFrame , image_col : str , label_cols : Union [ str , List [ str ]] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , extension : str = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from the given dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc augmentations: The batch_mixers to be used on images. label_cols: Column name or list of column names containing targets. extension : The image file extension. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. \"\"\" img_list = cls . _join_paths ( path = path , file_names = df . loc [:, image_col ] . values , extension = extension ) label_list = cls . _get_labels_from_df ( df = df , label_cols = label_cols ) return cls ( image_paths_list = img_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , ) from_folders ( path , augmentations = None , convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg Source code in torchflare/datasets/classification.py @classmethod def from_folders ( cls , path : str , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: path: The path where images are stored. augmentations:The batch_mixers to be used on images. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg \"\"\" class_to_idx = cls . _get_labels_from_folders ( path ) if class_to_idx : image_list , label_list = get_files_and_labels ( path , class_to_idx ) else : image_list , label_list = get_files ( path ), None return cls ( image_paths_list = image_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , ) Examples from_df from torchflare.datasets import ImageDataset ds = ImageDataset . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augmentations , extension = './jpg' convert_mode = \"RGB\" ) from_folders from torchflare.datasets import ImageDataset ds = ImageDataset . from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" )","title":"ImageDataset"},{"location":"datasets/image_data/#torchflare.datasets.classification.ImageDataset-methods","text":"","title":"Methods"},{"location":"datasets/image_data/#torchflare.datasets.classification.ImageDataset.from_df","text":"Classmethod to create pytorch dataset from the given dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None label_cols Union[str, List[str]] Column name or list of column names containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. Source code in torchflare/datasets/classification.py @classmethod def from_df ( cls , path : str , df : pd . DataFrame , image_col : str , label_cols : Union [ str , List [ str ]] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , extension : str = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from the given dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc augmentations: The batch_mixers to be used on images. label_cols: Column name or list of column names containing targets. extension : The image file extension. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. \"\"\" img_list = cls . _join_paths ( path = path , file_names = df . loc [:, image_col ] . values , extension = extension ) label_list = cls . _get_labels_from_df ( df = df , label_cols = label_cols ) return cls ( image_paths_list = img_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , )","title":"from_df()"},{"location":"datasets/image_data/#torchflare.datasets.classification.ImageDataset.from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg Source code in torchflare/datasets/classification.py @classmethod def from_folders ( cls , path : str , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: path: The path where images are stored. augmentations:The batch_mixers to be used on images. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg \"\"\" class_to_idx = cls . _get_labels_from_folders ( path ) if class_to_idx : image_list , label_list = get_files_and_labels ( path , class_to_idx ) else : image_list , label_list = get_files ( path ), None return cls ( image_paths_list = image_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , )","title":"from_folders()"},{"location":"datasets/image_data/#examples","text":"","title":"Examples"},{"location":"datasets/image_data/#from_df","text":"from torchflare.datasets import ImageDataset ds = ImageDataset . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augmentations , extension = './jpg' convert_mode = \"RGB\" )","title":"from_df"},{"location":"datasets/image_data/#from_folders","text":"from torchflare.datasets import ImageDataset ds = ImageDataset . from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" )","title":"from_folders"},{"location":"datasets/segmentation_data/","text":"Class to create a dataset for image segmentation tasks. Methods from_folders ( image_path , mask_path = None , augmentations = None , image_convert_mode = 'L' , mask_convert_mode = 'L' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SegmentationDataset returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note If you want to create a dataset for testing just set mask_path = None. Source code in torchflare/datasets/segmentation.py @classmethod def from_folders ( cls , image_path : str , mask_path : str = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , image_convert_mode : str = \"L\" , mask_convert_mode : str = \"L\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: image_path: The path where images are stored. mask_path: The path where masks are stored. augmentations: The batch_mixers to apply on images and masks. image_convert_mode: The mode to be passed to PIL.Image.convert for input images mask_convert_mode: The mode to be passed to PIL.Image.convert for masks. Returns: returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note: If you want to create a dataset for testing just set mask_path = None. \"\"\" image_files = cls . _join_paths ( image_path , os . listdir ( image_path )) mask_files = cls . _join_paths ( mask_path , os . listdir ( mask_path )) if mask_path is not None else None return cls ( image_paths_list = image_files , mask_list = mask_files , augmentations = augmentations , image_convert_mode = image_convert_mode , mask_convert_mode = mask_convert_mode , ) from_rle ( path , df , image_col , mask_cols = None , augmentations = None , mask_size = None , num_classes = None , extension = None , image_convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images and the masks. None mask_cols List[str] The list of columns containing the rule length encoding. None mask_size Tuple[int, int] The size of mask. None num_classes int The number of num_classes. None image_convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' extension str The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc None Returns: Type Description SegmentationDataset returns image_paths_list , labels , image_convert_mode , augmentations and extra kwargs. Note If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. Source code in torchflare/datasets/segmentation.py @classmethod def from_rle ( cls , path : str , df : pd . DataFrame , image_col : str , mask_cols : List [ str ] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , mask_size : Tuple [ int , int ] = None , num_classes : int = None , extension : str = None , image_convert_mode : str = \"RGB\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids. augmentations: The batch_mixers to be used on images and the masks. mask_cols: The list of columns containing the rule length encoding. mask_size: The size of mask. num_classes: The number of num_classes. image_convert_mode: The mode to be passed to PIL.Image.convert. extension : The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc Returns: returns image_paths_list , labels , image_convert_mode , augmentations and extra kwargs. Note: If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. \"\"\" image_list = cls . _join_paths ( path = path , file_names = df [ image_col ] . values . tolist (), extension = extension ) mask_list = ( cls . create_mask_list ( df = df , image_col = image_col , mask_cols = mask_cols ) if mask_cols is not None else None ) return cls ( image_list , mask_list , augmentations , image_convert_mode = image_convert_mode , shape = mask_size , num_classes = num_classes , ) Examples from_df from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" , ) from_folders from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , )","title":"SegmentationDataset"},{"location":"datasets/segmentation_data/#torchflare.datasets.segmentation.SegmentationDataset-methods","text":"","title":"Methods"},{"location":"datasets/segmentation_data/#torchflare.datasets.segmentation.SegmentationDataset.from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SegmentationDataset returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note If you want to create a dataset for testing just set mask_path = None. Source code in torchflare/datasets/segmentation.py @classmethod def from_folders ( cls , image_path : str , mask_path : str = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , image_convert_mode : str = \"L\" , mask_convert_mode : str = \"L\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: image_path: The path where images are stored. mask_path: The path where masks are stored. augmentations: The batch_mixers to apply on images and masks. image_convert_mode: The mode to be passed to PIL.Image.convert for input images mask_convert_mode: The mode to be passed to PIL.Image.convert for masks. Returns: returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note: If you want to create a dataset for testing just set mask_path = None. \"\"\" image_files = cls . _join_paths ( image_path , os . listdir ( image_path )) mask_files = cls . _join_paths ( mask_path , os . listdir ( mask_path )) if mask_path is not None else None return cls ( image_paths_list = image_files , mask_list = mask_files , augmentations = augmentations , image_convert_mode = image_convert_mode , mask_convert_mode = mask_convert_mode , )","title":"from_folders()"},{"location":"datasets/segmentation_data/#torchflare.datasets.segmentation.SegmentationDataset.from_rle","text":"Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images and the masks. None mask_cols List[str] The list of columns containing the rule length encoding. None mask_size Tuple[int, int] The size of mask. None num_classes int The number of num_classes. None image_convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' extension str The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc None Returns: Type Description SegmentationDataset returns image_paths_list , labels , image_convert_mode , augmentations and extra kwargs. Note If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. Source code in torchflare/datasets/segmentation.py @classmethod def from_rle ( cls , path : str , df : pd . DataFrame , image_col : str , mask_cols : List [ str ] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , mask_size : Tuple [ int , int ] = None , num_classes : int = None , extension : str = None , image_convert_mode : str = \"RGB\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids. augmentations: The batch_mixers to be used on images and the masks. mask_cols: The list of columns containing the rule length encoding. mask_size: The size of mask. num_classes: The number of num_classes. image_convert_mode: The mode to be passed to PIL.Image.convert. extension : The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc Returns: returns image_paths_list , labels , image_convert_mode , augmentations and extra kwargs. Note: If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. \"\"\" image_list = cls . _join_paths ( path = path , file_names = df [ image_col ] . values . tolist (), extension = extension ) mask_list = ( cls . create_mask_list ( df = df , image_col = image_col , mask_cols = mask_cols ) if mask_cols is not None else None ) return cls ( image_list , mask_list , augmentations , image_convert_mode = image_convert_mode , shape = mask_size , num_classes = num_classes , )","title":"from_rle()"},{"location":"datasets/segmentation_data/#examples","text":"","title":"Examples"},{"location":"datasets/segmentation_data/#from_df","text":"from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" , )","title":"from_df"},{"location":"datasets/segmentation_data/#from_folders","text":"from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , )","title":"from_folders"},{"location":"datasets/tabular_data/","text":"Class to create a dataset for tasks involving tabular data. Methods from_csv ( csv_path , feature_cols , label_cols = None ) classmethod Classmethod to create pytorch style dataset from csv file. Parameters: Name Type Description Default csv_path The full path to csv. required feature_cols Union[str, List[str]] The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of inputs , labels if label_cols is not None else return np.array of inputs. Source code in torchflare/datasets/tabular.py @classmethod def from_csv ( cls , csv_path , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from csv file. Args: csv_path: The full path to csv. feature_cols: The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of inputs , labels if label_cols is not None else return np.array of inputs. \"\"\" return cls . from_df ( df = pd . read_csv ( csv_path ), feature_cols = feature_cols , label_cols = label_cols ) from_df ( df , feature_cols , label_cols = None ) classmethod Classmethod to create pytorch style dataset from dataframes. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has inputs, and the labels/targets. required feature_cols Union[str, List[str]] The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of inputs , labels if label_cols is not None else return np.array of inputs. Source code in torchflare/datasets/tabular.py @classmethod def from_df ( cls , df : pd . DataFrame , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from dataframes. Args: df: The dataframe which has inputs, and the labels/targets. feature_cols: The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of inputs , labels if label_cols is not None else return np.array of inputs. \"\"\" features = df . loc [:, feature_cols ] . values labels = df . loc [:, label_cols ] . values if label_cols is not None else None return cls ( inputs = features , labels = labels ) Examples from_df from torchflare.datasets import TabularDataset ds = TabularDataset . from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) from_csv from torchflare.datasets import TabularDataset ds = TabularDataset . from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" )","title":"TabularDataset"},{"location":"datasets/tabular_data/#torchflare.datasets.tabular.TabularDataset-methods","text":"","title":"Methods"},{"location":"datasets/tabular_data/#torchflare.datasets.tabular.TabularDataset.from_csv","text":"Classmethod to create pytorch style dataset from csv file. Parameters: Name Type Description Default csv_path The full path to csv. required feature_cols Union[str, List[str]] The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of inputs , labels if label_cols is not None else return np.array of inputs. Source code in torchflare/datasets/tabular.py @classmethod def from_csv ( cls , csv_path , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from csv file. Args: csv_path: The full path to csv. feature_cols: The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of inputs , labels if label_cols is not None else return np.array of inputs. \"\"\" return cls . from_df ( df = pd . read_csv ( csv_path ), feature_cols = feature_cols , label_cols = label_cols )","title":"from_csv()"},{"location":"datasets/tabular_data/#torchflare.datasets.tabular.TabularDataset.from_df","text":"Classmethod to create pytorch style dataset from dataframes. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has inputs, and the labels/targets. required feature_cols Union[str, List[str]] The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of inputs , labels if label_cols is not None else return np.array of inputs. Source code in torchflare/datasets/tabular.py @classmethod def from_df ( cls , df : pd . DataFrame , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from dataframes. Args: df: The dataframe which has inputs, and the labels/targets. feature_cols: The name of columns which contain inputs. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of inputs , labels if label_cols is not None else return np.array of inputs. \"\"\" features = df . loc [:, feature_cols ] . values labels = df . loc [:, label_cols ] . values if label_cols is not None else None return cls ( inputs = features , labels = labels )","title":"from_df()"},{"location":"datasets/tabular_data/#examples","text":"","title":"Examples"},{"location":"datasets/tabular_data/#from_df","text":"from torchflare.datasets import TabularDataset ds = TabularDataset . from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" )","title":"from_df"},{"location":"datasets/tabular_data/#from_csv","text":"from torchflare.datasets import TabularDataset ds = TabularDataset . from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" )","title":"from_csv"},{"location":"datasets/text_data/","text":"Class to create a dataset for text classification as required by transformers. Methods from_df ( df , input_col , tokenizer , max_len , label_cols = None ) classmethod Classmethod to create the dataset from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has the data. required input_col str The column containing the inputs. required label_cols Union[str, List[str]] The column which contains corresponding labels. None tokenizer The tokenizer to be used.(Use only tokenizer available in huggingface) required max_len int The max_len to be used. required Returns: Type Description TextClassificationDataset A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. Source code in torchflare/datasets/text_dataset.py @classmethod def from_df ( cls , df : pd . DataFrame , input_col : str , tokenizer , max_len : int , label_cols : Union [ str , List [ str ]] = None , ) -> TextClassificationDataset : \"\"\"Classmethod to create the dataset from dataframe. Args: df: The dataframe which has the data. input_col: The column containing the inputs. label_cols: The column which contains corresponding labels. tokenizer: The tokenizer to be used.(Use only tokenizer available in huggingface) max_len: The max_len to be used. Returns: A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. \"\"\" inputs = df . loc [:, input_col ] . values . tolist () labels = df . loc [:, label_cols ] . values . tolist () if label_cols is not None else None return cls ( inputs = inputs , labels = labels , tokenizer = tokenizer , max_len = max_len ) Examples from_df import transformers from torchflare.datasets import TextClassificationDataset tokenizer = transformers . BertTokenizer . from_pretrained ( \"bert-base-uncased\" ) ds = TextClassificationDataset . from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 )","title":"TextClassificationDataset"},{"location":"datasets/text_data/#torchflare.datasets.text_dataset.TextClassificationDataset-methods","text":"","title":"Methods"},{"location":"datasets/text_data/#torchflare.datasets.text_dataset.TextClassificationDataset.from_df","text":"Classmethod to create the dataset from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has the data. required input_col str The column containing the inputs. required label_cols Union[str, List[str]] The column which contains corresponding labels. None tokenizer The tokenizer to be used.(Use only tokenizer available in huggingface) required max_len int The max_len to be used. required Returns: Type Description TextClassificationDataset A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. Source code in torchflare/datasets/text_dataset.py @classmethod def from_df ( cls , df : pd . DataFrame , input_col : str , tokenizer , max_len : int , label_cols : Union [ str , List [ str ]] = None , ) -> TextClassificationDataset : \"\"\"Classmethod to create the dataset from dataframe. Args: df: The dataframe which has the data. input_col: The column containing the inputs. label_cols: The column which contains corresponding labels. tokenizer: The tokenizer to be used.(Use only tokenizer available in huggingface) max_len: The max_len to be used. Returns: A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. \"\"\" inputs = df . loc [:, input_col ] . values . tolist () labels = df . loc [:, label_cols ] . values . tolist () if label_cols is not None else None return cls ( inputs = inputs , labels = labels , tokenizer = tokenizer , max_len = max_len )","title":"from_df()"},{"location":"datasets/text_data/#examples","text":"","title":"Examples"},{"location":"datasets/text_data/#from_df","text":"import transformers from torchflare.datasets import TextClassificationDataset tokenizer = transformers . BertTokenizer . from_pretrained ( \"bert-base-uncased\" ) ds = TextClassificationDataset . from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 )","title":"from_df"},{"location":"interpreters/cam/","text":"Implementation of GradCam algorithm. GradCam: Visual Explanations from Deep Networks via Gradient-based Localization Methods __init__ ( self , model , target_layer ) special Constructor method for GradCam. Parameters: Name Type Description Default model The model to be used for gradcam. required target_layer The target layer to be used for cam extraction. required Source code in torchflare/interpreters/grad_cam.py def __init__ ( self , model , target_layer ): \"\"\"Constructor method for GradCam. Args: model: The model to be used for gradcam. target_layer: The target layer to be used for cam extraction. \"\"\" super ( GradCam , self ) . __init__ ( model = model , target_layer = target_layer ) Example from torchflare.interpreters import GradCamPP , visualize_cam cam_model = GradCam ( model = model , target_layer = target_layer ) cam = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = cam )","title":"GradCam"},{"location":"interpreters/cam/#torchflare.interpreters.grad_cam.GradCam-methods","text":"","title":"Methods"},{"location":"interpreters/cam/#torchflare.interpreters.grad_cam.GradCam.__init__","text":"Constructor method for GradCam. Parameters: Name Type Description Default model The model to be used for gradcam. required target_layer The target layer to be used for cam extraction. required Source code in torchflare/interpreters/grad_cam.py def __init__ ( self , model , target_layer ): \"\"\"Constructor method for GradCam. Args: model: The model to be used for gradcam. target_layer: The target layer to be used for cam extraction. \"\"\" super ( GradCam , self ) . __init__ ( model = model , target_layer = target_layer )","title":"__init__()"},{"location":"interpreters/cam/#example","text":"from torchflare.interpreters import GradCamPP , visualize_cam cam_model = GradCam ( model = model , target_layer = target_layer ) cam = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = cam )","title":"Example"},{"location":"interpreters/camPP/","text":"Implementation of GradCam++. GradCam++ : Improved Visual Explanations forDeep Convolutional Networks Methods __init__ ( self , model , target_layer ) special Constructor method for GradCam. Parameters: Name Type Description Default model The model to be used for gradcam. required target_layer The target layer to be used for cam extraction. required Source code in torchflare/interpreters/grad_campp.py def __init__ ( self , model , target_layer ): \"\"\"Constructor method for GradCam. Args: model: The model to be used for gradcam. target_layer: The target layer to be used for cam extraction. \"\"\" super ( GradCamPP , self ) . __init__ ( model = model , target_layer = target_layer ) Example from torchflare.interpreters import GradCamPP , visualize_cam cam_model = GradCamPP ( model = model , target_layer = target_layer ) cam = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = cam )","title":"GradCam++"},{"location":"interpreters/camPP/#torchflare.interpreters.grad_campp.GradCamPP-methods","text":"","title":"Methods"},{"location":"interpreters/camPP/#torchflare.interpreters.grad_campp.GradCamPP.__init__","text":"Constructor method for GradCam. Parameters: Name Type Description Default model The model to be used for gradcam. required target_layer The target layer to be used for cam extraction. required Source code in torchflare/interpreters/grad_campp.py def __init__ ( self , model , target_layer ): \"\"\"Constructor method for GradCam. Args: model: The model to be used for gradcam. target_layer: The target layer to be used for cam extraction. \"\"\" super ( GradCamPP , self ) . __init__ ( model = model , target_layer = target_layer )","title":"__init__()"},{"location":"interpreters/camPP/#example","text":"from torchflare.interpreters import GradCamPP , visualize_cam cam_model = GradCamPP ( model = model , target_layer = target_layer ) cam = cam_model ( tensor , target_category = 282 ) visualize_cam ( image = image , cam = cam )","title":"Example"},{"location":"metrics/classification/accuracy/","text":"Computes Accuracy. Support binary,multilabel and multiclass cases Attributes value : Tensor property readonly Computes the Accuracy per epoch. Returns: Type Description Tensor The accuracy Methods __init__ ( self , num_classes , threshold = 0.5 , multilabel = False ) special Constructor method for Accuracy Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/accuracy_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False ): \"\"\"Constructor method for Accuracy Class. Args: num_classes: The number of num_classes. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Accuracy , self ) . __init__ ( multilabel = multilabel ) self . threshold = threshold self . num_classes = num_classes self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/accuracy_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/accuracy_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/accuracy_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import Accuracy # Binary-Classification Problems acc = Accuracy ( num_classes = 2 , threshold = 0.7 , multilabel = False ) # Mutliclass-Classification Problems multiclass_acc = Accuracy ( num_classes = 4 , multilabel = False ) # Multilabel-Classification Problems multilabel_acc = Accuracy ( num_classes = 5 , multilabel = True , threshold = 0.7 )","title":"Accuracy"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy-attributes","text":"","title":"Attributes"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.value","text":"Computes the Accuracy per epoch. Returns: Type Description Tensor The accuracy","title":"value"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy-methods","text":"","title":"Methods"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.__init__","text":"Constructor method for Accuracy Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/accuracy_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False ): \"\"\"Constructor method for Accuracy Class. Args: num_classes: The number of num_classes. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Accuracy , self ) . __init__ ( multilabel = multilabel ) self . threshold = threshold self . num_classes = num_classes self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/accuracy_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/accuracy_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/accuracy_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/accuracy/#examples","text":"from torchflare.metrics import Accuracy # Binary-Classification Problems acc = Accuracy ( num_classes = 2 , threshold = 0.7 , multilabel = False ) # Mutliclass-Classification Problems multiclass_acc = Accuracy ( num_classes = 4 , multilabel = False ) # Multilabel-Classification Problems multilabel_acc = Accuracy ( num_classes = 5 , multilabel = True , threshold = 0.7 )","title":"Examples"},{"location":"metrics/classification/f1_score/","text":"Computes F1 Score. Supports binary,multiclass and multilabel cases. Attributes value : Tensor property readonly Value of FBeta Score. Returns: Type Description Tensor The computed F1-score Methods __init__ ( self , num_classes , threshold = 0.5 , multilabel = False , average = 'macro' ) special Constructor method for F1-score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\". 'macro' multilabel bool Whether the problem is multilabel or not. False Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False , average : str = \"macro\" , ): \"\"\"Constructor method for F1-score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\". multilabel: Whether the problem is multilabel or not. \"\"\" super ( F1Score , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import F1Score # Binary-Classification Problems acc = F1Score ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = F1Score ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = F1Score ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"F1Score"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score-attributes","text":"","title":"Attributes"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.value","text":"Value of FBeta Score. Returns: Type Description Tensor The computed F1-score","title":"value"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score-methods","text":"","title":"Methods"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.__init__","text":"Constructor method for F1-score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\". 'macro' multilabel bool Whether the problem is multilabel or not. False Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False , average : str = \"macro\" , ): \"\"\"Constructor method for F1-score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\". multilabel: Whether the problem is multilabel or not. \"\"\" super ( F1Score , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/f1_score/#examples","text":"from torchflare.metrics import F1Score # Binary-Classification Problems acc = F1Score ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = F1Score ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = F1Score ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/classification/fbeta/","text":"Computes Fbeta Score. Supports binary,multiclass and multilabel cases. Attributes value : Tensor property readonly Computes the FBeta Score. Returns: Type Description Tensor The computed Fbeta score. Methods __init__ ( self , beta , num_classes , threshold = 0.5 , average = 'macro' , multilabel = False ) special Constructor method for Fbeta score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\" 'macro' beta float weight of precision in harmonic mean. required multilabel bool Whether problem is multilabel or not. False Note In case of binary classification, set num_classes = 1 Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , beta : float , num_classes : int , threshold : float = 0.5 , average : str = \"macro\" , multilabel : bool = False , ): \"\"\"Constructor method for Fbeta score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\" beta : weight of precision in harmonic mean. multilabel: Whether problem is multilabel or not. Note: In case of binary classification, set num_classes = 1 \"\"\" super ( FBeta , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . beta = beta self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import FBeta # Binary-Classification Problems acc = FBeta ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = FBeta ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = FBeta ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"FBeta"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta-attributes","text":"","title":"Attributes"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.value","text":"Computes the FBeta Score. Returns: Type Description Tensor The computed Fbeta score.","title":"value"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta-methods","text":"","title":"Methods"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.__init__","text":"Constructor method for Fbeta score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\" 'macro' beta float weight of precision in harmonic mean. required multilabel bool Whether problem is multilabel or not. False Note In case of binary classification, set num_classes = 1 Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , beta : float , num_classes : int , threshold : float = 0.5 , average : str = \"macro\" , multilabel : bool = False , ): \"\"\"Constructor method for Fbeta score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\" beta : weight of precision in harmonic mean. multilabel: Whether problem is multilabel or not. Note: In case of binary classification, set num_classes = 1 \"\"\" super ( FBeta , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . beta = beta self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/fbeta/#examples","text":"from torchflare.metrics import FBeta # Binary-Classification Problems acc = FBeta ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = FBeta ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = FBeta ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/classification/precision/","text":"Class to compute Precision Score. Support binary, multiclass and multilabel cases Attributes value : Tensor property readonly Computes the Precision Score. Returns: Type Description Tensor The computed precision score. Methods __init__ ( self , num_classes , average = 'macro' , threshold = 0.5 , multilabel = False ) special Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/precision_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Precision , self ) . __init__ ( threshold = threshold , num_classes = num_classes , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/precision_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/precision_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation Lists. Source code in torchflare/metrics/precision_meter.py def reset ( self ): \"\"\"Resets the accumulation Lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import Precision # Binary-Classification Problems acc = Precision ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Precision ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Precision ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Precision"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision-attributes","text":"","title":"Attributes"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.value","text":"Computes the Precision Score. Returns: Type Description Tensor The computed precision score.","title":"value"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision-methods","text":"","title":"Methods"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.__init__","text":"Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/precision_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Precision , self ) . __init__ ( threshold = threshold , num_classes = num_classes , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/precision_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/precision_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.reset","text":"Resets the accumulation Lists. Source code in torchflare/metrics/precision_meter.py def reset ( self ): \"\"\"Resets the accumulation Lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/precision/#examples","text":"from torchflare.metrics import Precision # Binary-Classification Problems acc = Precision ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Precision ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Precision ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/classification/recall/","text":"Class to compute Recall Score. Support binary, multiclass and multilabel cases Attributes value : Tensor property readonly Compute the recall score. Returns: Type Description Tensor The computed recall score. Methods __init__ ( self , num_classes , average = 'macro' , threshold = 0.5 , multilabel = False ) special Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/recall_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Recall , self ) . __init__ ( num_classes = num_classes , threshold = threshold , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/recall_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/recall_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/recall_meter.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import Recall # Binary-Classification Problems acc = Recall ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Recall ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Recallnum_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Recall"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall-attributes","text":"","title":"Attributes"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.value","text":"Compute the recall score. Returns: Type Description Tensor The computed recall score.","title":"value"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall-methods","text":"","title":"Methods"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.__init__","text":"Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/recall_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Recall , self ) . __init__ ( num_classes = num_classes , threshold = threshold , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/recall_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/recall_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/recall_meter.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/recall/#examples","text":"from torchflare.metrics import Recall # Binary-Classification Problems acc = Recall ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Recall ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Recallnum_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/regression/mae/","text":"Computes Mean Absolute Error. Attributes value : Tensor property readonly Computes the MAE. Returns: Type Description Tensor The computed MAE. Methods __init__ ( self ) special Constructor method for MAE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for MAE.\"\"\" self . _n_obs = None self . _abs_error_sum = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _abs_error_sum += torch . sum ( torch . abs ( outputs - targets )) self . _n_obs += targets . numel () handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _abs_error_sum = torch . tensor ( 0.0 )","title":"MAE"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE-attributes","text":"","title":"Attributes"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.value","text":"Computes the MAE. Returns: Type Description Tensor The computed MAE.","title":"value"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE-methods","text":"","title":"Methods"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.__init__","text":"Constructor method for MAE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for MAE.\"\"\" self . _n_obs = None self . _abs_error_sum = None self . reset ()","title":"__init__()"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _abs_error_sum += torch . sum ( torch . abs ( outputs - targets )) self . _n_obs += targets . numel ()","title":"accumulate()"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _abs_error_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/regression/mse/","text":"Computes Mean Squared Error. Attributes value : Tensor property readonly Computes the MSE. Returns: Type Description Tensor The computed MSE. Methods __init__ ( self ) special Constructor Method for MSE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSE.\"\"\" self . _n_obs = None self . _squared_error_sum = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _squared_error_sum += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _n_obs += targets . numel () handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _squared_error_sum = torch . tensor ( 0.0 )","title":"MSE"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE-attributes","text":"","title":"Attributes"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.value","text":"Computes the MSE. Returns: Type Description Tensor The computed MSE.","title":"value"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE-methods","text":"","title":"Methods"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.__init__","text":"Constructor Method for MSE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSE.\"\"\" self . _n_obs = None self . _squared_error_sum = None self . reset ()","title":"__init__()"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _squared_error_sum += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _n_obs += targets . numel ()","title":"accumulate()"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _squared_error_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/regression/msle/","text":"Computes Mean Squared Log Error. Attributes value : Tensor property readonly Computes the MSLE. Returns: Type Description Tensor The computed MSLE. Methods __init__ ( self ) special Constructor Method for MSLE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSLE.\"\"\" self . _n_obs = None self . _log_squared_error_sum = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) diff = torch . log1p ( outputs ) - torch . log1p ( targets ) self . _log_squared_error_sum += torch . sum ( torch . pow ( diff , 2 )) self . _n_obs += targets . numel () handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _log_squared_error_sum = torch . tensor ( 0.0 )","title":"MSLE"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE-attributes","text":"","title":"Attributes"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.value","text":"Computes the MSLE. Returns: Type Description Tensor The computed MSLE.","title":"value"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE-methods","text":"","title":"Methods"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.__init__","text":"Constructor Method for MSLE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSLE.\"\"\" self . _n_obs = None self . _log_squared_error_sum = None self . reset ()","title":"__init__()"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) diff = torch . log1p ( outputs ) - torch . log1p ( targets ) self . _log_squared_error_sum += torch . sum ( torch . pow ( diff , 2 )) self . _n_obs += targets . numel ()","title":"accumulate()"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _log_squared_error_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/regression/r2/","text":"Computes R2-score. Attributes value : Tensor property readonly Computes the R2Score. Exceptions: Type Description ValueError If no examples are found. Returns: Type Description Tensor The computed R2Score. Methods __init__ ( self ) special Constructor method for R2-score. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for R2-score.\"\"\" self . _num_examples = None self . _sum_of_errors = None self . _y_sq_sum = None self . _y_sum = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" self . _num_examples += outputs . shape [ 0 ] self . _sum_of_errors += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _y_sum += torch . sum ( targets ) self . _y_sq_sum += torch . sum ( torch . pow ( targets , 2 )) handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ) -> None : \"\"\"Reset the output and target lists.\"\"\" self . _num_examples = 0 self . _sum_of_errors = torch . tensor ( 0.0 ) self . _y_sq_sum = torch . tensor ( 0.0 ) self . _y_sum = torch . tensor ( 0.0 )","title":"R2Score"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score-attributes","text":"","title":"Attributes"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.value","text":"Computes the R2Score. Exceptions: Type Description ValueError If no examples are found. Returns: Type Description Tensor The computed R2Score.","title":"value"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score-methods","text":"","title":"Methods"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.__init__","text":"Constructor method for R2-score. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for R2-score.\"\"\" self . _num_examples = None self . _sum_of_errors = None self . _y_sq_sum = None self . _y_sum = None self . reset ()","title":"__init__()"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" self . _num_examples += outputs . shape [ 0 ] self . _sum_of_errors += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _y_sum += torch . sum ( targets ) self . _y_sq_sum += torch . sum ( torch . pow ( targets , 2 ))","title":"accumulate()"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ) -> None : \"\"\"Reset the output and target lists.\"\"\" self . _num_examples = 0 self . _sum_of_errors = torch . tensor ( 0.0 ) self . _y_sq_sum = torch . tensor ( 0.0 ) self . _y_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/segmentation/dice/","text":"Class to compute Dice Score. Attributes value : Tensor property readonly Computes the dice score. Returns: Type Description Tensor The computed Dice score. Methods __init__ ( self , threshold = None , class_dim = 1 ) special Constructor method for DiceScore. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/dice_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for DiceScore. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset () accumulate ( self , outputs , targets ) Class to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/dice_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Class to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets ) handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/dice_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/dice_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"DiceScore"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore-attributes","text":"","title":"Attributes"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.value","text":"Computes the dice score. Returns: Type Description Tensor The computed Dice score.","title":"value"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore-methods","text":"","title":"Methods"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.__init__","text":"Constructor method for DiceScore. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/dice_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for DiceScore. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset ()","title":"__init__()"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.accumulate","text":"Class to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/dice_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Class to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/dice_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/dice_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/segmentation/iou/","text":"Class which computes intersection over union. Attributes value : Tensor property readonly Method to Compute IOU. Returns: Type Description Tensor The computed iou. Methods __init__ ( self , threshold = None , class_dim = 1 ) special Constructor method for IOU. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/iou_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for IOU. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/iou_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets ) handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/iou_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Method to reset the accumulation lists. Source code in torchflare/metrics/iou_meter.py def reset ( self ): \"\"\"Method to reset the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"IOU"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU-attributes","text":"","title":"Attributes"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.value","text":"Method to Compute IOU. Returns: Type Description Tensor The computed iou.","title":"value"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU-methods","text":"","title":"Methods"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.__init__","text":"Constructor method for IOU. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/iou_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for IOU. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset ()","title":"__init__()"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/iou_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/iou_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.reset","text":"Method to reset the accumulation lists. Source code in torchflare/metrics/iou_meter.py def reset ( self ): \"\"\"Method to reset the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"modules/airface/","text":"Implements LiArcFace. Classes LiArcFace Implementation of Li-ArcFace. AirFace: Lightweight and Efficient Model for Face Recognition Methods __init__ ( self , in_features , out_features , s = 64 , m = 0.45 ) special Constructor class of LiArcFace. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 64 m margin 0.45 Source code in torchflare/modules/airface.py def __init__ ( self , in_features , out_features , s = 64 , m = 0.45 ): \"\"\"Constructor class of LiArcFace. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( LiArcFace , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . s = s self . m = m self . eps = 1e-7 self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/airface.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" cos_theta = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return cos_theta cos_theta . clamp ( - 1 + self . eps , 1 - self . eps ) theta = torch . acos ( cos_theta ) one_hot = torch . zeros_like ( cos_theta ) one_hot . scatter_ ( 1 , targets . data . view ( - 1 , 1 ), 1 ) target = ( math . pi - 2 * ( theta + self . m )) / math . pi other = ( math . pi - 2 * theta ) / math . pi output = ( one_hot * target ) + (( 1.0 - one_hot ) * other ) output = output * self . s return output Examples import torch.nn as nn from torchflare.modules import LiArcFace layer = LiArcFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"Lightweight ArcFace"},{"location":"modules/airface/#torchflare.modules.airface-classes","text":"","title":"Classes"},{"location":"modules/airface/#torchflare.modules.airface.LiArcFace","text":"Implementation of Li-ArcFace. AirFace: Lightweight and Efficient Model for Face Recognition","title":"LiArcFace"},{"location":"modules/airface/#torchflare.modules.airface.LiArcFace-methods","text":"","title":"Methods"},{"location":"modules/airface/#examples","text":"import torch.nn as nn from torchflare.modules import LiArcFace layer = LiArcFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"Examples"},{"location":"modules/am_softmax/","text":"Implements AM-softmax. Classes AMSoftmax Implementation of Additive Margin Softmax for Face Verification. Additive Margin Softmax: https://arxiv.org/abs/1801.05599 Methods __init__ ( self , in_features , out_features , m = 0.35 , s = 32 ) special Class Constructor. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 32 m margin 0.35 Source code in torchflare/modules/am_softmax.py def __init__ ( self , in_features , out_features , m = 0.35 , s = 32 ): \"\"\"Class Constructor. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( AMSoftmax , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . m = m self . s = s self . eps = 1e-7 self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/am_softmax.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" cos_theta = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return cos_theta one_hot = torch . zeros_like ( cos_theta ) one_hot . scatter_ ( 1 , targets . view ( - 1 , 1 ) . long (), 1 ) logits = torch . where ( one_hot . bool (), cos_theta - self . m , cos_theta ) logits = torch . cos ( logits ) logits *= self . s return logits import torch.nn as nn from torchflare.modules import AMSoftmax layer = AMSoftmax ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"AMSoftmax"},{"location":"modules/am_softmax/#torchflare.modules.am_softmax-classes","text":"","title":"Classes"},{"location":"modules/am_softmax/#torchflare.modules.am_softmax.AMSoftmax","text":"Implementation of Additive Margin Softmax for Face Verification. Additive Margin Softmax: https://arxiv.org/abs/1801.05599","title":"AMSoftmax"},{"location":"modules/am_softmax/#torchflare.modules.am_softmax.AMSoftmax-methods","text":"","title":"Methods"},{"location":"modules/arcface/","text":"Implements ArcFace. Classes ArcFace Implementation of ArcFace. ArcFace: : Additive Angular Margin Loss for Deep Face Recognition Methods __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ) special Class Constructor. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 30.0 m margin 0.35 Source code in torchflare/modules/arcface.py def __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ): \"\"\"Class Constructor. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( ArcFace , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . s = s self . m = m self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) self . threshold = math . pi - self . m self . eps = 1e-7 forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/arcface.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" cos_theta = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return cos_theta theta = torch . acos ( torch . clamp ( cos_theta , - 1 + self . eps , 1 - self . eps )) one_hot = torch . zeros_like ( cos_theta ) one_hot . scatter_ ( 1 , targets . view ( - 1 , 1 ) . long (), 1 ) mask = torch . where ( theta > self . threshold , torch . zeros_like ( one_hot ), one_hot ) logits = torch . where ( mask . bool (), theta + self . m , theta ) logits = torch . cos ( logits ) logits *= self . s return logits import torch.nn as nn from torchflare.modules import ArcFace layer = ArcFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"ArcFace"},{"location":"modules/arcface/#torchflare.modules.arcface-classes","text":"","title":"Classes"},{"location":"modules/arcface/#torchflare.modules.arcface.ArcFace","text":"Implementation of ArcFace. ArcFace: : Additive Angular Margin Loss for Deep Face Recognition","title":"ArcFace"},{"location":"modules/arcface/#torchflare.modules.arcface.ArcFace-methods","text":"","title":"Methods"},{"location":"modules/cosface/","text":"Implements CosFace. Classes CosFace Implementation of CosFace. CosFace: Large Margin Cosine Loss for Deep Face Recognition Methods __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ) special Class Constructor. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 30.0 m margin 0.35 Source code in torchflare/modules/cosface.py def __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ): \"\"\"Class Constructor. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( CosFace , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . s = s self . m = m self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/cosface.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" # normalize features and weights logits = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return logits # add margin target_logits = logits - self . m one_hot = torch . zeros_like ( logits ) one_hot . scatter_ ( 1 , targets . view ( - 1 , 1 ) . long (), 1 ) output = logits * ( 1 - one_hot ) + target_logits * one_hot # feature re-scale output *= self . s return output import torch.nn as nn from torchflare.modules import CosFace layer = CosFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"CosFace"},{"location":"modules/cosface/#torchflare.modules.cosface-classes","text":"","title":"Classes"},{"location":"modules/cosface/#torchflare.modules.cosface.CosFace","text":"Implementation of CosFace. CosFace: Large Margin Cosine Loss for Deep Face Recognition","title":"CosFace"},{"location":"modules/cosface/#torchflare.modules.cosface.CosFace-methods","text":"","title":"Methods"},{"location":"modules/se_modules/","text":"Implementation of Squeeze and Excitation BLocks. Classes CSE Implementation of Channel Wise Squeeze and Excitation Block. Paper : https://arxiv.org/abs/1709.01507 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939 and https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178 Methods __init__ ( self , in_channels , r = 16 ) special Constructor for CSE class. Parameters: Name Type Description Default in_channels int The number of input channels in the feature map. required r int The reduction ration (Default : 16) 16 Source code in torchflare/modules/se_modules.py def __init__ ( self , in_channels : int , r : int = 16 ): \"\"\"Constructor for CSE class. Args: in_channels : The number of input channels in the feature map. r : The reduction ration (Default : 16) \"\"\" super ( CSE , self ) . __init__ () self . in_channels = in_channels self . r = r self . linear1 = nn . Linear ( self . in_channels , self . in_channels // self . r ) self . linear2 = nn . Linear ( self . in_channels // r , self . in_channels ) forward ( self , x ) Forward Method. Parameters: Name Type Description Default x Tensor The input tensor of shape (batch, channels, height, width) required Returns: Type Description Tensor Tensor of same shape Source code in torchflare/modules/se_modules.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method. Args: x: The input tensor of shape (batch, channels, height, width) Returns: Tensor of same shape \"\"\" x_inp = x x = x . view ( * ( x . shape [: - 2 ]), - 1 ) . mean ( - 1 ) x = F . relu ( self . linear1 ( x ), inplace = True ) x = self . linear2 ( x ) x = x . unsqueeze ( - 1 ) . unsqueeze ( - 1 ) x = torch . sigmoid ( x ) x = torch . mul ( x_inp , x ) return x SCSE Implementation of SCSE : Concurrent Spatial and Channel Squeeze and Channel Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178 Methods __init__ ( self , in_channels , r = 16 ) special Constructor for SCSE class. Parameters: Name Type Description Default in_channels The number of input channels in the feature map. required r The reduction ration (Default : 16) 16 Source code in torchflare/modules/se_modules.py def __init__ ( self , in_channels , r = 16 ): \"\"\"Constructor for SCSE class. Args: in_channels : The number of input channels in the feature map. r : The reduction ration (Default : 16) \"\"\" super ( SCSE , self ) . __init__ () self . in_channels = in_channels self . r = r self . cse = CSE ( in_channels = self . in_channels , r = self . r ) self . sse = SSE ( in_channels = self . in_channels ) forward ( self , x ) Forward method. Parameters: Name Type Description Default x The input tensor of shape (batch, channels, height, width) required Returns: Type Description Tensor Tensor of same shape Source code in torchflare/modules/se_modules.py def forward ( self , x ) -> torch . Tensor : \"\"\"Forward method. Args: x: The input tensor of shape (batch, channels, height, width) Returns: Tensor of same shape \"\"\" cse = self . cse ( x ) sse = self . sse ( x ) op = torch . add ( cse , sse ) return op SSE SSE : Channel Squeeze and Spatial Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178 Methods __init__ ( self , in_channels ) special Constructor method for SSE class. Parameters: Name Type Description Default in_channels The number of input channels in the feature map. required Source code in torchflare/modules/se_modules.py def __init__ ( self , in_channels ): \"\"\"Constructor method for SSE class. Args: in_channels : The number of input channels in the feature map. \"\"\" super ( SSE , self ) . __init__ () self . in_channels = in_channels # noinspection PyTypeChecker self . conv = nn . Conv2d ( in_channels = self . in_channels , out_channels = 1 , kernel_size = 1 , stride = 1 ) forward ( self , x ) Forward Method. Parameters: Name Type Description Default x The input tensor of shape (batch, channels, height, width) required Returns: Type Description Tensor Tensor of same shape Source code in torchflare/modules/se_modules.py def forward ( self , x ) -> torch . Tensor : \"\"\"Forward Method. Args: x: The input tensor of shape (batch, channels, height, width) Returns: Tensor of same shape \"\"\" x_inp = x x = self . conv ( x ) x = torch . sigmoid ( x ) x = torch . mul ( x_inp , x ) return x","title":"Sqeeze and Excitation Blocks"},{"location":"modules/se_modules/#torchflare.modules.se_modules-classes","text":"","title":"Classes"},{"location":"modules/se_modules/#torchflare.modules.se_modules.CSE","text":"Implementation of Channel Wise Squeeze and Excitation Block. Paper : https://arxiv.org/abs/1709.01507 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939 and https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178","title":"CSE"},{"location":"modules/se_modules/#torchflare.modules.se_modules.CSE-methods","text":"","title":"Methods"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SCSE","text":"Implementation of SCSE : Concurrent Spatial and Channel Squeeze and Channel Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178","title":"SCSE"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SCSE-methods","text":"","title":"Methods"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SSE","text":"SSE : Channel Squeeze and Spatial Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178","title":"SSE"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SSE-methods","text":"","title":"Methods"}]}